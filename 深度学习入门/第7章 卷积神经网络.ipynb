{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ordinary-observation",
   "metadata": {},
   "source": [
    "# 第七章 卷积神经网络（20200222-20200223） "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "partial-typing",
   "metadata": {},
   "source": [
    "- CNN在此前的全连接层的网络中新增了卷积层和池化层\n",
    "- 使用im2col函数可以简单、高效地实现卷积层和池化层\n",
    "- 通过CNN的可视化，可随着层次变深，提取的信息越加高级\n",
    "- LeNet和AlexNet是CNN的代表性网络\n",
    "- 在深度学习的发展中，大数据和GPU做出来很大的贡献"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proved-segment",
   "metadata": {},
   "source": [
    "本章的主题是卷积神经网络（Convolutional Neural Network,CNN）,CNN被广泛应用于图像识别、语音识别等各种场合，在图像识别的比赛中，基于深度学习的方法几乎都以CNN为基础。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "packed-sensitivity",
   "metadata": {},
   "source": [
    "## 7.1 整体结构 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dated-translator",
   "metadata": {},
   "source": [
    "CNN中新出现了卷积层（Convolution层）和池化层（pooling层）。之前介绍地神经网络中，相邻层的所有神经元之间都有连接，这称为全连接，在全连接的神经网络中，Affine层后面跟着激活函数ReLU层（或Sigmoid层）。最后经过Affine-Softmax层输出结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "administrative-candy",
   "metadata": {},
   "source": [
    "CNN层的连接顺序为Convolution-ReLU-(Pooling)可以理解为之前的“Affine-ReLU”被替换成了“Convolution-ReLU-(Pooling连接)”（Pooling层有时会被省略）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flush-flight",
   "metadata": {},
   "source": [
    "## 7.2 卷积层   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stunning-animation",
   "metadata": {},
   "source": [
    "  全连接层中数据的形状被“忽视”了，图像一般是三维数据，但是向全连接层输入数据时需要将三维数据拉平为一维数据。\\\n",
    "  而卷积层可以保持形状不变，当输入数据是图像时，卷积层会以三维数据的形式接受输入数据，并同样以三维数据的形式输出至下一层。\\\n",
    "  CNN中，有时将卷积层的输入输出数据称为特征图。其中，卷积层的输入数据称为输入特征图，输出数据称为输出特征图。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lucky-verse",
   "metadata": {},
   "source": [
    "卷积层进行的处理就是卷积运算，卷积运算相当于图像处理中的“滤波器运算”,将各个位置上滤波器的元素和输入的对应元素相乘，然后再求和（有时将这个运算称为**乘积累加运算**）。\\\n",
    "CNN中，滤波器的参数就对应之前的权重，并且，CNN中还存在偏置，偏置通常只有一个。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "obvious-print",
   "metadata": {},
   "source": [
    "- 填充、步幅"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innovative-garbage",
   "metadata": {},
   "source": [
    "在进行卷积层的处理之前，有时要向输入数据的周围填入固定的数据，这称为**填充（padding）**，是卷积运算中经常会用到的处理。填充的数据为幅度为1/2/3...像素为0的数据。\\\n",
    "使用填充主要是为了调整输出的大小，卷积运算可以在保持空间大小不变的情况下将数据传给下一层。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bearing-volunteer",
   "metadata": {},
   "source": [
    "使用滤波器的位置间隔称为**步幅**。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "underlying-colors",
   "metadata": {},
   "source": [
    "- 输出大小"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charming-hypothetical",
   "metadata": {},
   "source": [
    "假设输入的大小为$(H,W)$,滤波器的大小为$（FH,FW）$,输出大小为$(OH,OW)$，填充为P，步幅为S，这时输出大小为："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "human-partner",
   "metadata": {},
   "source": [
    "$$OH = \\frac{H+2P-FH}{S} +1 \\tag{7.1} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afraid-northwest",
   "metadata": {},
   "source": [
    "$$OW = \\frac{W+2P-FW}{S} + 1 \\tag{7.2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "listed-northern",
   "metadata": {},
   "source": [
    "一般设定的值要是上式中的$\\frac{H+2P-FH}{S}$和$\\frac{W+2P-FW}{S}$分别可以除尽，否则进行报错处理，但根据深入学习框架的不同，有时会向最接近的整数四舍五入，不进行报错而继续运行。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "requested-effect",
   "metadata": {},
   "source": [
    "- 数据书写顺序"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accepted-belarus",
   "metadata": {},
   "source": [
    "对于三维数据，通道方向上有多个特征图时，会按通道进行输入数据和滤波器的卷积运算，并将结果相加，从而得到输出。\\\n",
    "在三维数据的卷积运算中，**输入数据和滤波器的通道数要设为相同的值**，滤波器的大小可以设为任意值。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gross-roulette",
   "metadata": {},
   "source": [
    "把三维数据表示为多维数组，书写顺序为（channel,height,weight）,比如，通道数为C，高度为H，长度为W的数据可以写成$(C,H,W)$，滤波器的通道数为C，高度为FH（Filter Height），长度为FW时，可写成$(C,FH,FW)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equipped-prince",
   "metadata": {},
   "source": [
    "- 批处理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statistical-capture",
   "metadata": {},
   "source": [
    "通过应用FN个滤波器，输出的特征也生成了FN个。如果将这FN个特征图汇集在一起，就得到了形如（FN,OH,OW）的方块。将这个方块传给下一层，就是CNN的处理流。\\\n",
    "不通形状的方块相加时，可以基于Numpy的广播功能轻松实现。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "willing-genesis",
   "metadata": {},
   "source": [
    "我们希望卷积运算也同样进行批处理。为此，需要将在各层之间传递的数据保存为四维数据，就是按(batch_num,channel,height,width)保存数据。\\\n",
    "需要注意，网络间传递的是四维数据，对N个数据进行了卷积运算，相当于将N次的处理汇总成了一次运行。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "square-workstation",
   "metadata": {},
   "source": [
    "## 7.3 池化层"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coral-geometry",
   "metadata": {},
   "source": [
    "池化是缩小高、长方向上的空间的运算。\\\n",
    "另外，一般来说，池化的窗口的大小会会和步幅设定为相同的值。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respective-delhi",
   "metadata": {},
   "source": [
    "除了Max池化之外，还有Average池化，在图像识别领域，主要使用Max池化。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charged-proposition",
   "metadata": {},
   "source": [
    "池化层的特征：\n",
    "- 没有要学习的参数\n",
    "- 输入层和输出层的通道数不会发生变化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modified-confidentiality",
   "metadata": {},
   "source": [
    "## 7.4 卷积层和池化层的实现 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perceived-stephen",
   "metadata": {},
   "source": [
    "- 基于im2col的展开"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imported-league",
   "metadata": {},
   "source": [
    "CNN中传递的数据是四维数据，所谓四维数据，比如（10,1,28,28），则它对应10个高为28，长为28，通道为1的数据。\\\n",
    "im2col将输入数据展开以适合滤波器。对3维的数据应用im2col后，数据转换为2维数据。\\\n",
    "使用im2col展开输入数据后，之后就只需将卷积层的滤波器（权重）纵向展开为一列，并计算两个矩阵的乘积即可。\\\n",
    "因为CNN会保存为4维数组，所以将2维输出数据转换为合适的形状。以上就是卷积层的实现流程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "processed-torture",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load util.py\n",
    "import numpy as np\n",
    "\n",
    "def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n",
    "    \"\"\"\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_data : 由(数据量, 通道, 高, 长)的4维数组构成的输入数据\n",
    "    filter_h : 滤波器的高\n",
    "    filter_w : 滤波器的长\n",
    "    stride : 步幅\n",
    "    pad : 填充\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    col : 2维数组\n",
    "    \"\"\"\n",
    "    N, C, H, W = input_data.shape\n",
    "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "\n",
    "    img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n",
    "    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
    "\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride*out_w\n",
    "            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
    "\n",
    "    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n",
    "    return col\n",
    "\n",
    "\n",
    "def col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0):\n",
    "    \"\"\"\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    col :\n",
    "    input_shape : 输入数据的形状（例：(10, 1, 28, 28)）\n",
    "    filter_h :\n",
    "    filter_w\n",
    "    stride\n",
    "    pad\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    \"\"\"\n",
    "    N, C, H, W = input_shape\n",
    "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "    col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n",
    "\n",
    "    img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride*out_w\n",
    "            img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n",
    "\n",
    "    return img[:, :, pad:H + pad, pad:W + pad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "practical-integrity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 75)\n",
      "(90, 75)\n"
     ]
    }
   ],
   "source": [
    "# 实际使用im2col\n",
    "import sys,os\n",
    "sys.path.append(os.pardir)\n",
    "\n",
    "x1 = np.random.rand(1,3,7,7)\n",
    "col1 = im2col(x1,5,5,stride=1,pad=0)\n",
    "print(col1.shape)\n",
    "\n",
    "x2 = np.random.rand(10,3,7,7)\n",
    "col2 = im2col(x2,5,5,stride=1,pad=0)\n",
    "print(col2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "native-calvin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用im2col实现卷积层\n",
    "class Convolution:\n",
    "    def __init__(self,w,b,stride=1,pad=0):          \n",
    "        self.w = w\n",
    "        self.b = b\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "    def forward(self,x):\n",
    "        FN,C,FH,FW = self.w.shape\n",
    "        N,C,H,W = x.shape\n",
    "        out_h = int(1+ (H + 2*self.pad - FH)/self.stride)\n",
    "        out_w = int(1+ (W + 2*self.pad - FW)/self.stride)\n",
    "        \n",
    "        col = im2col(x,FH,FW,self.stride,self.pad)\n",
    "        col_w = self.w.reshape(FN,-1)                        #滤波器展开为2维数组\n",
    "        out = np.dot(col,col_w) + self.b\n",
    "        \n",
    "        out = out.reshape(N,out_h,out_w,-1).transpose(0,3,1,2)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bound-shock",
   "metadata": {},
   "source": [
    "卷积层的初始化方法将滤波器（权重）、偏置、步幅、填充作为参数接收，滤波器是（FN,C,FH,FW）的四维形状。FN,C,FH,FW分别指滤波器数量、通道数、滤波器高度、滤波器长度。\\\n",
    "展开滤波器的部分，将各个滤波器的方块纵向展开为一列，将滤波器展开为2维数组，然后计算展开后矩阵的乘积。\\\n",
    "最后将输出大小转换为合适的形状。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinate-washington",
   "metadata": {},
   "source": [
    "卷积层反向传播的实现，必须进行im2col的逆处理。使用util.py中的col2im函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "incorrect-victim",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolution:\n",
    "    def __init__(self, W, b, stride=1, pad=0):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "        # 中间数据（backward时使用）\n",
    "        self.x = None   \n",
    "        self.col = None\n",
    "        self.col_W = None\n",
    "        \n",
    "        # 权重和偏置参数的梯度\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = 1 + int((H + 2*self.pad - FH) / self.stride)\n",
    "        out_w = 1 + int((W + 2*self.pad - FW) / self.stride)\n",
    "\n",
    "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
    "        col_W = self.W.reshape(FN, -1).T                                 #自动计算-1维度上的元素的个数\n",
    "\n",
    "        out = np.dot(col, col_W) + self.b\n",
    "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
    "\n",
    "        self.x = x\n",
    "        self.col = col\n",
    "        self.col_W = col_W\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        dout = dout.transpose(0,2,3,1).reshape(-1, FN)\n",
    "\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        self.dW = np.dot(self.col.T, dout)\n",
    "        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)\n",
    "\n",
    "        dcol = np.dot(dout, self.col_W.T)\n",
    "        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unusual-victor",
   "metadata": {},
   "source": [
    "- 池化层的实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serial-access",
   "metadata": {},
   "source": [
    "池化的实现也是使用im2col展开输入数据，不同的是，池化时在通道方向上是独立的，池化的应用区域按通道单独展开。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "internal-claim",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pooling:\n",
    "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "        self.x = None\n",
    "        self.arg_max = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
    "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
    "\n",
    "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        col = col.reshape(-1, self.pool_h*self.pool_w)                 #自动计算-1维度上元素的个数\n",
    "\n",
    "        arg_max = np.argmax(col, axis=1)\n",
    "        out = np.max(col, axis=1)\n",
    "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
    "\n",
    "        self.x = x\n",
    "        self.arg_max = arg_max\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout = dout.transpose(0, 2, 3, 1)\n",
    "        \n",
    "        pool_size = self.pool_h * self.pool_w\n",
    "        dmax = np.zeros((dout.size, pool_size))\n",
    "        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n",
    "        dmax = dmax.reshape(dout.shape + (pool_size,)) \n",
    "        \n",
    "        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n",
    "        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "german-trial",
   "metadata": {},
   "source": [
    "池化层的实现：1.展开输入数据 2.求各行的最大值 3.转换为合适的输出大小"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certain-remains",
   "metadata": {},
   "source": [
    "## 7.5 CNN的实现 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranking-liverpool",
   "metadata": {},
   "source": [
    "这里要实现的CNN的网络构成是“Convolution-ReLU-Pooling-Affine-ReLU-Affine-Softmax”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "received-error",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load simple_convnet.py\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 为了导入父目录的文件而进行的设定\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "\n",
    "class SimpleConvNet:\n",
    "    \"\"\"简单的ConvNet\n",
    "\n",
    "    conv - relu - pool - affine - relu - affine - softmax\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_size : 输入大小（MNIST的情况下为784）\n",
    "    hidden_size_list : 隐藏层的神经元数量的列表（e.g. [100, 100, 100]）\n",
    "    output_size : 输出大小（MNIST的情况下为10）\n",
    "    activation : 'relu' or 'sigmoid'\n",
    "    weight_init_std : 指定权重的标准差（e.g. 0.01）\n",
    "        指定'relu'或'he'的情况下设定“He的初始值”\n",
    "        指定'sigmoid'或'xavier'的情况下设定“Xavier的初始值”\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=(1, 28, 28), \n",
    "                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
    "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
    "\n",
    "        # 初始化权重\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * \\\n",
    "                            np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * \\\n",
    "                            np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std * \\\n",
    "                            np.random.randn(hidden_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "\n",
    "        # 生成层\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n",
    "                                           conv_param['stride'], conv_param['pad'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        \"\"\"求损失函数\n",
    "        参数x是输入数据、t是教师标签\n",
    "        \"\"\"\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        acc = 0.0\n",
    "        \n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt) \n",
    "        \n",
    "        return acc / x.shape[0]\n",
    "\n",
    "    def numerical_gradient(self, x, t):\n",
    "        \"\"\"求梯度（数值微分）\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 输入数据\n",
    "        t : 教师标签\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        具有各层的梯度的字典变量\n",
    "            grads['W1']、grads['W2']、...是各层的权重\n",
    "            grads['b1']、grads['b2']、...是各层的偏置\n",
    "        \"\"\"\n",
    "        loss_w = lambda w: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        for idx in (1, 2, 3):\n",
    "            grads['W' + str(idx)] = numerical_gradient(loss_w, self.params['W' + str(idx)])\n",
    "            grads['b' + str(idx)] = numerical_gradient(loss_w, self.params['b' + str(idx)])\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        \"\"\"求梯度（误差反向传播法）\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 输入数据\n",
    "        t : 教师标签\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        具有各层的梯度的字典变量\n",
    "            grads['W1']、grads['W2']、...是各层的权重\n",
    "            grads['b1']、grads['b2']、...是各层的偏置\n",
    "        \"\"\"\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 设定\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads\n",
    "        \n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "\n",
    "        for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):\n",
    "            self.layers[key].W = self.params['W' + str(i+1)]\n",
    "            self.layers[key].b = self.params['b' + str(i+1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confidential-spank",
   "metadata": {},
   "source": [
    "- 使用SimpleConvNet学习MNIST数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "likely-tucson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.2990887164204667\n",
      "=== epoch:1, train acc:0.265, test acc:0.302 ===\n",
      "train loss:2.2986400670932756\n",
      "train loss:2.293927213094625\n",
      "train loss:2.2875329976195995\n",
      "train loss:2.2837539904721633\n",
      "train loss:2.2686327408707805\n",
      "train loss:2.2568392496078498\n",
      "train loss:2.249226313933457\n",
      "train loss:2.2276507229012834\n",
      "train loss:2.2046082973513585\n",
      "train loss:2.1613952069878595\n",
      "train loss:2.1137232337611143\n",
      "train loss:2.0776099440127025\n",
      "train loss:2.0288393044469815\n",
      "train loss:1.9750228872263673\n",
      "train loss:1.929999341232218\n",
      "train loss:1.8743957779949527\n",
      "train loss:1.8019034734797492\n",
      "train loss:1.7814179014098233\n",
      "train loss:1.693991930069755\n",
      "train loss:1.5568107293822826\n",
      "train loss:1.5352049561362653\n",
      "train loss:1.3473113896769895\n",
      "train loss:1.3232047279474444\n",
      "train loss:1.2471725011208235\n",
      "train loss:1.2281038045362107\n",
      "train loss:1.1206511375790467\n",
      "train loss:1.0366166116001145\n",
      "train loss:0.8832001679235182\n",
      "train loss:0.9844043386875623\n",
      "train loss:0.9117578455110994\n",
      "train loss:0.8843923007240346\n",
      "train loss:0.7292322696906539\n",
      "train loss:0.7338948069718885\n",
      "train loss:0.8742695677501194\n",
      "train loss:0.8189356368625507\n",
      "train loss:0.7162139078716071\n",
      "train loss:0.5614928319079859\n",
      "train loss:0.6786109740774552\n",
      "train loss:0.4397549279493083\n",
      "train loss:0.5302261072142455\n",
      "train loss:0.6576496115862007\n",
      "train loss:0.6559265621976657\n",
      "train loss:0.566351307554036\n",
      "train loss:0.5566303765125491\n",
      "train loss:0.5498941583329302\n",
      "train loss:0.4637602223236426\n",
      "train loss:0.5211551113146637\n",
      "train loss:0.6379793049260514\n",
      "train loss:0.5941751439141548\n",
      "train loss:0.6127997878142103\n",
      "=== epoch:2, train acc:0.82, test acc:0.801 ===\n",
      "train loss:0.3836403667987973\n",
      "train loss:0.5434913757182613\n",
      "train loss:0.5125381381740864\n",
      "train loss:0.5959047719455175\n",
      "train loss:0.473276983217372\n",
      "train loss:0.3020796653299872\n",
      "train loss:0.44832026681256487\n",
      "train loss:0.6226084519674241\n",
      "train loss:0.45134045626917446\n",
      "train loss:0.5807011684562418\n",
      "train loss:0.44707046301051984\n",
      "train loss:0.706278656125802\n",
      "train loss:0.4670646448618867\n",
      "train loss:0.4986693911578349\n",
      "train loss:0.46244781366026916\n",
      "train loss:0.41234891266717144\n",
      "train loss:0.47884992377214475\n",
      "train loss:0.44971048115574547\n",
      "train loss:0.37098098483209113\n",
      "train loss:0.36519498874071943\n",
      "train loss:0.39724331696158804\n",
      "train loss:0.2739548474960023\n",
      "train loss:0.2830890537293801\n",
      "train loss:0.5118334566390272\n",
      "train loss:0.3983984978803206\n",
      "train loss:0.47974192327383797\n",
      "train loss:0.4423267358175644\n",
      "train loss:0.35688927381704744\n",
      "train loss:0.45879906296659845\n",
      "train loss:0.22328234228848576\n",
      "train loss:0.4696857599111858\n",
      "train loss:0.2731097209565444\n",
      "train loss:0.33108876580185403\n",
      "train loss:0.5909589627596342\n",
      "train loss:0.3868984843273103\n",
      "train loss:0.4124611573065443\n",
      "train loss:0.45575502681251634\n",
      "train loss:0.2623237471196005\n",
      "train loss:0.6281076129503116\n",
      "train loss:0.372962290795364\n",
      "train loss:0.3427400249456394\n",
      "train loss:0.23379006266025773\n",
      "train loss:0.452397863215263\n",
      "train loss:0.19587207769754564\n",
      "train loss:0.3175443429868382\n",
      "train loss:0.366663139029713\n",
      "train loss:0.3229603292704445\n",
      "train loss:0.2930086707658105\n",
      "train loss:0.2888640476260609\n",
      "train loss:0.3172841802461187\n",
      "=== epoch:3, train acc:0.865, test acc:0.851 ===\n",
      "train loss:0.34550996863379474\n",
      "train loss:0.48465711541190687\n",
      "train loss:0.3388509668035111\n",
      "train loss:0.3770018889501007\n",
      "train loss:0.37064837983092663\n",
      "train loss:0.316812803335613\n",
      "train loss:0.48408175100076456\n",
      "train loss:0.34492007978616734\n",
      "train loss:0.4229681896160004\n",
      "train loss:0.27190634451656054\n",
      "train loss:0.31352668750495977\n",
      "train loss:0.40404553792946013\n",
      "train loss:0.3254677105597283\n",
      "train loss:0.2476726979425543\n",
      "train loss:0.22898869230042598\n",
      "train loss:0.23073367764168526\n",
      "train loss:0.3942327147358483\n",
      "train loss:0.35698182252621374\n",
      "train loss:0.3166429649026418\n",
      "train loss:0.559391867121837\n",
      "train loss:0.23524403192800794\n",
      "train loss:0.38589373255890375\n",
      "train loss:0.25823602730966416\n",
      "train loss:0.198182875438636\n",
      "train loss:0.17742599892200933\n",
      "train loss:0.17567433958155362\n",
      "train loss:0.2280412117439025\n",
      "train loss:0.3064851468243862\n",
      "train loss:0.31995844904362064\n",
      "train loss:0.21585746735627956\n",
      "train loss:0.5173663470333769\n",
      "train loss:0.2811190891470734\n",
      "train loss:0.18920019307126126\n",
      "train loss:0.2474823186400209\n",
      "train loss:0.3560758598383802\n",
      "train loss:0.29247593154762624\n",
      "train loss:0.33272927549984693\n",
      "train loss:0.23744661431612932\n",
      "train loss:0.341054682458075\n",
      "train loss:0.20150835951370766\n",
      "train loss:0.1523280713966806\n",
      "train loss:0.2700333726336604\n",
      "train loss:0.3337752007487545\n",
      "train loss:0.3640797234708358\n",
      "train loss:0.3417348322179663\n",
      "train loss:0.30200811299002067\n",
      "train loss:0.43913783690424646\n",
      "train loss:0.2918468617807131\n",
      "train loss:0.3418009754448029\n",
      "train loss:0.27351236105543597\n",
      "=== epoch:4, train acc:0.907, test acc:0.887 ===\n",
      "train loss:0.19741381716461603\n",
      "train loss:0.34770802361003894\n",
      "train loss:0.3523012106283258\n",
      "train loss:0.3556987779069491\n",
      "train loss:0.2581840346911762\n",
      "train loss:0.3230066257785492\n",
      "train loss:0.2593086783290344\n",
      "train loss:0.21920939789593136\n",
      "train loss:0.34548927720988837\n",
      "train loss:0.2823701875879502\n",
      "train loss:0.3856195116973482\n",
      "train loss:0.2170822777238822\n",
      "train loss:0.33149140280802597\n",
      "train loss:0.1425803163049134\n",
      "train loss:0.260143998146986\n",
      "train loss:0.20838571023162675\n",
      "train loss:0.14811168039934328\n",
      "train loss:0.29306091831475534\n",
      "train loss:0.23272726952186232\n",
      "train loss:0.21215058006441165\n",
      "train loss:0.26113000226896693\n",
      "train loss:0.33437791870023986\n",
      "train loss:0.22461589858302042\n",
      "train loss:0.2267169046247589\n",
      "train loss:0.29595040259324024\n",
      "train loss:0.1889061536132887\n",
      "train loss:0.22414386356326943\n",
      "train loss:0.24509549140029901\n",
      "train loss:0.4562980874104204\n",
      "train loss:0.22506364908393775\n",
      "train loss:0.40794322675822486\n",
      "train loss:0.325932029325149\n",
      "train loss:0.22987009369522518\n",
      "train loss:0.40742411871097894\n",
      "train loss:0.2177281467163165\n",
      "train loss:0.27707509170738526\n",
      "train loss:0.21667379010081078\n",
      "train loss:0.26379255514015343\n",
      "train loss:0.25932364457994816\n",
      "train loss:0.18757245061202193\n",
      "train loss:0.10999881571001968\n",
      "train loss:0.22912695300383984\n",
      "train loss:0.2662770057113244\n",
      "train loss:0.2059746942083766\n",
      "train loss:0.1617619328635179\n",
      "train loss:0.32999432727965866\n",
      "train loss:0.1736597539668315\n",
      "train loss:0.30179732922892877\n",
      "train loss:0.17821936326668988\n",
      "train loss:0.322321655243641\n",
      "=== epoch:5, train acc:0.903, test acc:0.899 ===\n",
      "train loss:0.18396602115960195\n",
      "train loss:0.2731951924278551\n",
      "train loss:0.32104708569498663\n",
      "train loss:0.2054821241736201\n",
      "train loss:0.21543448242778054\n",
      "train loss:0.13852682572688674\n",
      "train loss:0.13514944323388506\n",
      "train loss:0.39001765088083945\n",
      "train loss:0.19368569554062312\n",
      "train loss:0.18823423804416448\n",
      "train loss:0.17916221636927035\n",
      "train loss:0.2877745472711403\n",
      "train loss:0.21446906786655048\n",
      "train loss:0.24303984607852308\n",
      "train loss:0.09139387742610278\n",
      "train loss:0.13603218330361078\n",
      "train loss:0.2565882277512248\n",
      "train loss:0.202952985394751\n",
      "train loss:0.19021041608957723\n",
      "train loss:0.17439676682160704\n",
      "train loss:0.4005627072616965\n",
      "train loss:0.2168182044096204\n",
      "train loss:0.17267904975605133\n",
      "train loss:0.2935430778905734\n",
      "train loss:0.0825299768973191\n",
      "train loss:0.1372414797466038\n",
      "train loss:0.1667886346283079\n",
      "train loss:0.17762489058258907\n",
      "train loss:0.21667697449301088\n",
      "train loss:0.28334368923986597\n",
      "train loss:0.17460506690250954\n",
      "train loss:0.22310171923648364\n",
      "train loss:0.17739377204438456\n",
      "train loss:0.1732062448671314\n",
      "train loss:0.1348745429178421\n",
      "train loss:0.2568522522262682\n",
      "train loss:0.2605819609935331\n",
      "train loss:0.2572701122747517\n",
      "train loss:0.29588625681597647\n",
      "train loss:0.12652474131768143\n",
      "train loss:0.1641490445904771\n",
      "train loss:0.1279317312858246\n",
      "train loss:0.24954079770679827\n",
      "train loss:0.16234773757483553\n",
      "train loss:0.14608905577861744\n",
      "train loss:0.16351302527053493\n",
      "train loss:0.2862699363476905\n",
      "train loss:0.15115034784482623\n",
      "train loss:0.17584929956642303\n",
      "train loss:0.11612297445742165\n",
      "=== epoch:6, train acc:0.937, test acc:0.907 ===\n",
      "train loss:0.1727523572316032\n",
      "train loss:0.26710199366131976\n",
      "train loss:0.09453959612105699\n",
      "train loss:0.19374068557695487\n",
      "train loss:0.17940862841381258\n",
      "train loss:0.155949565047035\n",
      "train loss:0.20491854579680704\n",
      "train loss:0.120619185521961\n",
      "train loss:0.19733556073734299\n",
      "train loss:0.0910990218099243\n",
      "train loss:0.17080283597508272\n",
      "train loss:0.12404164162471282\n",
      "train loss:0.25569410315792773\n",
      "train loss:0.11341811927473992\n",
      "train loss:0.1854112986382477\n",
      "train loss:0.2535261113641971\n",
      "train loss:0.09854393138659771\n",
      "train loss:0.13591228354153662\n",
      "train loss:0.2944074799930227\n",
      "train loss:0.1848044241164519\n",
      "train loss:0.15003996023618227\n",
      "train loss:0.1330521981499008\n",
      "train loss:0.21690278364865\n",
      "train loss:0.12239378320560985\n",
      "train loss:0.19662409029655983\n",
      "train loss:0.13106800454545378\n",
      "train loss:0.10433304013222536\n",
      "train loss:0.138209022573506\n",
      "train loss:0.12552477796905095\n",
      "train loss:0.20729594683712346\n",
      "train loss:0.12724087436913253\n",
      "train loss:0.14926220179998848\n",
      "train loss:0.1387598615869899\n",
      "train loss:0.07301938146389199\n",
      "train loss:0.14159294147801074\n",
      "train loss:0.16488338915096634\n",
      "train loss:0.24319342881470934\n",
      "train loss:0.12679342913062938\n",
      "train loss:0.15314198650748392\n",
      "train loss:0.2024338135768485\n",
      "train loss:0.2148522646601845\n",
      "train loss:0.1890161067279189\n",
      "train loss:0.07601580238459542\n",
      "train loss:0.1317556053977965\n",
      "train loss:0.08802418960311187\n",
      "train loss:0.23586833109855088\n",
      "train loss:0.20154306541075026\n",
      "train loss:0.0761242686851416\n",
      "train loss:0.0981508869265499\n",
      "train loss:0.18739808495668633\n",
      "=== epoch:7, train acc:0.948, test acc:0.916 ===\n",
      "train loss:0.16801194203722555\n",
      "train loss:0.14911638688176418\n",
      "train loss:0.18665186627998126\n",
      "train loss:0.21123786830791005\n",
      "train loss:0.13571227259880486\n",
      "train loss:0.1536904772044464\n",
      "train loss:0.18713653191495452\n",
      "train loss:0.1536823839657354\n",
      "train loss:0.13677217190482965\n",
      "train loss:0.1163480214136085\n",
      "train loss:0.2879175806295816\n",
      "train loss:0.1024807427948118\n",
      "train loss:0.09641577482126001\n",
      "train loss:0.0809107313976889\n",
      "train loss:0.1777075054641487\n",
      "train loss:0.15510332184935322\n",
      "train loss:0.19971471485471565\n",
      "train loss:0.1357546242916711\n",
      "train loss:0.11947844038141447\n",
      "train loss:0.10063172556767995\n",
      "train loss:0.11031829813918331\n",
      "train loss:0.16938175286783486\n",
      "train loss:0.0823431184544647\n",
      "train loss:0.269454202305816\n",
      "train loss:0.10130063658211588\n",
      "train loss:0.1081512032838066\n",
      "train loss:0.10030838384734934\n",
      "train loss:0.14055025965448376\n",
      "train loss:0.14603627626236418\n",
      "train loss:0.06138967739003907\n",
      "train loss:0.190700150428726\n",
      "train loss:0.11657382359052106\n",
      "train loss:0.1694847722126202\n",
      "train loss:0.1424707214673884\n",
      "train loss:0.06451834043918138\n",
      "train loss:0.12432695316982349\n",
      "train loss:0.07088782712126522\n",
      "train loss:0.11807336878200486\n",
      "train loss:0.09143130576701555\n",
      "train loss:0.23556084946093747\n",
      "train loss:0.11905130923163262\n",
      "train loss:0.22355311096086183\n",
      "train loss:0.12163796133846795\n",
      "train loss:0.12306960456228257\n",
      "train loss:0.2131116597216662\n",
      "train loss:0.24017314663211664\n",
      "train loss:0.13323920097998623\n",
      "train loss:0.06625412677797439\n",
      "train loss:0.06397561110826745\n",
      "train loss:0.174945579282056\n",
      "=== epoch:8, train acc:0.955, test acc:0.929 ===\n",
      "train loss:0.07435683356951653\n",
      "train loss:0.11121519428724902\n",
      "train loss:0.05728823593130402\n",
      "train loss:0.12106241092031687\n",
      "train loss:0.17583949043292485\n",
      "train loss:0.23097753659098713\n",
      "train loss:0.08231846633777945\n",
      "train loss:0.0947994428550995\n",
      "train loss:0.0915000833207312\n",
      "train loss:0.10197653484584497\n",
      "train loss:0.0841007210237066\n",
      "train loss:0.10625486729277445\n",
      "train loss:0.08054833896188013\n",
      "train loss:0.09786897139581516\n",
      "train loss:0.056622744863730416\n",
      "train loss:0.1865590840422663\n",
      "train loss:0.06670088147002004\n",
      "train loss:0.07243481523670621\n",
      "train loss:0.09046783134868443\n",
      "train loss:0.09201914211501325\n",
      "train loss:0.10144440544096563\n",
      "train loss:0.08393949637294018\n",
      "train loss:0.22377924630175397\n",
      "train loss:0.09754832926662903\n",
      "train loss:0.18261618585556913\n",
      "train loss:0.08732723579523695\n",
      "train loss:0.07901034288923099\n",
      "train loss:0.18637395548643038\n",
      "train loss:0.1996484577262405\n",
      "train loss:0.1494501834844066\n",
      "train loss:0.21651168731320225\n",
      "train loss:0.12746881381534658\n",
      "train loss:0.09368893492569667\n",
      "train loss:0.08839936268135057\n",
      "train loss:0.26197934108878784\n",
      "train loss:0.08622409825164119\n",
      "train loss:0.09961252263871476\n",
      "train loss:0.12422025358607086\n",
      "train loss:0.0813928903471423\n",
      "train loss:0.11925723474037804\n",
      "train loss:0.10466559324818\n",
      "train loss:0.12984182556459373\n",
      "train loss:0.0969472933035131\n",
      "train loss:0.1829673579722998\n",
      "train loss:0.08163320762529383\n",
      "train loss:0.15696942653324522\n",
      "train loss:0.06427340245699212\n",
      "train loss:0.09514624623259409\n",
      "train loss:0.13104510438282224\n",
      "train loss:0.07771051196441993\n",
      "=== epoch:9, train acc:0.962, test acc:0.946 ===\n",
      "train loss:0.07756009765381153\n",
      "train loss:0.19542835414904\n",
      "train loss:0.12498253163551735\n",
      "train loss:0.14369811279134462\n",
      "train loss:0.0662267653372786\n",
      "train loss:0.13012298501054628\n",
      "train loss:0.13099100991030238\n",
      "train loss:0.11054148485264902\n",
      "train loss:0.07443041703170981\n",
      "train loss:0.10460287027934673\n",
      "train loss:0.14553179968675134\n",
      "train loss:0.1704327039916948\n",
      "train loss:0.06494196774314366\n",
      "train loss:0.10386606606272958\n",
      "train loss:0.04691106506227932\n",
      "train loss:0.04349567619775667\n",
      "train loss:0.14048075021430756\n",
      "train loss:0.07103003041076215\n",
      "train loss:0.06270119159058943\n",
      "train loss:0.15145242812028978\n",
      "train loss:0.039802621448916733\n",
      "train loss:0.14006929260893572\n",
      "train loss:0.10138790437346942\n",
      "train loss:0.05293491027825056\n",
      "train loss:0.06669181217391015\n",
      "train loss:0.056675422130219824\n",
      "train loss:0.12328635678160767\n",
      "train loss:0.15229736560136753\n",
      "train loss:0.05744922997473007\n",
      "train loss:0.09516874041132457\n",
      "train loss:0.03350377825532907\n",
      "train loss:0.10418536805827831\n",
      "train loss:0.05167006498595775\n",
      "train loss:0.06900381699053626\n",
      "train loss:0.08113390652846604\n",
      "train loss:0.07172352791662225\n",
      "train loss:0.05627884511929495\n",
      "train loss:0.07190531596571495\n",
      "train loss:0.11951386123689657\n",
      "train loss:0.0642160872583288\n",
      "train loss:0.11436305512655665\n",
      "train loss:0.03987107766307507\n",
      "train loss:0.08540465622625927\n",
      "train loss:0.09990941326062495\n",
      "train loss:0.06524867218603235\n",
      "train loss:0.04749345225743627\n",
      "train loss:0.08624820322370148\n",
      "train loss:0.11193135336702024\n",
      "train loss:0.12013258923437611\n",
      "train loss:0.09420390405279937\n",
      "=== epoch:10, train acc:0.967, test acc:0.947 ===\n",
      "train loss:0.13399873549238722\n",
      "train loss:0.07833418360457911\n",
      "train loss:0.05322461842695512\n",
      "train loss:0.04408280887519534\n",
      "train loss:0.1063371653240858\n",
      "train loss:0.16665014993392785\n",
      "train loss:0.08163105815686598\n",
      "train loss:0.12817022044677726\n",
      "train loss:0.04404360751248238\n",
      "train loss:0.06312794956228243\n",
      "train loss:0.12973108716534112\n",
      "train loss:0.06516883435985638\n",
      "train loss:0.12226073105946883\n",
      "train loss:0.2811073155765907\n",
      "train loss:0.13716233363259372\n",
      "train loss:0.1661943180238726\n",
      "train loss:0.05474276430463196\n",
      "train loss:0.07685430228282507\n",
      "train loss:0.08964179242790644\n",
      "train loss:0.05785012853646236\n",
      "train loss:0.12530898286281272\n",
      "train loss:0.050500202517015626\n",
      "train loss:0.22612939069519367\n",
      "train loss:0.055124104912980185\n",
      "train loss:0.09772140529952505\n",
      "train loss:0.07985605761824835\n",
      "train loss:0.0898290851600609\n",
      "train loss:0.12203422701069738\n",
      "train loss:0.07416981253078783\n",
      "train loss:0.06890774942952375\n",
      "train loss:0.16712894424510533\n",
      "train loss:0.07502618480470181\n",
      "train loss:0.0781588125508037\n",
      "train loss:0.04633868125432563\n",
      "train loss:0.1001178911506362\n",
      "train loss:0.08714622270693446\n",
      "train loss:0.06800705967075199\n",
      "train loss:0.05521276644188501\n",
      "train loss:0.10381802323947983\n",
      "train loss:0.07409057588281578\n",
      "train loss:0.12464592035502964\n",
      "train loss:0.0863712684647035\n",
      "train loss:0.07637646749209678\n",
      "train loss:0.039028462804596244\n",
      "train loss:0.07271820230299991\n",
      "train loss:0.06622508754689968\n",
      "train loss:0.11021948492196156\n",
      "train loss:0.07432280853953842\n",
      "train loss:0.07738419368582547\n",
      "train loss:0.19188261556908157\n",
      "=== epoch:11, train acc:0.972, test acc:0.947 ===\n",
      "train loss:0.05218585105601074\n",
      "train loss:0.16465659199515406\n",
      "train loss:0.047705935601211585\n",
      "train loss:0.1311355248766586\n",
      "train loss:0.07174169909142124\n",
      "train loss:0.026230077331336928\n",
      "train loss:0.06396470856538712\n",
      "train loss:0.051156080467414974\n",
      "train loss:0.051792194031165965\n",
      "train loss:0.07782974042112539\n",
      "train loss:0.05722150473709247\n",
      "train loss:0.05100839029086661\n",
      "train loss:0.1804322847420998\n",
      "train loss:0.06846615224649576\n",
      "train loss:0.06023727834001827\n",
      "train loss:0.04915744223084972\n",
      "train loss:0.10720012691647646\n",
      "train loss:0.11283220396606819\n",
      "train loss:0.0869299488264377\n",
      "train loss:0.05465461948994284\n",
      "train loss:0.08041015358934078\n",
      "train loss:0.03535148905152636\n",
      "train loss:0.0701515840895387\n",
      "train loss:0.039511306936017614\n",
      "train loss:0.07641295446186\n",
      "train loss:0.04076390546647524\n",
      "train loss:0.10191744184669632\n",
      "train loss:0.09823472001225415\n",
      "train loss:0.03336328867175828\n",
      "train loss:0.14548048378217035\n",
      "train loss:0.1261684368456191\n",
      "train loss:0.0969965174961676\n",
      "train loss:0.025308313012327686\n",
      "train loss:0.08853519974795669\n",
      "train loss:0.11964077480392168\n",
      "train loss:0.06841378421845101\n",
      "train loss:0.09413161418673845\n",
      "train loss:0.07087896661952761\n",
      "train loss:0.07666041071114577\n",
      "train loss:0.040594974061795505\n",
      "train loss:0.059378472526646145\n",
      "train loss:0.04932089356420727\n",
      "train loss:0.030804035230234682\n",
      "train loss:0.07408407875733998\n",
      "train loss:0.06575675798558077\n",
      "train loss:0.07594937635109132\n",
      "train loss:0.09852154139392302\n",
      "train loss:0.07475387749968503\n",
      "train loss:0.062333391969673795\n",
      "train loss:0.07825399646926617\n",
      "=== epoch:12, train acc:0.977, test acc:0.952 ===\n",
      "train loss:0.04489498307908289\n",
      "train loss:0.030323157014682748\n",
      "train loss:0.061533375616052065\n",
      "train loss:0.041938595919147524\n",
      "train loss:0.08492226041871088\n",
      "train loss:0.06063239471982875\n",
      "train loss:0.05145251496963328\n",
      "train loss:0.06254496009706223\n",
      "train loss:0.029796674912321318\n",
      "train loss:0.016142477425459777\n",
      "train loss:0.048487111493894536\n",
      "train loss:0.04748783360494089\n",
      "train loss:0.10340842435375627\n",
      "train loss:0.025379444591123897\n",
      "train loss:0.036773672333757224\n",
      "train loss:0.06282562129450513\n",
      "train loss:0.03760790699186962\n",
      "train loss:0.057360693557148255\n",
      "train loss:0.02924257674861503\n",
      "train loss:0.040510340158121855\n",
      "train loss:0.08731795288814567\n",
      "train loss:0.043493820565474614\n",
      "train loss:0.05711037883542997\n",
      "train loss:0.0637777335614651\n",
      "train loss:0.05820710067267401\n",
      "train loss:0.049577603811566125\n",
      "train loss:0.12964175841667283\n",
      "train loss:0.054401810667726244\n",
      "train loss:0.06775293130677619\n",
      "train loss:0.05496936983598952\n",
      "train loss:0.03843659220693703\n",
      "train loss:0.09371286774616237\n",
      "train loss:0.057011283903128145\n",
      "train loss:0.058935360771448175\n",
      "train loss:0.051859715884836746\n",
      "train loss:0.04732566610616141\n",
      "train loss:0.07212868013714979\n",
      "train loss:0.10268169684623597\n",
      "train loss:0.046615574936494396\n",
      "train loss:0.04644997080888577\n",
      "train loss:0.06586991046718366\n",
      "train loss:0.05967163043157102\n",
      "train loss:0.12176176702268714\n",
      "train loss:0.039987081505448024\n",
      "train loss:0.04981284354180981\n",
      "train loss:0.04190061060714934\n",
      "train loss:0.021162226093024677\n",
      "train loss:0.0781332281615917\n",
      "train loss:0.022845146839913524\n",
      "train loss:0.05197705091623685\n",
      "=== epoch:13, train acc:0.976, test acc:0.946 ===\n",
      "train loss:0.057155536442717286\n",
      "train loss:0.05387521054301943\n",
      "train loss:0.05873733568846656\n",
      "train loss:0.0298648140595134\n",
      "train loss:0.047493805907171636\n",
      "train loss:0.06757521143481274\n",
      "train loss:0.1460746080540854\n",
      "train loss:0.08161926117080703\n",
      "train loss:0.06613520926119207\n",
      "train loss:0.06113841811550535\n",
      "train loss:0.06169915667163708\n",
      "train loss:0.06080256599741502\n",
      "train loss:0.056873010555278096\n",
      "train loss:0.040735331313081505\n",
      "train loss:0.04364694159688085\n",
      "train loss:0.038446968633281176\n",
      "train loss:0.044384490657622976\n",
      "train loss:0.08866347363957852\n",
      "train loss:0.03667293391067104\n",
      "train loss:0.13575150941192432\n",
      "train loss:0.040938622263142044\n",
      "train loss:0.05118085924650735\n",
      "train loss:0.13631716706654015\n",
      "train loss:0.0330529627057661\n",
      "train loss:0.06850125983587584\n",
      "train loss:0.05247305021813151\n",
      "train loss:0.04763980794633399\n",
      "train loss:0.03378319640739465\n",
      "train loss:0.05517025602730047\n",
      "train loss:0.045808539415835334\n",
      "train loss:0.0482139853124685\n",
      "train loss:0.07654006697957066\n",
      "train loss:0.03414571889898192\n",
      "train loss:0.031382688741125556\n",
      "train loss:0.061790474779502995\n",
      "train loss:0.062199620976709086\n",
      "train loss:0.018782845837214304\n",
      "train loss:0.1286680765718579\n",
      "train loss:0.022162779251684375\n",
      "train loss:0.024482692048682666\n",
      "train loss:0.03175495290157698\n",
      "train loss:0.022084267760439272\n",
      "train loss:0.04886845505468464\n",
      "train loss:0.030650191592133134\n",
      "train loss:0.05055126683727098\n",
      "train loss:0.029452306847776395\n",
      "train loss:0.04591260079163299\n",
      "train loss:0.046492949106693694\n",
      "train loss:0.020652640583129652\n",
      "train loss:0.04308434539521838\n",
      "=== epoch:14, train acc:0.979, test acc:0.951 ===\n",
      "train loss:0.053469653578797176\n",
      "train loss:0.036803181239750925\n",
      "train loss:0.034524011995446746\n",
      "train loss:0.0364813507631791\n",
      "train loss:0.04998520196017545\n",
      "train loss:0.049441589845944246\n",
      "train loss:0.026545746759473934\n",
      "train loss:0.016130812152914592\n",
      "train loss:0.10117859939887791\n",
      "train loss:0.03560848160404788\n",
      "train loss:0.02409432366482148\n",
      "train loss:0.09746589272152799\n",
      "train loss:0.03738281796912768\n",
      "train loss:0.049178017698965376\n",
      "train loss:0.045812221354245286\n",
      "train loss:0.018387421547067483\n",
      "train loss:0.013244906459192272\n",
      "train loss:0.04654960082455345\n",
      "train loss:0.0826371368043804\n",
      "train loss:0.11984563613630028\n",
      "train loss:0.02793182575038579\n",
      "train loss:0.0687544014842541\n",
      "train loss:0.033929603788410055\n",
      "train loss:0.021625014064081065\n",
      "train loss:0.03837830884201707\n",
      "train loss:0.07749264161321676\n",
      "train loss:0.02200457911656745\n",
      "train loss:0.04037875090085233\n",
      "train loss:0.02503257549473916\n",
      "train loss:0.033066187186112005\n",
      "train loss:0.09310609495400919\n",
      "train loss:0.03860178005661042\n",
      "train loss:0.026088891794071062\n",
      "train loss:0.03394533742849\n",
      "train loss:0.034627496652689646\n",
      "train loss:0.09726224915588916\n",
      "train loss:0.051944290092643\n",
      "train loss:0.021989386496616588\n",
      "train loss:0.04872287562539196\n",
      "train loss:0.05039164502437852\n",
      "train loss:0.03479185006393381\n",
      "train loss:0.042607611825192224\n",
      "train loss:0.061668331907115424\n",
      "train loss:0.0410655449972099\n",
      "train loss:0.05250583981427398\n",
      "train loss:0.10735718897384493\n",
      "train loss:0.02954302876719976\n",
      "train loss:0.09928414486963201\n",
      "train loss:0.0402165495458264\n",
      "train loss:0.06682473230666953\n",
      "=== epoch:15, train acc:0.98, test acc:0.946 ===\n",
      "train loss:0.023757762012789462\n",
      "train loss:0.07470452371694643\n",
      "train loss:0.0323562115453623\n",
      "train loss:0.02368875798064173\n",
      "train loss:0.054016053704295644\n",
      "train loss:0.05351489614290207\n",
      "train loss:0.04186433553064532\n",
      "train loss:0.06583051706150815\n",
      "train loss:0.044150541929331044\n",
      "train loss:0.05966765545200145\n",
      "train loss:0.02369721390110699\n",
      "train loss:0.0300730945785119\n",
      "train loss:0.056564471324275056\n",
      "train loss:0.03437475477844823\n",
      "train loss:0.05384177888294905\n",
      "train loss:0.021720783215860573\n",
      "train loss:0.04507415299534436\n",
      "train loss:0.02272859452228482\n",
      "train loss:0.043954830136729885\n",
      "train loss:0.033256620007143474\n",
      "train loss:0.023173660779619052\n",
      "train loss:0.05502662805128988\n",
      "train loss:0.07946372743864692\n",
      "train loss:0.03263371128426669\n",
      "train loss:0.021875426389437232\n",
      "train loss:0.03153928110896378\n",
      "train loss:0.05765062961164135\n",
      "train loss:0.0594484842007763\n",
      "train loss:0.03826218067246658\n",
      "train loss:0.025622158613286386\n",
      "train loss:0.035642298851391925\n",
      "train loss:0.04859489253575214\n",
      "train loss:0.014942158393923847\n",
      "train loss:0.023992762077929396\n",
      "train loss:0.011941974416123142\n",
      "train loss:0.06485161917670051\n",
      "train loss:0.056395062730425737\n",
      "train loss:0.013764865225455874\n",
      "train loss:0.05004809460579158\n",
      "train loss:0.03769791485615752\n",
      "train loss:0.024079052366670503\n",
      "train loss:0.018123760614938738\n",
      "train loss:0.055654681322832536\n",
      "train loss:0.056043989987386994\n",
      "train loss:0.018003778820112268\n",
      "train loss:0.03637395147813165\n",
      "train loss:0.039513915631949205\n",
      "train loss:0.026895953012980943\n",
      "train loss:0.028345804470727688\n",
      "train loss:0.0362927430415134\n",
      "=== epoch:16, train acc:0.984, test acc:0.954 ===\n",
      "train loss:0.05254457442639291\n",
      "train loss:0.038941893376747534\n",
      "train loss:0.08022044093520611\n",
      "train loss:0.04479730272553459\n",
      "train loss:0.027930970780222712\n",
      "train loss:0.01310852840048819\n",
      "train loss:0.021095826207890037\n",
      "train loss:0.021850068510793137\n",
      "train loss:0.023768450888719067\n",
      "train loss:0.021553464498731495\n",
      "train loss:0.029222586886523266\n",
      "train loss:0.0477396265911008\n",
      "train loss:0.030022382992914196\n",
      "train loss:0.03573927875751652\n",
      "train loss:0.018224208901781728\n",
      "train loss:0.05022194341555924\n",
      "train loss:0.022451545967610727\n",
      "train loss:0.030816906492557434\n",
      "train loss:0.06758111378229914\n",
      "train loss:0.023248982771920234\n",
      "train loss:0.05357378017867484\n",
      "train loss:0.011348887184895698\n",
      "train loss:0.021044092290331814\n",
      "train loss:0.028166250909603827\n",
      "train loss:0.013782340630752456\n",
      "train loss:0.03533132708062096\n",
      "train loss:0.007315868029553994\n",
      "train loss:0.027001178962433384\n",
      "train loss:0.03926389894896918\n",
      "train loss:0.035542752615502125\n",
      "train loss:0.034924716102263914\n",
      "train loss:0.01569139957359633\n",
      "train loss:0.021300222202952454\n",
      "train loss:0.01553874160567166\n",
      "train loss:0.015101320300815608\n",
      "train loss:0.07781049008904986\n",
      "train loss:0.05366794003629036\n",
      "train loss:0.024908187926730198\n",
      "train loss:0.010678086881369906\n",
      "train loss:0.02251347835980884\n",
      "train loss:0.020860551377219667\n",
      "train loss:0.03747878442608845\n",
      "train loss:0.023830362606093926\n",
      "train loss:0.011479250668335368\n",
      "train loss:0.02311140521328809\n",
      "train loss:0.030369738918905993\n",
      "train loss:0.016895101986311868\n",
      "train loss:0.042404529058070316\n",
      "train loss:0.06419047237864063\n",
      "train loss:0.013470881399163241\n",
      "=== epoch:17, train acc:0.986, test acc:0.96 ===\n",
      "train loss:0.009641089623018509\n",
      "train loss:0.03542679199308798\n",
      "train loss:0.03933856298018494\n",
      "train loss:0.034524984470301355\n",
      "train loss:0.05955772161929624\n",
      "train loss:0.017513909715709687\n",
      "train loss:0.018590903571630686\n",
      "train loss:0.0418321093360061\n",
      "train loss:0.022509536039334456\n",
      "train loss:0.04317306575741471\n",
      "train loss:0.016417398734691595\n",
      "train loss:0.012991976468951182\n",
      "train loss:0.04451817642269268\n",
      "train loss:0.07536517856713608\n",
      "train loss:0.019143614333231893\n",
      "train loss:0.013439174804463909\n",
      "train loss:0.014289987295783721\n",
      "train loss:0.03540621139485216\n",
      "train loss:0.015057716152805767\n",
      "train loss:0.04487371797818585\n",
      "train loss:0.049686200996742896\n",
      "train loss:0.025129038835968075\n",
      "train loss:0.03185328157777895\n",
      "train loss:0.048609395316236335\n",
      "train loss:0.04437532288007787\n",
      "train loss:0.0224976410459372\n",
      "train loss:0.04853815305306407\n",
      "train loss:0.019667586888712733\n",
      "train loss:0.017553553495577055\n",
      "train loss:0.019757092379657636\n",
      "train loss:0.020043056055228332\n",
      "train loss:0.011885068031107343\n",
      "train loss:0.022139752313265836\n",
      "train loss:0.010511287780545725\n",
      "train loss:0.029194972857836437\n",
      "train loss:0.023988638336658916\n",
      "train loss:0.03140506935676218\n",
      "train loss:0.03943453883532621\n",
      "train loss:0.019093047568341615\n",
      "train loss:0.041684024987459836\n",
      "train loss:0.02122827726905792\n",
      "train loss:0.05597956699689515\n",
      "train loss:0.06938292101127244\n",
      "train loss:0.018152607452247914\n",
      "train loss:0.04901191650752251\n",
      "train loss:0.05875011997083722\n",
      "train loss:0.020415941275627833\n",
      "train loss:0.032401894904959795\n",
      "train loss:0.010414497605153587\n",
      "train loss:0.037765616469016924\n",
      "=== epoch:18, train acc:0.99, test acc:0.958 ===\n",
      "train loss:0.033255095780655344\n",
      "train loss:0.034487023678252855\n",
      "train loss:0.025722964800134432\n",
      "train loss:0.016453018310373373\n",
      "train loss:0.0469208665757949\n",
      "train loss:0.019382611865626967\n",
      "train loss:0.01576926609159096\n",
      "train loss:0.01694768693746001\n",
      "train loss:0.02999254751975795\n",
      "train loss:0.034512513911071935\n",
      "train loss:0.03373050596141297\n",
      "train loss:0.009354182862584957\n",
      "train loss:0.013915856988684035\n",
      "train loss:0.04718102948773708\n",
      "train loss:0.010174236129207023\n",
      "train loss:0.034368839260201546\n",
      "train loss:0.0318040425745316\n",
      "train loss:0.01820748119750393\n",
      "train loss:0.011290139008150403\n",
      "train loss:0.014113058808405127\n",
      "train loss:0.05240240207668481\n",
      "train loss:0.039833438412569276\n",
      "train loss:0.02078176942136713\n",
      "train loss:0.015611393165091607\n",
      "train loss:0.016919773894768446\n",
      "train loss:0.03513185681321996\n",
      "train loss:0.024818824254140785\n",
      "train loss:0.05245617333538037\n",
      "train loss:0.023068788640362034\n",
      "train loss:0.02110491072607136\n",
      "train loss:0.01024294321839855\n",
      "train loss:0.016988959736355294\n",
      "train loss:0.01914079203523642\n",
      "train loss:0.020953478892490583\n",
      "train loss:0.007503624025762252\n",
      "train loss:0.028166800853059407\n",
      "train loss:0.012172715698749932\n",
      "train loss:0.007134101204726986\n",
      "train loss:0.017914939564814037\n",
      "train loss:0.009385808720223078\n",
      "train loss:0.023124360339455773\n",
      "train loss:0.007880949394093508\n",
      "train loss:0.01804000599570068\n",
      "train loss:0.044928245149511825\n",
      "train loss:0.016334427536448693\n",
      "train loss:0.02418943637282565\n",
      "train loss:0.04034911041101263\n",
      "train loss:0.018961699732991628\n",
      "train loss:0.010645472636094121\n",
      "train loss:0.009322828621807921\n",
      "=== epoch:19, train acc:0.996, test acc:0.954 ===\n",
      "train loss:0.01994638131422578\n",
      "train loss:0.025154366124601864\n",
      "train loss:0.018458992284826007\n",
      "train loss:0.02093389628538373\n",
      "train loss:0.025846307219684173\n",
      "train loss:0.029378049420937115\n",
      "train loss:0.0375461648588996\n",
      "train loss:0.018116500797279705\n",
      "train loss:0.01500287304812465\n",
      "train loss:0.008597751012097182\n",
      "train loss:0.01722405145518652\n",
      "train loss:0.0249565496695647\n",
      "train loss:0.01919614287094107\n",
      "train loss:0.02239023978016207\n",
      "train loss:0.011412124632104743\n",
      "train loss:0.011281730515756419\n",
      "train loss:0.02229948546180075\n",
      "train loss:0.0184751655497771\n",
      "train loss:0.00491593629863068\n",
      "train loss:0.009681741404092866\n",
      "train loss:0.008538253198998624\n",
      "train loss:0.014549901931670545\n",
      "train loss:0.024966691438157208\n",
      "train loss:0.037546020969751626\n",
      "train loss:0.03404298480042482\n",
      "train loss:0.009633064168598876\n",
      "train loss:0.008972378460493825\n",
      "train loss:0.022184504855127886\n",
      "train loss:0.011919479370564862\n",
      "train loss:0.02509483285857336\n",
      "train loss:0.026582806066675256\n",
      "train loss:0.01867949322023055\n",
      "train loss:0.008031758706977301\n",
      "train loss:0.01914867027359612\n",
      "train loss:0.021794340941284532\n",
      "train loss:0.006053062216770991\n",
      "train loss:0.014533767022369904\n",
      "train loss:0.01334230626539831\n",
      "train loss:0.015813551254802403\n",
      "train loss:0.023442422824542897\n",
      "train loss:0.05778856988222883\n",
      "train loss:0.009295566897906313\n",
      "train loss:0.012227081007839964\n",
      "train loss:0.020831808382019014\n",
      "train loss:0.014122420987416322\n",
      "train loss:0.036879359040512726\n",
      "train loss:0.016635435856220706\n",
      "train loss:0.012302287889677804\n",
      "train loss:0.03163384293149083\n",
      "train loss:0.018049193995717576\n",
      "=== epoch:20, train acc:0.992, test acc:0.953 ===\n",
      "train loss:0.009011280410692366\n",
      "train loss:0.013618594836362326\n",
      "train loss:0.03840229707760591\n",
      "train loss:0.020076717363936524\n",
      "train loss:0.007962878681186877\n",
      "train loss:0.017134904401944543\n",
      "train loss:0.018666512794490588\n",
      "train loss:0.022304704177225642\n",
      "train loss:0.007438822462041374\n",
      "train loss:0.004931919014235463\n",
      "train loss:0.0057247179674238344\n",
      "train loss:0.008651572078010582\n",
      "train loss:0.011761010926194608\n",
      "train loss:0.030291271431588506\n",
      "train loss:0.0066509170841591\n",
      "train loss:0.024240640976633955\n",
      "train loss:0.015114742609087002\n",
      "train loss:0.007271452989387242\n",
      "train loss:0.010658304924011555\n",
      "train loss:0.015200990409729173\n",
      "train loss:0.021913772522826513\n",
      "train loss:0.051140801175243125\n",
      "train loss:0.007391620632812242\n",
      "train loss:0.012150366883389559\n",
      "train loss:0.052676513024925854\n",
      "train loss:0.01711365635404097\n",
      "train loss:0.011596062758378923\n",
      "train loss:0.012258923013711613\n",
      "train loss:0.011448652631750903\n",
      "train loss:0.017729908910496646\n",
      "train loss:0.01378297092252196\n",
      "train loss:0.007025770197817329\n",
      "train loss:0.011536968730643339\n",
      "train loss:0.0032152068238439093\n",
      "train loss:0.009807645025680019\n",
      "train loss:0.03586410695129752\n",
      "train loss:0.007855567936389389\n",
      "train loss:0.04801541697427123\n",
      "train loss:0.011168903131850656\n",
      "train loss:0.014181603126605545\n",
      "train loss:0.017385872162865926\n",
      "train loss:0.05839833672108683\n",
      "train loss:0.011837143432741868\n",
      "train loss:0.011835719368144157\n",
      "train loss:0.013028973333395649\n",
      "train loss:0.022574955715313177\n",
      "train loss:0.014655528618921358\n",
      "train loss:0.035598298853771554\n",
      "train loss:0.008546000709906342\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.958\n",
      "Saved Network Parameters!\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %load train_convnet.py\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 为了导入父目录的文件而进行的设定\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mnist import load_mnist\n",
    "from simple_convnet import SimpleConvNet\n",
    "from common.trainer import Trainer\n",
    "\n",
    "# 读入数据\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "# 处理花费时间较长的情况下减少数据 \n",
    "x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "                        \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "# 保存参数\n",
    "network.save_params(\"params.pkl\")\n",
    "print(\"Saved Network Parameters!\")\n",
    "\n",
    "# 绘制图形\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weird-arrow",
   "metadata": {},
   "source": [
    "## 7.6 CNN的可视化 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ready-arena",
   "metadata": {},
   "source": [
    "上例中第一层的卷积层的权重的形状是（30,1,5,5），意味着滤波器可以可视化为1通道的灰度图像。\\\n",
    "学习前的滤波器是随机进行初始化的，但学习后的滤波器变成了有规律的图像，比如从白到黑渐变的滤波器、含有块状区域的滤波器等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "spread-quantity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 30 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 30 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %load visualize_filter.py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from simple_convnet import SimpleConvNet\n",
    "\n",
    "def filter_show(filters, nx=8, margin=3, scale=10):\n",
    "    \"\"\"\n",
    "    c.f. https://gist.github.com/aidiary/07d530d5e08011832b12#file-draw_weight-py\n",
    "    \"\"\"\n",
    "    FN, C, FH, FW = filters.shape\n",
    "    ny = int(np.ceil(FN / nx))\n",
    "\n",
    "    fig = plt.figure()\n",
    "    fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "\n",
    "    for i in range(FN):\n",
    "        ax = fig.add_subplot(ny, nx, i+1, xticks=[], yticks=[])\n",
    "        ax.imshow(filters[i, 0], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "network = SimpleConvNet()\n",
    "# 随机进行初始化后的权重\n",
    "filter_show(network.params['W1'])\n",
    "\n",
    "# 学习后的权重\n",
    "network.load_params(\"params.pkl\")\n",
    "filter_show(network.params['W1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sitting-restaurant",
   "metadata": {},
   "source": [
    "最开始的层对简单的边缘有响应，接下来的层对纹理有响应，再后面的层对更加复杂的物体部件有响应。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "waiting-indianapolis",
   "metadata": {},
   "source": [
    "## 7.7 具有代表性的CNN "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endangered-occupation",
   "metadata": {},
   "source": [
    "- LeNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "august-capture",
   "metadata": {},
   "source": [
    "LeNet有连续的卷积层和池化层（只抽选元素的子采样层）。LeNet使用sigmoid函数，而现在的CNN中主要使用ReLU函数，LeNet使用子采样（sampling）缩小中间数据的大小，而现在的CNN中Max池化是主流。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "streaming-chapel",
   "metadata": {},
   "source": [
    "- AlexNet\n",
    "\n",
    "AlexNet叠有多个卷积层和池化层，最后经由全输出层输出结果。与AlexNet有以下几点差异：\n",
    "激活函数使用ReLU；使用局部正规化的LRN（Local Response Normalization）层\n",
    "；使用Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjustable-witch",
   "metadata": {},
   "source": [
    "- CNN在此前的全连接层的网络中新增了卷积层和池化层\n",
    "- 使用im2col函数可以简单、高效地实现卷积层和池化层\n",
    "- 通过CNN的可视化，可随着层次变深，提取的信息越加高级\n",
    "- LeNet和AlexNet是CNN的代表性网络\n",
    "- 在深度学习的发展中，大数据和GPU做出来很大的贡献"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
