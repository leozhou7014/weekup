{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "warming-stretch",
   "metadata": {},
   "source": [
    "# 第5章 误差反向传播算法（2020/02/18-） "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forward-treasury",
   "metadata": {},
   "source": [
    "- 通过使用计算图，可以直观的把握计算过程。\n",
    "- 计算图的节点是由局部计算构成的。局部计算构成全局计算。\n",
    "- 计算图的正向传播进行一般的计算。通过计算图的反向传播，可以计算各个节点的导数。\n",
    "- 通过将神经网络组成的元素实现为层，可以高效地计算梯度（反向传播法）。\n",
    "- 通过比较数值微分和误差反向传播法的结果，可以确认误差反向传播法的实现是否正确。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formed-owner",
   "metadata": {},
   "source": [
    "通过数值微分计算神经网络权重参数的梯度虽然容易实现，但是计算上比较费时间，误差反向传播法可以高效地计算损失函数关于权重参数的梯度。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promotional-martial",
   "metadata": {},
   "source": [
    "## 5.1-5.3 计算图、链式法则、反向传播 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "permanent-ecuador",
   "metadata": {},
   "source": [
    " - 计算图采用的“从左向右进行计算”是一种正方向的传播，简称为正向传播\n",
    " - 计算图可以通过传递“局部计算”来获得最终结果，各节点只需进行与自己有关的计算\n",
    " \n",
    " - 反向传播将局部导数向正方向的反方向（从右到左）传递，传递局部导数的原理，是基于链式法则的\n",
    " - 加法节点的反向传播只乘1，所以输入的值会原封不动流向下一个节点\n",
    " - 乘法节点的反向传播会乘以正向传播时的输入信号的“翻转值”后传递给下游"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "combined-moore",
   "metadata": {},
   "source": [
    "## 5.4 简单层的实现 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adapted-stretch",
   "metadata": {},
   "source": [
    "这里的层是神经网络中功能的单位。比如，负责sigmoid函数的Sigmoid，负责矩阵乘积的Affine等，都是以层为单位的实现\n",
    "\n",
    "层的实现中有两个共同的方法（接口）forward()和backward(),forward对应正向传播，backward对应反向传播"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "derived-stewart",
   "metadata": {},
   "source": [
    "- 乘法层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "earlier-saver",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load layer_naive.py\n",
    "\n",
    "\n",
    "class MulLayer:\n",
    "    def __init__(self):           #初始化两个实例变量x和y\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "\n",
    "    def forward(self, x, y):     \n",
    "        self.x = x\n",
    "        self.y = y                \n",
    "        out = x * y\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):    #将从上游传来的导数乘以正向传播时的翻转值\n",
    "        dx = dout * self.y\n",
    "        dy = dout * self.x\n",
    "\n",
    "        return dx, dy\n",
    "\n",
    "\n",
    "class AddLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        out = x + y\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * 1\n",
    "        dy = dout * 1\n",
    "\n",
    "        return dx, dy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "compliant-spokesman",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\datasets\\\\Deep learning from scratch'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "removable-demographic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apple_price: 200\n",
      "price:220.00000000000003\n"
     ]
    }
   ],
   "source": [
    "# 买苹果\n",
    "\n",
    "apple = 100\n",
    "apple_num = 2\n",
    "tax = 1.1\n",
    "\n",
    "#layer\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "#forward\n",
    "apple_price = mul_apple_layer.forward(apple,apple_num)\n",
    "price = mul_tax_layer.forward(apple_price,tax)\n",
    "\n",
    "print(f\"apple_price: {apple_price}\")\n",
    "print(f\"price:{price}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "waiting-annotation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dapple_price = 1.1, dtax = 200\n",
      "dapple = 2.2, dapple_num = 110.00000000000001\n"
     ]
    }
   ],
   "source": [
    "#backward \n",
    "dprice = 1\n",
    "dapple_price,dtax = mul_tax_layer.backward(dprice)\n",
    "dapple,dapple_num = mul_apple_layer.backward(dapple_price)\n",
    "\n",
    "print(f\"dapple_price = {dapple_price}, dtax = {dtax}\")\n",
    "print(f\"dapple = {dapple}, dapple_num = {dapple_num}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "indoor-assistant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "price: 715\n",
      "dApple: 2.2\n",
      "dApple_num: 110\n",
      "dOrange: 3.3000000000000003\n",
      "dOrange_num: 165\n",
      "dTax: 650\n"
     ]
    }
   ],
   "source": [
    "# %load buy_apple_orange.py\n",
    "# from layer_naive import *\n",
    "\n",
    "apple = 100\n",
    "apple_num = 2\n",
    "orange = 150\n",
    "orange_num = 3\n",
    "tax = 1.1\n",
    "\n",
    "# layer\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_orange_layer = MulLayer()\n",
    "add_apple_orange_layer = AddLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "# forward\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)  # (1)\n",
    "orange_price = mul_orange_layer.forward(orange, orange_num)  # (2)\n",
    "all_price = add_apple_orange_layer.forward(apple_price, orange_price)  # (3)\n",
    "price = mul_tax_layer.forward(all_price, tax)  # (4)\n",
    "\n",
    "# backward\n",
    "dprice = 1\n",
    "dall_price, dtax = mul_tax_layer.backward(dprice)  # (4)\n",
    "dapple_price, dorange_price = add_apple_orange_layer.backward(dall_price)  # (3)\n",
    "dorange, dorange_num = mul_orange_layer.backward(dorange_price)  # (2)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)  # (1)\n",
    "\n",
    "print(\"price:\", int(price))\n",
    "print(\"dApple:\", dapple)\n",
    "print(\"dApple_num:\", int(dapple_num))\n",
    "print(\"dOrange:\", dorange)\n",
    "print(\"dOrange_num:\", int(dorange_num))\n",
    "print(\"dTax:\", dtax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guided-parade",
   "metadata": {},
   "source": [
    "## 5.5 激活函数层的实现 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "canadian-chaos",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load layers.py\n",
    "import numpy as np\n",
    "from common.functions import *\n",
    "from common.util import im2col, col2im\n",
    "\n",
    "\n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "\n",
    "        return dx\n",
    "\n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = sigmoid(x)\n",
    "        self.out = out\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "\n",
    "        return dx\n",
    "\n",
    "\n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W =W\n",
    "        self.b = b\n",
    "        \n",
    "        self.x = None\n",
    "        self.original_x_shape = None\n",
    "        # 权重和偏置参数的导数\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 对应张量\n",
    "        self.original_x_shape = x.shape\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        self.x = x\n",
    "\n",
    "        out = np.dot(self.x, self.W) + self.b\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        \n",
    "        dx = dx.reshape(*self.original_x_shape)  # 还原输入数据的形状（对应张量）\n",
    "        return dx\n",
    "\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None # softmax的输出\n",
    "        self.t = None # 监督数据\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        \n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        if self.t.size == self.y.size: # 监督数据是one-hot-vector的情况\n",
    "            dx = (self.y - self.t) / batch_size\n",
    "        else:\n",
    "            dx = self.y.copy()\n",
    "            dx[np.arange(batch_size), self.t] -= 1\n",
    "            dx = dx / batch_size\n",
    "        \n",
    "        return dx\n",
    "\n",
    "\n",
    "class Dropout:\n",
    "    \"\"\"\n",
    "    http://arxiv.org/abs/1207.0580\n",
    "    \"\"\"\n",
    "    def __init__(self, dropout_ratio=0.5):\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x, train_flg=True):\n",
    "        if train_flg:\n",
    "            self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n",
    "            return x * self.mask\n",
    "        else:\n",
    "            return x * (1.0 - self.dropout_ratio)\n",
    "\n",
    "    def backward(self, dout):\n",
    "        return dout * self.mask\n",
    "\n",
    "\n",
    "class BatchNormalization:\n",
    "    \"\"\"\n",
    "    http://arxiv.org/abs/1502.03167\n",
    "    \"\"\"\n",
    "    def __init__(self, gamma, beta, momentum=0.9, running_mean=None, running_var=None):\n",
    "        self.gamma = gamma\n",
    "        self.beta = beta\n",
    "        self.momentum = momentum\n",
    "        self.input_shape = None # Conv层的情况下为4维，全连接层的情况下为2维  \n",
    "\n",
    "        # 测试时使用的平均值和方差\n",
    "        self.running_mean = running_mean\n",
    "        self.running_var = running_var  \n",
    "        \n",
    "        # backward时使用的中间数据\n",
    "        self.batch_size = None\n",
    "        self.xc = None\n",
    "        self.std = None\n",
    "        self.dgamma = None\n",
    "        self.dbeta = None\n",
    "\n",
    "    def forward(self, x, train_flg=True):\n",
    "        self.input_shape = x.shape\n",
    "        if x.ndim != 2:\n",
    "            N, C, H, W = x.shape\n",
    "            x = x.reshape(N, -1)\n",
    "\n",
    "        out = self.__forward(x, train_flg)\n",
    "        \n",
    "        return out.reshape(*self.input_shape)\n",
    "            \n",
    "    def __forward(self, x, train_flg):\n",
    "        if self.running_mean is None:\n",
    "            N, D = x.shape\n",
    "            self.running_mean = np.zeros(D)\n",
    "            self.running_var = np.zeros(D)\n",
    "                        \n",
    "        if train_flg:\n",
    "            mu = x.mean(axis=0)\n",
    "            xc = x - mu\n",
    "            var = np.mean(xc**2, axis=0)\n",
    "            std = np.sqrt(var + 10e-7)\n",
    "            xn = xc / std\n",
    "            \n",
    "            self.batch_size = x.shape[0]\n",
    "            self.xc = xc\n",
    "            self.xn = xn\n",
    "            self.std = std\n",
    "            self.running_mean = self.momentum * self.running_mean + (1-self.momentum) * mu\n",
    "            self.running_var = self.momentum * self.running_var + (1-self.momentum) * var            \n",
    "        else:\n",
    "            xc = x - self.running_mean\n",
    "            xn = xc / ((np.sqrt(self.running_var + 10e-7)))\n",
    "            \n",
    "        out = self.gamma * xn + self.beta \n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        if dout.ndim != 2:\n",
    "            N, C, H, W = dout.shape\n",
    "            dout = dout.reshape(N, -1)\n",
    "\n",
    "        dx = self.__backward(dout)\n",
    "\n",
    "        dx = dx.reshape(*self.input_shape)\n",
    "        return dx\n",
    "\n",
    "    def __backward(self, dout):\n",
    "        dbeta = dout.sum(axis=0)\n",
    "        dgamma = np.sum(self.xn * dout, axis=0)\n",
    "        dxn = self.gamma * dout\n",
    "        dxc = dxn / self.std\n",
    "        dstd = -np.sum((dxn * self.xc) / (self.std * self.std), axis=0)\n",
    "        dvar = 0.5 * dstd / self.std\n",
    "        dxc += (2.0 / self.batch_size) * self.xc * dvar\n",
    "        dmu = np.sum(dxc, axis=0)\n",
    "        dx = dxc - dmu / self.batch_size\n",
    "        \n",
    "        self.dgamma = dgamma\n",
    "        self.dbeta = dbeta\n",
    "        \n",
    "        return dx\n",
    "\n",
    "\n",
    "class Convolution:\n",
    "    def __init__(self, W, b, stride=1, pad=0):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "        # 中间数据（backward时使用）\n",
    "        self.x = None   \n",
    "        self.col = None\n",
    "        self.col_W = None\n",
    "        \n",
    "        # 权重和偏置参数的梯度\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = 1 + int((H + 2*self.pad - FH) / self.stride)\n",
    "        out_w = 1 + int((W + 2*self.pad - FW) / self.stride)\n",
    "\n",
    "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
    "        col_W = self.W.reshape(FN, -1).T\n",
    "\n",
    "        out = np.dot(col, col_W) + self.b\n",
    "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
    "\n",
    "        self.x = x\n",
    "        self.col = col\n",
    "        self.col_W = col_W\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        dout = dout.transpose(0,2,3,1).reshape(-1, FN)\n",
    "\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        self.dW = np.dot(self.col.T, dout)\n",
    "        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)\n",
    "\n",
    "        dcol = np.dot(dout, self.col_W.T)\n",
    "        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n",
    "\n",
    "        return dx\n",
    "\n",
    "\n",
    "class Pooling:\n",
    "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "        self.x = None\n",
    "        self.arg_max = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
    "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
    "\n",
    "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        col = col.reshape(-1, self.pool_h*self.pool_w)\n",
    "\n",
    "        arg_max = np.argmax(col, axis=1)\n",
    "        out = np.max(col, axis=1)\n",
    "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
    "\n",
    "        self.x = x\n",
    "        self.arg_max = arg_max\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout = dout.transpose(0, 2, 3, 1)\n",
    "        \n",
    "        pool_size = self.pool_h * self.pool_w\n",
    "        dmax = np.zeros((dout.size, pool_size))\n",
    "        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n",
    "        dmax = dmax.reshape(dout.shape + (pool_size,)) \n",
    "        \n",
    "        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n",
    "        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        \n",
    "        return dx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convinced-aurora",
   "metadata": {},
   "source": [
    "RELU层的作用就像电路中的开关一样。正向传播时，有电流通过的话，就将开关设为ON；没有电流通过的话，就将开关设为OFF。\n",
    "\n",
    "反向传播时，开关为ON的话，电流会直接通过；开关为OFF的话，则不会有电流通过。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threaded-wisconsin",
   "metadata": {},
   "source": [
    "- Sigmoid层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "polish-hearts",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = sigmoid(x)\n",
    "        self.out = out\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secondary-invasion",
   "metadata": {},
   "source": [
    "Sigmoid层的反向传播，只根据正向传播的输出就能计算出来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "powered-memorial",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affine/Softmax层的实现\n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W =W\n",
    "        self.b = b\n",
    "        \n",
    "        self.x = None\n",
    "        self.original_x_shape = None\n",
    "        # 权重和偏置参数的导数\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 对应张量\n",
    "        self.original_x_shape = x.shape\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        self.x = x\n",
    "\n",
    "        out = np.dot(self.x, self.W) + self.b\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        \n",
    "        dx = dx.reshape(*self.original_x_shape)  # 还原输入数据的形状（对应张量）\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thousand-affair",
   "metadata": {},
   "source": [
    "神经网络的正向传播中进行的矩阵乘积运算在几何学领域被称为“仿射变换”。因此，这里将进行仿射变换的处理实现为Affine层。\n",
    "\n",
    "反向传播时，各数据的反向传播的值需要汇总为偏置的元素。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recreational-lancaster",
   "metadata": {},
   "source": [
    "- Softmax-with-Loss层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "temporal-aviation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None # softmax的输出\n",
    "        self.t = None # 监督数据\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        \n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        if self.t.size == self.y.size: # 监督数据是one-hot-vector的情况\n",
    "            dx = (self.y - self.t) / batch_size\n",
    "        else:\n",
    "            dx = self.y.copy()\n",
    "            dx[np.arange(batch_size), self.t] -= 1\n",
    "            dx = dx / batch_size\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eight-agent",
   "metadata": {},
   "source": [
    "softmax会将输入值正规化之后再输出。\n",
    "\n",
    "神经网络中进行的处理有推理和学习两个阶段。神经网络的推理通常不使用softmax层，神经网络的学习阶段需要softmax层。\n",
    "\n",
    "神经网络中未被正规化的输出结果又是被称为“得分”。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescribed-settle",
   "metadata": {},
   "source": [
    "**反向传播的结果**\n",
    "\n",
    "Softmax层的反向传播得到了$(y_1-t_1,y_2-t_2,y_3-t_3)$这样漂亮的结果，$(y_1-t_1,y_2-t_2,y_3-t_3)$是Softmax层的输出与监督标签的差分。神经网络的反向传播会把这个差分表示的误差传递给前面的层，这是神经网络学习中的重要性质。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinate-weapon",
   "metadata": {},
   "source": [
    "## 5.7 误差反向传播法的实现 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "textile-sensitivity",
   "metadata": {},
   "source": [
    "神经网络中有合适的权重和偏置，调整权重和偏置以便拟合训练数据的过程称为学习。神经网络的学习分为以下4个步骤。\n",
    "\n",
    "- step1: mini-batch 从训练数据中随机选择一部分数据\n",
    "- step2: 计算梯度 计算损失函数关于各个权重参数的梯度\n",
    "- step3: 更新参数 将权重参数沿梯度方向进行微小的更新\n",
    "- step4: 重复步骤1、2、3\n",
    "\n",
    "误差反向传播法会在步骤2中出现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "apparent-devices",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load two_layer_net.py\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 为了导入父目录的文件而进行的设定\n",
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "\n",
    "class TwoLayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 初始化权重\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "    def predict(self, x):\n",
    "        #进行识别（推理）\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "    \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "        \n",
    "    # x:输入数据, t:监督数据\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "    # x:输入数据, t:监督数据\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "        \n",
    "    def gradient(self, x, t):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        grads = {}\n",
    "        \n",
    "        batch_num = x.shape[0]\n",
    "        \n",
    "        # forward\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        # backward\n",
    "        dy = (y - t) / batch_num\n",
    "        grads['W2'] = np.dot(z1.T, dy)\n",
    "        grads['b2'] = np.sum(dy, axis=0)\n",
    "        \n",
    "        da1 = np.dot(dy, W2.T)\n",
    "        dz1 = sigmoid_grad(a1) * da1\n",
    "        grads['W1'] = np.dot(x.T, dz1)\n",
    "        grads['b1'] = np.sum(dz1, axis=0)\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adult-comedy",
   "metadata": {},
   "source": [
    "在确认误差反向传播算法是否正确时，是需要用到数值微分的。确认数值微分求出的梯度结果与误差反向传播算法求出的结果是否一致的操作称为**梯度确认**。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "nominated-norfolk",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1:2.0096590921814724e-10\n",
      "b1:9.026121303959585e-10\n",
      "W2:7.067581857116146e-08\n",
      "b2:1.4177126500536374e-07\n"
     ]
    }
   ],
   "source": [
    "# %load gradient_check.py\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 为了导入父目录的文件而进行的设定\n",
    "import numpy as np\n",
    "from mnist import load_mnist\n",
    "from two_layer_net import TwoLayerNet\n",
    "\n",
    "# 读入数据\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "x_batch = x_train[:3]\n",
    "t_batch = t_train[:3]\n",
    "\n",
    "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
    "grad_backprop = network.gradient(x_batch, t_batch)\n",
    "\n",
    "for key in grad_numerical.keys():\n",
    "    diff = np.average( np.abs(grad_backprop[key] - grad_numerical[key]) )\n",
    "    print(key + \":\" + str(diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "pretty-myanmar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc | 0.10218333333333333, 0.101\n",
      "train acc, test acc | 0.7846, 0.7928\n",
      "train acc, test acc | 0.8741166666666667, 0.878\n",
      "train acc, test acc | 0.8991, 0.8991\n",
      "train acc, test acc | 0.9083166666666667, 0.9098\n",
      "train acc, test acc | 0.9136333333333333, 0.9163\n",
      "train acc, test acc | 0.91915, 0.9216\n",
      "train acc, test acc | 0.9235333333333333, 0.9261\n",
      "train acc, test acc | 0.9278333333333333, 0.9294\n",
      "train acc, test acc | 0.9316666666666666, 0.9332\n",
      "train acc, test acc | 0.9352166666666667, 0.9355\n",
      "train acc, test acc | 0.9367166666666666, 0.9371\n",
      "train acc, test acc | 0.9390666666666667, 0.9391\n",
      "train acc, test acc | 0.9416833333333333, 0.9405\n",
      "train acc, test acc | 0.94315, 0.9436\n",
      "train acc, test acc | 0.94515, 0.946\n",
      "train acc, test acc | 0.9469166666666666, 0.9474\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAArBElEQVR4nO3deXxcdb3/8ddn9uxpk3RNoQUKthShUEqVRbZqyyaIbLKLFBcQ7+WiVQER/XkRFJcrckEFlVVEEJDKXkAvaylFlgINZWm6N03TpmkymZnP74+Z1nSjE8jkpJn38/HIo3OWOeedpecz33PO93vM3RERkeIVCjqAiIgES4VARKTIqRCIiBQ5FQIRkSKnQiAiUuRUCEREilzBCoGZ3Whmy8zs1a0sNzP7pZk1mNm/zGzvQmUREZGtK2SL4PfAlA9YPhUYnfuaBlxXwCwiIrIVBSsE7v4UsPIDVvks8EfPehaoNrOhhcojIiJbFglw38OBBV2mG3PzFm+6oplNI9tqoKysbJ+PfexjvRJQRKS/ePHFF1e4e92WlgVZCPLm7jcANwBMmDDBZ82aFXAiEZHti5m9t7VlQd41tBAY0WW6PjdPRER6UZCF4D7gjNzdQ5OAFnff7LSQiIgUVsFODZnZ7cDBQK2ZNQLfA6IA7v6/wAzgCKABaAPOLlQWERHZuoIVAnc/ZRvLHfhaofYvIiL5Uc9iEZEip0IgIlLkVAhERIqcCoGISJFTIRARKXLbRc9iEZFC81SSTFszqY42Up3tpDvaSXW2s65iRzqjVWRWLyayeBaZzg4ynR14ZzuZVAeLh32a1vggSpteZeiCByCdhFQHlk5i6ST/t+NXWRkdwoimfzB+yV1YJoV5CsukMU9x49DLWRGuZb/mB5i86k5CpAn5v7++WvkrVlLB0M4FHP+ZQzlmz2E9/r2rEIhIYbmTcUimMyRXL6cz2UFnsp3OZAepzk7WRcppi9XRkewksfRFUqnO7FdninSqk6bYMJbFdyTd0cbI5Y+TSXeSSaXIpDvxdCdvxcYwL7wLJclmPrPmL0QyHUTXf3kHM6Kf5rnwXgzvfJ9LO64h5h3ESWa/PMl3/TweSE9igr/K7bH/RxiId4l/TvIiHsvsw+GhF/lt7KebfXuXPes8k9mdo0NPc3X0FpJE6SBCkihJj3Dbgrm85a0cFVnAbuFlZCxMhjBpi5CxGHMXr6Y5mmBQuoS3QyPJWBhCEdzCeChCXVUZ1dFyBvgQBpbGCvIrsuzt/NsPjTUkRS2VhFQ7pDsh3QGpDsBJVo5kXWeazkWv0Ll6GZ0d7SST7aQ62llnCZYM/hSpjDN4wQxibUsh3YlnUpDupDVWx2tDjyOdcfZ693eUtS/FPAWZFJbpZGl8Jx6vO410JsMJC35IRecKQplOwplOQp7iX9E9+V3pF0mmMvym5TwqfA0RUkRJEfEUd2c+xcWd0wB4O34qYdv4mHNjagpXpM4gQQdvJDbvV/o/qWP5aepEamjhxcRXNlv+u8SZ3Ft+IsN9Cf/TNI2kxem0GJ0WpzMUZ8bAM3ip6jAGpZZw/PL/IRVKkA7FSYfjpMMJ5tZOYXnFGKpSK9i1+Sk8HIdIAiIxLBJn9cA9yJTVUZJZS2X7YsLROKFo4t//llQRjUWJhkPEwiGi4RDRsBGNhIiGQkTCRiRkmFlB/iTyZWYvuvuELS1Ti0CkJ3WsgbYmSK6FZBt0Zv9Nj/4MbZ0ZUg1P4I2zSLe3kulYSybZSibVyXN7/j/akmnGvHEtOy57lHC6nXCmg1Cmk3Yr4Vv1t7CuM83XV3yfSR1Pb7TLRq/lgI5fAvDH6H9zUPiVjZa/mannK8kKAO6OXcdeoYaNls/K7MqVc8YAcFfsUUbYYlJESBMmbWHm08kjy5cSDRtHdbYQt/bcp9kyMhZhbaSagWUxYuEQ8/gkMUtDOIKHYhCOEq4YyzcGjSYWCfHs4umEwxEsEiMUiRGOxBhbNYqbB40jFoKG5bcQjUSJRKNEo1Gi0RhnVg3j3AHDiVkGWvaDUPYT8/qvc2JlnBMtAXewz292UDtno6kjNvuVTdpo6oBt/IJ328by7ZNaBFIc3MEse6BetSD7b3JN9oDduQ5GfxpKB8LC2WTm/o3O9jZSHWtJd7SRSbbx1t6X0hweSM3bd7PTvJsIp9ZlD9bpdiKZDi4bdTtLM1UcueJ3HN96+2a7H9t+I20kuCRyM1+K/J2Uh2gjzjrirPUEhyV/ghPirPCD7BeaS9LipEJxCEVpD5dzW8VZlETDHJB6luG+BIvGCUXihKJxPF7JwsGHkoiGGdbeQDltRGMlRBMJYrESYiXleFV99lNq5xrCISMSjRIKR4lG44QjYSIhIxIKEQkZoVCwn1ylMD6oRaBCIH2Xe/ZA3d4C8QpIVMLaJpg/EzpWQ0crJFtJt6+hbcyJrK7+GOn3n2fgP6+AZCuhzlbCnWuJpNbytz1+wVulezNy8UOc8O5lm+3qvPiPeSG1M1OTD3NF6DesI846YnQQY53H+VLnRbznQ5gcmsUJ4Sezyz1GKpwgE07w55ITyMSrGGfvsIu/B7FSiJYRipcRipezuno3SuMJysIpSmMREokEJfEoZbEwJbEwpbEIpetfR8NEwrqhT3qWCoEEwz37yXvdSoiWQvmg7IH9tXuyB/f2Fry9hc61q2jZ6SiWDTmI5NIGdnv0TMLJ1URTrYQ8DcCfhn6TRxKfoW71q/x304Ub7WaNl/BfnV/mocy+jLP5fCdyG2spoZUEaz1BKyXcmT6Y92w4O8WamRiZTyZWRiZWTiiWPVAnS4cQLymlLBaiLBalLJE9SJfFI5TFw5TFIrnX/54uiYb16Vm2G7pGIB/d+lMrAO89kz0Pvm4l6bVNdK5pYs3AcSwdMZU1ra2Me+AYIh3NxJIthD0FwKO1p3Nn5Vn42hX8Zum/xxpc4yWspoxfv1TGbekQNbTw3eiOG+av8RKS0QrebNqBlrJ1dCRGcmn9TYQSlYRLKoiWlFMej7FfPMKh8TBl8fF0xE+iOh5heCxCeTxCaTzMWfEI8Ugo8At2In2RCkGxc4f2VdDRyprEEJaubif25I8INb1FdO1i4slmEp0tvFr2CX418FusaU9x89LPUco6AMJAyiPcnz6cK1IVgPOraA2rfQSrqKDZy1kXqeLdltGsSLVRFa/gm/W3ESmrJF5aRUVpgspEhL1KohyUiFJVEqWy5EgqE1EqS6KUxyOE9albpKBUCPo7d9JrlrGyaRkLIyNY0tLOoDm/onLFS5SuW0R1cgml3sYLPoYTOi4F4K+xhyijnfk+kGZGsTZSxfzkbjS1JqksiXDd8B8RSZQTKq0hXF5LSVkFlSUxbkhEqCyJUpE4MHsgT0QpT+hALtLXqRD0E6lkO7MXtjHrvZXsOO9mdlz5f1QnF1ObXkaCJOsydRyb/AUAv4o+T8KWsDgyiNWJcawrG0579Wi+PeJjDKlK0F75CNWVCfapiFMaC2/hdMqkzQOIyHZLhWB71baS5jf/wdJXHie+8FlqOt7nC+3/S4oIlyfeZpdwM4tiI3m79JOkK0dgA0bx210mMKQqweDKw6kpi+lCp4gAKgTbjzVLSMUqmb2onZYnr2Xyuz9hAFDqEV4PjeaduuO57hNjmThmJFUlRwadVkS2IyoEfZE7NL8L7z/DunlPkXrn/6hoe5+v8R0eah/HHuGBNA04g/guBzJmn4PZq75Od8OIyIemQtAXuMPyNyEcJVU9irlznmaP+7Of6tu9nBcyu/F67BBG7rw31+0xjv1Hf5rKRDTg0CLSX6gQBO2Vu8jM+CahdU08VXUMX2s5jbXtSU6NfJF1Qyay8+4TOPhjg5k8pEKf+kWkIFQIgvTSLfi95/NSZhfuSB/PvLbxHDFuKAfvVsf+o6foU7+I9AoVgqC8/Tjc+zX+kd6De3a7immHjeNj+tQvIgFQIQjI7xqHs6zzFFbsfhY/OXk/dboSkcCoEPS22X/kTy1j+cFDSzlij3P55UnjVQREJFAqBL3pqZ/A4z9gZepoJo/9Br84ebyGGxaRwKkQ9AZ3eOJKePJK7k4fwIs7n8+1XxhPVEVARPoAFYJCc4fHvg///Bl3pj/FAyO/zfWn7Us8Eg46mYgIoEJQeMlW1rx8H/elD2PGiP/id2dMJBFVERCRvkOFoFAyGfA0f3+rle80TWf0DvX8/iwVARHpe1QICiGTgb9dyNJly7hw/unsMaKeG8+eSGlMP24R6Xt0tbKnZdJw79dg9h/5y3sJxgyr5qaz96U8riIgIn2Tjk49KZ2Ce86DV+/i5+kTeHTQmdz6xf00VISI9GkqBD3p/gvh1bv4SfoUHqs9ldu+uB9VpSoCItK3qRD0oNeHHsvdsyM8NfBz3H7ORAaUxYKOJCKyTQW9RmBmU8zsTTNrMLPpW1i+g5nNNLOXzOxfZnZEIfMUROc6eO2vvPheMyc8kGZm1XHc+qVJ1JTHg04mIpKXgrUIzCwMXAtMBhqBF8zsPnd/vctqlwB3uvt1ZjYWmAGMLFSmHpdcC7efgr/zFD/wn1BXsTO3nTuJugoVARHZfhSyRTARaHD3+e6eBO4APrvJOg5U5l5XAYsKmKdndayBW0/A3/0Hl/BVmsp24rZzJzG4MhF0MhGRbinkNYLhwIIu043AfpuscznwsJldAJQBh29pQ2Y2DZgGsMMOO/R40G5rXw23fh5vnMV0vs4/E5/iji9NYlh1SdDJRES6Leh+BKcAv3f3euAI4GYz2yyTu9/g7hPcfUJdXV2vh9zMa3fDgueYbhfyZPQgbjt3P0YMLA06lYjIh1LIFsFCYESX6frcvK7OAaYAuPszZpYAaoFlBcz1kS0YdSInhAeTCUX507RJ7FhTFnQkEZEPrZAtgheA0WY2ysxiwMnAfZus8z5wGICZjQESwPICZuoR985ZyJK1GW790n6MqlUREJHtW8EKgbungPOBh4C5ZO8Oes3MrjCzY3KrXQSca2YvA7cDZ7m7FypTT9n71R9yTuk/GD24IugoIiIfWUE7lLn7DLK3hHadd1mX168D+xcyQ49zZ+/mv7MiMTXoJCIiPSLoi8Xbn7UrSHgHyYoR215XRGQ7oELQTamV7wIQGrBjsEFERHqICkE3rVr0NgAldaMCTiIi0jM06Fw3Na9ZQ8arGTBs56CjiIj0CLUIuuml6ilM7Pg1QwcPCjqKiEiPUCHopgXNbYQMDSchIv2GTg110+RXLiJRNopo+Migo4iI9Ai1CLrDnd3WPMeI+Nqgk4iI9BgVgu5oXUqcJCn1IRCRfkSFoBs6VrwDQHjgyGCDiIj0IBWCbli1qAGAkkHqQyAi/YcuFnfDinZjSWYnBg4fHXQUEZEeo0LQDS+VHcglyQE8M2hg0FFERHqMTg11w4LmNmLhEIMr9FxiEek/1CLohlPmnMUOpbsTCmkIahHpP9QiyFcmzfCOBioTqp0i0r+oEORrzRKipEhVqg+BiPQvKgR5alue7UMQqdFzCESkf1EhyNOqRfMAKBuk4adFpH/RCe88LU5V8XJ6X4YNVyEQkf5FLYI8vRwbz1c6/4P6ugFBRxER6VEqBHla2NRCSTTMwLJY0FFERHqUTg3l6cuvnMzeJR/HbErQUUREepRaBPlIpxiQWoaX1ASdRESkx6kQ5MFXLyRCmkyV+hCISP+jQpCH1qXzAYjUjAw2iIhIAagQ5GHVorcBKB+sW0dFpP9RIcjD+6Hh3JiaQu1wPZBGRPofFYI8vGq7ckXqDPUhEJF+SYUgDy1L36U2AZWJaNBRRER6nPoR5OGLb36ZCbHdgSODjiIi0uPUItiWdCcD0itYV1YfdBIRkYJQIdiGzKpGwmRw9SEQkX5KhWAbWhY3ABCt1R1DItI/FbQQmNkUM3vTzBrMbPpW1jnRzF43s9fM7LZC5vkw1heCyiHqQyAi/VPBLhabWRi4FpgMNAIvmNl97v56l3VGA98G9nf3ZjMbVKg8H9bbsTHc0nkqJ6sPgYj0U4VsEUwEGtx9vrsngTuAz26yzrnAte7eDODuywqY50N5PTWc36aPpL62KugoIiIFUchCMBxY0GW6MTevq12BXc3s/8zsWdvKGM9mNs3MZpnZrOXLlxco7pb5otmMLWslEQ336n5FRHpL0BeLI8Bo4GDgFOA3Zla96UrufoO7T3D3CXV1db0a8NR3vsNF0Tt7dZ8iIr0pr0JgZneb2ZFm1p3CsRDoes9lfW5eV43Afe7e6e7vAG+RLQx9Q6qDAZkmOtSHQET6sXwP7L8GvgDMM7MrzWy3PN7zAjDazEaZWQw4Gbhvk3X+SrY1gJnVkj1VND/PTAWXWvk+IRwG7Bh0FBGRgsmrELj7o+5+KrA38C7wqJk9bWZnm9kWB+Bx9xRwPvAQMBe4091fM7MrzOyY3GoPAU1m9jowE7jY3Zs+2rfUc1Yuyt46Gq8dGWwQEZECyvv2UTOrAU4DTgdeAm4FDgDOJPepflPuPgOYscm8y7q8duA/c199zprFDQwCKofuEnQUEZGCyfcawT3AP4BS4Gh3P8bd/+TuFwDlhQwYpLll+/HV5NcZNGxk0FFERAom3xbBL9195pYWuPuEHszTp7zVXsWDPolfDOy3tU5EJO+LxWO73tZpZgPM7KuFidR3VLz/OAdWLCEaDvouWxGRwsn3CHeuu69aP5HrCXxuQRL1IZ9f9GPODD8UdAwRkYLKtxCEzczWT+TGEYoVJlIf0bmOAZlmOirUh0BE+rd8rxE8CPzJzK7PTZ+Xm9dvdax4lzhg1TsEHUVEpKDyLQTfInvw/0pu+hHgtwVJ1EesXNjAUCBRt1PQUURECiqvQuDuGeC63FdRWLN0PkOBKvUhEJF+Lq9CkHtuwH8DY4HE+vnu3m8/Lr9UeSjf6nB+Xa/hJUSkf8v3YvFNZFsDKeAQ4I/ALYUK1RfMb43wWmg3BleWBh1FRKSg8i0EJe7+GGDu/p67Xw4cWbhYwRvxzl0cXfEWoZBte2URke1YvoWgIzcE9TwzO9/MjqMfDy0BcNTy33BU+JmgY4iIFFy+heBCsuMMfR3Yh+zgc2cWKlTgkmup9hY6KnTrqIj0f9u8WJzrPHaSu/8X0AqcXfBUAWtb9g6lQGiACoGI9H/bbBG4e5rscNNFo2nhPABKBvXbm6JERDbIt0PZS2Z2H/BnYO36me5+d0FSBWztkrcBGDBMfQhEpP/LtxAkgCbg0C7zHOiXheDpgcdxZvtgHhimU0Mi0v/l27O4318X6KpxVQero3XUlMeDjiIiUnD59iy+iWwLYCPu/sUeT9QHfHz+DYTK6zCbEnQUEZGCy/fU0N+6vE4AxwGLej5O33DoqruoKj846BgiIr0i31NDf+k6bWa3A/8sSKKAeXsLlb6GVMWIoKOIiPSKD/sMxtHAoJ4M0lesWTwfgNDAkcEGERHpJfleI1jDxtcIlpB9RkG/s3JRA5VAyaBRQUcREekV+Z4aqih0kL5i9cqlpDzEwOHqQyAixSGvU0NmdpyZVXWZrjazYwuWKkDPVE5lt44/MGyYnlUsIsUh32sE33P3lvUT7r4K+F5BEgVsQXMb5SUJKktiQUcREekV+d4+uqWCke97tysHN/yYoaUjgE8HHUVEpFfk2yKYZWbXmNnOua9rgBcLGSwok1ofZ2y433aREBHZTL6F4AIgCfwJuANoB75WqFBB8XXNlLOWdJX6EIhI8cj3rqG1wPQCZwlc88IGBgIR9SEQkSKS711Dj5hZdZfpAWb2UMFSBaR5UQMAZYN3DjiJiEjvyfeCb23uTiEA3L3ZzPpdz+KVq9eS8Bpq6lUIRKR45HuNIGNmGwbnN7ORbGE00u3dsyWfYv+O/2HY0OFBRxER6TX5tgi+C/zTzJ4EDDgQmFawVAFZ0NxGbXmcRDQcdBQRkV6T78XiB81sAtmD/0vAX4F1BcwViM83fJs94zsBhwcdRUSk1+R7sfhLwGPARcB/ATcDl+fxvilm9qaZNZjZVu86MrPjzcxzxSYY7oxrf5HhsbbAIoiIBCHfawQXAvsC77n7IcB4YNUHvcHMwsC1wFRgLHCKmY3dwnoVue0/l3/snpdqbaKUdtJVek6xiBSXfAtBu7u3A5hZ3N3fAHbbxnsmAg3uPt/dk2Q7on12C+v9APgx2U5qgVnR+BYAsZqRQcYQEel1+RaCxlw/gr8Cj5jZvcB723jPcGBB123k5m1gZnsDI9z9gQ/akJlNM7NZZjZr+fLleUbunpbFbwNQrj4EIlJk8r1YfFzu5eVmNhOoAh78KDs2sxBwDXBWHvu/AbgBYMKECQW5bXXZOliT2ZVB9XoOgYgUl26PIOruT+a56kKg66A99bl561UA44AnzAxgCHCfmR3j7rO6m+ujeiE6kV91Xs6bgwf39q5FRAL1YZ9ZnI8XgNFmNsrMYsDJwH3rF7p7i7vXuvtIdx8JPAsEUgQAFjSvY2hVCdFwIX8kIiJ9T8GeKeDuKTM7H3gICAM3uvtrZnYFMMvd7/vgLfSuLzecx+GxMcChQUcREelVBX24jLvPAGZsMu+yrax7cCGzfCB3duycz5LKjwcWQUQkKDoPArSvWkKCJBn1IRCRIqRCADQ1zgMgVjsq4CQiIr1PhQBoWTIfgMoh6kMgIsWnXz6AvrsWJst5Oz2JfUeMDjqKiEivU4sAmBXanf/MXMigmpqgo4iI9DoVAmDJihaGV5cQClnQUUREep1ODQHffPeLzI+PAQ4JOoqISK9TiyCToTa9jExZv3sEs4hIXoq+ELQ1LyJOJ1SP2PbKIiL9UNEXguULss8hSKgPgYgUqaIvBKsX5/oQDFUfAhEpTkVfCOb7UK5PHUmd+hCISJEq+kLwcnoUP7czqKmuDjqKiEggir4QrF32DqMHGLmH44iIFJ2i70fw9YUX0VjyMeAzQUcREQlEUbcIPJ1iUHoZyfL6oKOIiASmqAtBy/JGopbGqvUcAhEpXkVdCJoas30I4nU7BZxERCQ4RV0I1uSeQ1A9TH0IRKR4FXUheCM8mss7z2DwDupDICLFq6gLwasdg7gndjSV5eVBRxERCUxR3z4aWTyHvatKg44hIhKooi4E5y27ggVlewDHBx1FRCQwRXtqyNOd1GVWkKzQ8NMiUtyKthA0LXqHiGUIDdwx6CgiIoEq3kKwsAGA0jo9h0BEilvRFoK1S3N9CIbvEnASEZFgFW0hmBPdm/OS/8HgevUhEJHiVrSF4M22cl4sPYCSkkTQUUREAlW0t4/WLXqcgyuqgo4hIhK4om0RnNb8K05Mzwg6hohI4IqyEKSSHdRlVpCq0HMIRESKshAsWzifsDnhgSODjiIiEriiLATNC7PPISgdpOcQiIgUtBCY2RQze9PMGsxs+haW/6eZvW5m/zKzx8ysV7r5ti17B4CB6kMgIlK4QmBmYeBaYCowFjjFzMZustpLwAR3/zhwF3BVofJ09WzJQRyT/CGDRuiBNCIihWwRTAQa3H2+uyeBO4DPdl3B3We6e1tu8lmgV67ezl8doqlyd6LRWG/sTkSkTytkIRgOLOgy3ZibtzXnAH/f0gIzm2Zms8xs1vLlyz9ysF0b7+bIktc+8nZERPqDPnGx2MxOAyYAV29pubvf4O4T3H1CXV3dR97f8Wtu5nB/+iNvR0SkPyhkIVgIdB3svz43byNmdjjwXeAYd+8oYB4AOtrbqPVm0pXqQyAiAoUtBC8Ao81slJnFgJOB+7quYGbjgevJFoFlBcyywdIFDYTMiagPgYgIUMBC4O4p4HzgIWAucKe7v2ZmV5jZMbnVrgbKgT+b2Rwzu28rm+sxqxZln0NQNkR3DImIQIEHnXP3GcCMTeZd1uX14YXc/5as70NQU68+BCIiUISjj84sm8o3Oofy9FA9mUykr+rs7KSxsZH29vago2x3EokE9fX1RKPRvN9TdIWgcVU7ieqhhMLhoKOIyFY0NjZSUVHByJEjMbOg42w33J2mpiYaGxsZNSr/D7t94vbR3vSJxhs5Ka5bR0X6svb2dmpqalQEusnMqKmp6XZLqugKwZS2+5ngrwcdQ0S2QUXgw/kwP7eiKgRrW9dQyyoy1TsEHUVEpM8oqkKwdME8AKI1I4MNIiJ92qpVq/j1r3/9od57xBFHsGrVqp4NVGBFVQjW9yEoH6znEIjI1n1QIUilUh/43hkzZlBdXV2AVIVTVHcNrWleRodHqB2xa9BRRCRP37//NV5ftLpHtzl2WCXfO3r3rS6fPn06b7/9NnvttReTJ0/myCOP5NJLL2XAgAG88cYbvPXWWxx77LEsWLCA9vZ2LrzwQqZNmwbAyJEjmTVrFq2trUydOpUDDjiAp59+muHDh3PvvfdSUlKy0b7uv/9+fvjDH5JMJqmpqeHWW29l8ODBtLa2csEFFzBr1izMjO9973scf/zxPPjgg3znO98hnU5TW1vLY4899pF/HkVVCJ6MH8pXMjvx2qAR215ZRIrWlVdeyauvvsqcOXMAeOKJJ5g9ezavvvrqhtsyb7zxRgYOHMi6devYd999Of7446mpqdloO/PmzeP222/nN7/5DSeeeCJ/+ctfOO200zZa54ADDuDZZ5/FzPjtb3/LVVddxU9/+lN+8IMfUFVVxSuvvAJAc3Mzy5cv59xzz+Wpp55i1KhRrFy5ske+36IqBAua26gfWIaFiuqMmMh27YM+ufemiRMnbnRv/i9/+UvuueceABYsWMC8efM2KwSjRo1ir732AmCfffbh3Xff3Wy7jY2NnHTSSSxevJhkMrlhH48++ih33HHHhvUGDBjA/fffz0EHHbRhnYEDB/bI91ZUR8SjG6/h7NAWH3kgIvKBysrKNrx+4oknePTRR3nmmWd4+eWXGT9+/Bbv3Y/H4xteh8PhLV5fuOCCCzj//PN55ZVXuP766wPpTV00hcDd2b/jH4wOLwo6ioj0cRUVFaxZs2ary1taWhgwYAClpaW88cYbPPvssx96Xy0tLQwfnn1m1x/+8IcN8ydPnsy11167Ybq5uZlJkybx1FNP8c472THTeurUUNEUgpaWZmpsNV6l6wMi8sFqamrYf//9GTduHBdffPFmy6dMmUIqlWLMmDFMnz6dSZMmfeh9XX755Zxwwgnss88+1NbWbph/ySWX0NzczLhx49hzzz2ZOXMmdXV13HDDDXzuc59jzz335KSTTvrQ++3K3L1HNtRbJkyY4LNmzer2+9565Xl2/ctk5ux3DXtNPacAyUSkp8ydO5cxY8YEHWO7taWfn5m96O4TtrR+8bQIFr8NQOUQ9SEQEemqaArBytVreS8ziLoRo4OOIiLSpxRNIdj14FOY87knqKjVs4pFRLoqmn4Eo2rLGFVbtu0VRUSKTNG0CEREZMtUCEREipwKgYjIJj7KMNQAP//5z2lra+vBRIWlQiAisoliKwRFc7FYRLZjNx25+bzdj4WJ50KyDW49YfPle30Bxp8Ka5vgzjM2Xnb2Ax+4u02Hob766qu5+uqrufPOO+no6OC4447j+9//PmvXruXEE0+ksbGRdDrNpZdeytKlS1m0aBGHHHIItbW1zJw5c6NtX3HFFdx///2sW7eOT37yk1x//fWYGQ0NDXz5y19m+fLlhMNh/vznP7Pzzjvz4x//mFtuuYVQKMTUqVO58soru/nD2zYVAhGRTWw6DPXDDz/MvHnzeP7553F3jjnmGJ566imWL1/OsGHDeOCBbGFpaWmhqqqKa665hpkzZ240ZMR6559/PpdddhkAp59+On/72984+uijOfXUU5k+fTrHHXcc7e3tZDIZ/v73v3Pvvffy3HPPUVpa2mNjC21KhUBE+r4P+gQfK/3g5WU122wBbMvDDz/Mww8/zPjx4wFobW1l3rx5HHjggVx00UV861vf4qijjuLAAw/c5rZmzpzJVVddRVtbGytXrmT33Xfn4IMPZuHChRx33HEAJBIJIDsU9dlnn01paSnQc8NOb0qFQERkG9ydb3/725x33nmbLZs9ezYzZszgkksu4bDDDtvwaX9L2tvb+epXv8qsWbMYMWIEl19+eSDDTm9KF4tFRDax6TDUn/nMZ7jxxhtpbW0FYOHChSxbtoxFixZRWlrKaaedxsUXX8zs2bO3+P711h/0a2traW1t5a677tqwfn19PX/9618B6OjooK2tjcmTJ3PTTTdtuPCsU0MiIr2k6zDUU6dO5eqrr2bu3Ll84hOfAKC8vJxbbrmFhoYGLr74YkKhENFolOuuuw6AadOmMWXKFIYNG7bRxeLq6mrOPfdcxo0bx5AhQ9h33303LLv55ps577zzuOyyy4hGo/z5z39mypQpzJkzhwkTJhCLxTjiiCP40Y9+1OPfb9EMQy0i2w8NQ/3RaBhqERHpFhUCEZEip0IgIn3S9nbauq/4MD83FQIR6XMSiQRNTU0qBt3k7jQ1NW3oh5Av3TUkIn1OfX09jY2NLF++POgo251EIkF9ffcewKVCICJ9TjQaZdSoUUHHKBoFPTVkZlPM7E0zazCz6VtYHjezP+WWP2dmIwuZR0RENlewQmBmYeBaYCowFjjFzMZusto5QLO77wL8DPhxofKIiMiWFbJFMBFocPf57p4E7gA+u8k6nwX+kHt9F3CYmVkBM4mIyCYKeY1gOLCgy3QjsN/W1nH3lJm1ADXAiq4rmdk0YFpustXM3vyQmWo33XYfoVzdo1zd11ezKVf3fJRcO25twXZxsdjdbwBu+KjbMbNZW+tiHSTl6h7l6r6+mk25uqdQuQp5amghMKLLdH1u3hbXMbMIUAU0FTCTiIhsopCF4AVgtJmNMrMYcDJw3ybr3AecmXv9eeBxVw8SEZFeVbBTQ7lz/ucDDwFh4EZ3f83MrgBmuft9wO+Am82sAVhJtlgU0kc+vVQgytU9ytV9fTWbcnVPQXJtd8NQi4hIz9JYQyIiRU6FQESkyBVNIdjWcBdBMLMRZjbTzF43s9fM7MKgM3VlZmEze8nM/hZ0lvXMrNrM7jKzN8xsrpl9IuhMAGb2H7nf4atmdruZdW/4x57LcaOZLTOzV7vMG2hmj5jZvNy/A/pIrqtzv8d/mdk9ZlbdF3J1WXaRmbmZ1faVXGZ2Qe5n9pqZXdVT+yuKQpDncBdBSAEXuftYYBLwtT6Sa70LgblBh9jEL4AH3f1jwJ70gXxmNhz4OjDB3ceRvTmi0Dc+bM3vgSmbzJsOPObuo4HHctO97fdsnusRYJy7fxx4C/h2b4diy7kwsxHAp4H3eztQzu/ZJJeZHUJ2NIY93X134Cc9tbOiKATkN9xFr3P3xe4+O/d6DdmD2vBgU2WZWT1wJPDboLOsZ2ZVwEFk7zbD3ZPuvirQUP8WAUpy/WFKgUVBhHD3p8jegddV16Fc/gAc25uZYMu53P1hd0/lJp8l29co8Fw5PwO+CQRyN81Wcn0FuNLdO3LrLOup/RVLIdjScBd94oC7Xm7k1fHAcwFHWe/nZP8jZALO0dUoYDlwU+6U1W/NrCzoUO6+kOyns/eBxUCLuz8cbKqNDHb3xbnXS4DBQYbZii8Cfw86BICZfRZY6O4vB51lE7sCB+ZGan7SzPbtqQ0XSyHo08ysHPgL8A13X90H8hwFLHP3F4POsokIsDdwnbuPB9YSzGmOjeTOuX+WbKEaBpSZ2WnBptqyXIfNPnXPuJl9l+xp0lv7QJZS4DvAZUFn2YIIMJDsaeSLgTt7apDOYikE+Qx3EQgzi5ItAre6+91B58nZHzjGzN4lexrtUDO7JdhIQLYl1+ju61tNd5EtDEE7HHjH3Ze7eydwN/DJgDN1tdTMhgLk/u2xUwoflZmdBRwFnNpHRhXYmWxBfzn3918PzDazIYGmymoE7vas58m21nvkQnaxFIJ8hrvodblq/jtgrrtfE3Se9dz92+5e7+4jyf6sHnf3wD/huvsSYIGZ7ZabdRjweoCR1nsfmGRmpbnf6WH0gYvYXXQdyuVM4N4As2xgZlPInn48xt3bgs4D4O6vuPsgdx+Z+/tvBPbO/e0F7a/AIQBmtisQo4dGSC2KQpC7ILV+uIu5wJ3u/lqwqYDsJ+/TyX7inpP7OiLoUH3cBcCtZvYvYC/gR8HGgVwL5S5gNvAK2f9XgQxRYGa3A88Au5lZo5mdA1wJTDazeWRbL1f2kVy/AiqAR3J/+//bR3IFbiu5bgR2yt1SegdwZk+1ojTEhIhIkSuKFoGIiGydCoGISJFTIRARKXIqBCIiRU6FQESkyKkQiBSYmR3cl0ZwFdmUCoGISJFTIRDJMbPTzOz5XOem63PPY2g1s5/lxn9/zMzqcuvuZWbPdhlLf0Bu/i5m9qiZvWxms81s59zmy7s8R+HW9WPEmNmVln0exb/MrMeGFRbpDhUCEcDMxgAnAfu7+15AGjgVKANm5cZ/fxL4Xu4tfwS+lRtL/5Uu828FrnX3PcmON7R+1M/xwDfIPg9jJ2B/M6sBjgN2z23nh4X8HkW2RoVAJOswYB/gBTObk5veiezAXn/KrXMLcEDuuQjV7v5kbv4fgIPMrAIY7u73ALh7e5cxdJ5390Z3zwBzgJFAC9AO/M7MPgf0ifF2pPioEIhkGfAHd98r97Wbu1++hfU+7JgsHV1ep4FIbgysiWTHKToKePBDblvkI1EhEMl6DPi8mQ2CDc/53ZHs/5HP59b5AvBPd28Bms3swNz804Enc0+ZazSzY3PbiOfGt9+i3HMoqtx9BvAfZB+9KdLrIkEHEOkL3P11M7sEeNjMQkAn8DWyD7+ZmFu2jOx1BMgO5/y/uQP9fODs3PzTgevN7IrcNk74gN1WAPda9kH3BvxnD39bInnR6KMiH8DMWt29POgcIoWkU0MiIkVOLQIRkSKnFoGISJFTIRARKXIqBCIiRU6FQESkyKkQiIgUuf8PZWL6yLCVeuYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %load train_neuralnet.py\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 为了导入父目录的文件而进行的设定\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mnist import load_mnist\n",
    "from two_layer_net import TwoLayerNet\n",
    "\n",
    "# 读入数据\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "iters_num = 10000  # 适当设定循环的次数\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 计算梯度\n",
    "    #grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    # 更新参数\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))\n",
    "\n",
    "# 绘制图形\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, label='train acc')\n",
    "plt.plot(x, test_acc_list, label='test acc', linestyle='--')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
