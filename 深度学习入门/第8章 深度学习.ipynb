{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "demographic-senior",
   "metadata": {},
   "source": [
    "# 第8章 深度学习 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "freelance-ratio",
   "metadata": {},
   "source": [
    "- 对于绝大多数问题，都可以期待通过加深网络来提高性能\n",
    "- 在最近的图像识别大赛ILSVRC中，基于深度学习的方法独占鳌头，使用的网络也在深化\n",
    "- VGG、GoogleNet、ResNet等是几个著名的网络\n",
    "- 基于GPU、分布式学习、位数精度的缩减，可以实现深度学习的高速化\n",
    "- 深度学习不仅可以用于物体识别，还可以用于物体检查、图像分割\n",
    "- 深度学习的应用包括图像标题的生成、图像的生成、强化学习等。最近，深度学习在自动驾驶上的应用也备受期待。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honey-sherman",
   "metadata": {},
   "source": [
    "## 8.1 加深网路 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contrary-essence",
   "metadata": {},
   "source": [
    "创建一个进行手写数字识别的深度CNN\n",
    "输入-conv-relu-conv-relu-pool(*3)-Affine-ReLU-Dropout-Affine-Dropout-Softmax\n",
    "这个卷积层有如下特点：\n",
    "- 基于3*3的小型滤波器的卷积层\n",
    "- 激活函数是ReLU\n",
    "- 全连接层的后面使用Dropout层\n",
    "- 基于Adam的最优化\n",
    "- 使用He的初始值作为权重初始值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "twenty-ireland",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load deep_convnet.py\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 为了导入父目录的文件而进行的设定\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from common.layers import *\n",
    "\n",
    "\n",
    "class DeepConvNet:\n",
    "    \"\"\"识别率为99%以上的高精度的ConvNet\n",
    "\n",
    "    网络结构如下所示\n",
    "        conv - relu - conv- relu - pool -\n",
    "        conv - relu - conv- relu - pool -\n",
    "        conv - relu - conv- relu - pool -\n",
    "        affine - relu - dropout - affine - dropout - softmax\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=(1, 28, 28),\n",
    "                 conv_param_1 = {'filter_num':16, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_2 = {'filter_num':16, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_3 = {'filter_num':32, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_4 = {'filter_num':32, 'filter_size':3, 'pad':2, 'stride':1},\n",
    "                 conv_param_5 = {'filter_num':64, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_6 = {'filter_num':64, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 hidden_size=50, output_size=10):\n",
    "        # 初始化权重===========\n",
    "        # 各层的神经元平均与前一层的几个神经元有连接（TODO:自动计算）\n",
    "        pre_node_nums = np.array([1*3*3, 16*3*3, 16*3*3, 32*3*3, 32*3*3, 64*3*3, 64*4*4, hidden_size])\n",
    "        wight_init_scales = np.sqrt(2.0 / pre_node_nums)  # 使用ReLU的情况下推荐的初始值\n",
    "        \n",
    "        self.params = {}\n",
    "        pre_channel_num = input_dim[0]\n",
    "        for idx, conv_param in enumerate([conv_param_1, conv_param_2, conv_param_3, conv_param_4, conv_param_5, conv_param_6]):\n",
    "            self.params['W' + str(idx+1)] = wight_init_scales[idx] * np.random.randn(conv_param['filter_num'], pre_channel_num, conv_param['filter_size'], conv_param['filter_size'])\n",
    "            self.params['b' + str(idx+1)] = np.zeros(conv_param['filter_num'])\n",
    "            pre_channel_num = conv_param['filter_num']\n",
    "        self.params['W7'] = wight_init_scales[6] * np.random.randn(64*4*4, hidden_size)\n",
    "        self.params['b7'] = np.zeros(hidden_size)\n",
    "        self.params['W8'] = wight_init_scales[7] * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b8'] = np.zeros(output_size)\n",
    "\n",
    "        # 生成层===========\n",
    "        self.layers = []\n",
    "        self.layers.append(Convolution(self.params['W1'], self.params['b1'], \n",
    "                           conv_param_1['stride'], conv_param_1['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Convolution(self.params['W2'], self.params['b2'], \n",
    "                           conv_param_2['stride'], conv_param_2['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
    "        self.layers.append(Convolution(self.params['W3'], self.params['b3'], \n",
    "                           conv_param_3['stride'], conv_param_3['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Convolution(self.params['W4'], self.params['b4'],\n",
    "                           conv_param_4['stride'], conv_param_4['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
    "        self.layers.append(Convolution(self.params['W5'], self.params['b5'],\n",
    "                           conv_param_5['stride'], conv_param_5['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Convolution(self.params['W6'], self.params['b6'],\n",
    "                           conv_param_6['stride'], conv_param_6['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
    "        self.layers.append(Affine(self.params['W7'], self.params['b7']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Dropout(0.5))\n",
    "        self.layers.append(Affine(self.params['W8'], self.params['b8']))\n",
    "        self.layers.append(Dropout(0.5))\n",
    "        \n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x, train_flg=False):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, Dropout):\n",
    "                x = layer.forward(x, train_flg)\n",
    "            else:\n",
    "                x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x, train_flg=True)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "\n",
    "        acc = 0.0\n",
    "\n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx, train_flg=False)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt)\n",
    "\n",
    "        return acc / x.shape[0]\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        tmp_layers = self.layers.copy()\n",
    "        tmp_layers.reverse()\n",
    "        for layer in tmp_layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 设定\n",
    "        grads = {}\n",
    "        for i, layer_idx in enumerate((0, 2, 5, 7, 10, 12, 15, 18)):\n",
    "            grads['W' + str(i+1)] = self.layers[layer_idx].dW\n",
    "            grads['b' + str(i+1)] = self.layers[layer_idx].db\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "\n",
    "        for i, layer_idx in enumerate((0, 2, 5, 7, 10, 12, 15, 18)):\n",
    "            self.layers[layer_idx].W = self.params['W' + str(i+1)]\n",
    "            self.layers[layer_idx].b = self.params['b' + str(i+1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "compound-snapshot",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.2965710803557386\n",
      "=== epoch:1, train acc:0.111, test acc:0.1 ===\n",
      "train loss:2.2980105708453094\n",
      "train loss:2.26363318381936\n",
      "train loss:2.2858915788913894\n",
      "train loss:2.2835555978606608\n",
      "train loss:2.268136892580566\n",
      "train loss:2.2650452006772084\n",
      "train loss:2.2821945294165498\n",
      "train loss:2.2419515904349816\n",
      "train loss:2.261610549614618\n",
      "train loss:2.2352433683791673\n",
      "train loss:2.2643039273741037\n",
      "train loss:2.2727100602173165\n",
      "train loss:2.2077377866884844\n",
      "train loss:2.234419748852371\n",
      "train loss:2.1989507938843857\n",
      "train loss:2.168210401957887\n",
      "train loss:2.1834008031866734\n",
      "train loss:2.170959386928643\n",
      "train loss:2.2330886479609418\n",
      "train loss:2.2204550232826734\n",
      "train loss:2.1246138375131665\n",
      "train loss:2.185455989219102\n",
      "train loss:2.120743128027156\n",
      "train loss:2.122029809810164\n",
      "train loss:2.116273735984411\n",
      "train loss:2.085262405395451\n",
      "train loss:2.0656611079320912\n",
      "train loss:2.0580873432854347\n",
      "train loss:2.0460421844803833\n",
      "train loss:1.999621431717964\n",
      "train loss:2.0566842625271566\n",
      "train loss:2.001411990330089\n",
      "train loss:2.113641779556038\n",
      "train loss:1.9166790375396179\n",
      "train loss:1.993360239274996\n",
      "train loss:1.9618684687251249\n",
      "train loss:1.962997097005092\n",
      "train loss:1.9948665941417858\n",
      "train loss:1.9702419840532526\n",
      "train loss:1.890359741235242\n",
      "train loss:1.9918493960260406\n",
      "train loss:1.9634887147246243\n",
      "train loss:1.8421759049871307\n",
      "train loss:2.127086175283172\n",
      "train loss:1.9731909866064887\n",
      "train loss:1.98660536997214\n",
      "train loss:1.943145358503783\n",
      "train loss:1.8239525046531102\n",
      "train loss:1.9771214726347688\n",
      "train loss:1.9388416644751638\n",
      "train loss:1.9538006639247876\n",
      "train loss:1.9664961664065925\n",
      "train loss:1.9364003306017492\n",
      "train loss:1.878755330378899\n",
      "train loss:1.9283888767911705\n",
      "train loss:1.9461022654206712\n",
      "train loss:1.9000766790613957\n",
      "train loss:1.9231887130591887\n",
      "train loss:1.6947031361603857\n",
      "train loss:1.8431565660677516\n",
      "train loss:1.916142128718183\n",
      "train loss:1.8877327435720037\n",
      "train loss:1.930947622844606\n",
      "train loss:1.8237861070428714\n",
      "train loss:1.8685676671606382\n",
      "train loss:1.9270031161167156\n",
      "train loss:1.7262978087030796\n",
      "train loss:1.7376996684689172\n",
      "train loss:1.820336890328094\n",
      "train loss:1.6826711387343605\n",
      "train loss:1.9637526001846424\n",
      "train loss:1.660357093978936\n",
      "train loss:1.7419472445125848\n",
      "train loss:1.413832213694969\n",
      "train loss:1.6732047867140993\n",
      "train loss:1.7351198393506244\n",
      "train loss:1.5274678580234835\n",
      "train loss:1.7314024344860757\n",
      "train loss:1.666351987789011\n",
      "train loss:1.7660286555832163\n",
      "train loss:1.7638400617263832\n",
      "train loss:1.7198052550161045\n",
      "train loss:1.7941187700875174\n",
      "train loss:1.8263479513462266\n",
      "train loss:1.5860268957019446\n",
      "train loss:1.7463165815481496\n",
      "train loss:1.5994704377353914\n",
      "train loss:1.828619827279534\n",
      "train loss:1.6543539412173585\n",
      "train loss:1.669651300213032\n",
      "train loss:1.5767905159663271\n",
      "train loss:1.6461780840690352\n",
      "train loss:1.705940927448851\n",
      "train loss:1.8455507680103949\n",
      "train loss:1.537695862342655\n",
      "train loss:1.6127692925321224\n",
      "train loss:1.641463345557148\n",
      "train loss:1.701528297898899\n",
      "train loss:1.6801003952758558\n",
      "train loss:1.6244947885637404\n",
      "train loss:1.5401370726680719\n",
      "train loss:1.5889891681284178\n",
      "train loss:1.7193978932104863\n",
      "train loss:1.8045751009246727\n",
      "train loss:1.5094161420900247\n",
      "train loss:1.6633563015004251\n",
      "train loss:1.6705892238822155\n",
      "train loss:1.6725641513440583\n",
      "train loss:1.5664491565544065\n",
      "train loss:1.6351958337681876\n",
      "train loss:1.6182644119645326\n",
      "train loss:1.5227924766125756\n",
      "train loss:1.6229142252547168\n",
      "train loss:1.6036010155273188\n",
      "train loss:1.5138796213703705\n",
      "train loss:1.5320690119945033\n",
      "train loss:1.5366019726516205\n",
      "train loss:1.4883963133699636\n",
      "train loss:1.4967656634017614\n",
      "train loss:1.5739020178119563\n",
      "train loss:1.615335395147913\n",
      "train loss:1.5855697179959238\n",
      "train loss:1.4359851112646242\n",
      "train loss:1.5884806936286047\n",
      "train loss:1.4682493629395157\n",
      "train loss:1.508082621116534\n",
      "train loss:1.6931719329119155\n",
      "train loss:1.2454577293574602\n",
      "train loss:1.5560449589678162\n",
      "train loss:1.6557310797294753\n",
      "train loss:1.4149975035422913\n",
      "train loss:1.365807716566671\n",
      "train loss:1.3941129964767764\n",
      "train loss:1.4126540122214701\n",
      "train loss:1.3558973242876204\n",
      "train loss:1.5545566231848873\n",
      "train loss:1.421530282767681\n",
      "train loss:1.2822126927017847\n",
      "train loss:1.4066471295863243\n",
      "train loss:1.2331485471097923\n",
      "train loss:1.4961288133117494\n",
      "train loss:1.5443068278717431\n",
      "train loss:1.4713901419770203\n",
      "train loss:1.6036640510181075\n",
      "train loss:1.3535361056800206\n",
      "train loss:1.3817778713821138\n",
      "train loss:1.4939352789824962\n",
      "train loss:1.5560307051595652\n",
      "train loss:1.5094328869428657\n",
      "train loss:1.5892008302314946\n",
      "train loss:1.6164341797236914\n",
      "train loss:1.633201873744075\n",
      "train loss:1.378582344594823\n",
      "train loss:1.2897345229884707\n",
      "train loss:1.3314231160222867\n",
      "train loss:1.349987258364252\n",
      "train loss:1.4960372030090832\n",
      "train loss:1.421856395436134\n",
      "train loss:1.3834666617212152\n",
      "train loss:1.3170419658816157\n",
      "train loss:1.326782434696698\n",
      "train loss:1.1983585632243576\n",
      "train loss:1.6134027481797206\n",
      "train loss:1.4849239888766803\n",
      "train loss:1.3916595087850778\n",
      "train loss:1.6147779010318077\n",
      "train loss:1.2411947695504377\n",
      "train loss:1.1710818586560783\n",
      "train loss:1.3337787638582637\n",
      "train loss:1.3817618399862124\n",
      "train loss:1.6653018089350655\n",
      "train loss:1.3915763250487245\n",
      "train loss:1.47763461379143\n",
      "train loss:1.4148732697493298\n",
      "train loss:1.4889470164091136\n",
      "train loss:1.419391402297723\n",
      "train loss:1.4066282124858804\n",
      "train loss:1.3891268165442352\n",
      "train loss:1.3331805740851481\n",
      "train loss:1.3086050861446141\n",
      "train loss:1.5263827234644338\n",
      "train loss:1.4051288379041629\n",
      "train loss:1.4849944465114329\n",
      "train loss:1.304823729556755\n",
      "train loss:1.510361082142341\n",
      "train loss:1.3480225426049182\n",
      "train loss:1.406218027773772\n",
      "train loss:1.271380183434205\n",
      "train loss:1.6063912398438853\n",
      "train loss:1.3124993653933685\n",
      "train loss:1.4402953021951022\n",
      "train loss:1.4022679184281992\n",
      "train loss:1.4376771704572335\n",
      "train loss:1.4319749998697273\n",
      "train loss:1.3546409498039431\n",
      "train loss:1.3576630337668838\n",
      "train loss:1.3337856434175088\n",
      "train loss:1.429443235702692\n",
      "train loss:1.3665934226513161\n",
      "train loss:1.3779300417746865\n",
      "train loss:1.3421657338961606\n",
      "train loss:1.319668279060292\n",
      "train loss:1.3236612449927778\n",
      "train loss:1.3530206086342875\n",
      "train loss:1.304816674141441\n",
      "train loss:1.3852138625334174\n",
      "train loss:1.2247523186324287\n",
      "train loss:1.3129262469302787\n",
      "train loss:1.3642046920221171\n",
      "train loss:1.1696349157821968\n",
      "train loss:1.4062701887899203\n",
      "train loss:1.527623685859925\n",
      "train loss:1.2577310147854803\n",
      "train loss:1.4531675194633047\n",
      "train loss:1.235973411109386\n",
      "train loss:1.3250062418175\n",
      "train loss:1.118780007912656\n",
      "train loss:1.2418655762743227\n",
      "train loss:1.165624503395153\n",
      "train loss:1.24233664112541\n",
      "train loss:1.3390347974578554\n",
      "train loss:1.4305411679359432\n",
      "train loss:1.3221379014501553\n",
      "train loss:1.550341267623885\n",
      "train loss:1.2906953999526833\n",
      "train loss:1.270563745420574\n",
      "train loss:1.2460435871867974\n",
      "train loss:1.3367723049361344\n",
      "train loss:1.1893735331727129\n",
      "train loss:1.3226905613465203\n",
      "train loss:1.2736241218807942\n",
      "train loss:1.4186172996755908\n",
      "train loss:1.2219570233003036\n",
      "train loss:1.369862945095858\n",
      "train loss:1.147662055711691\n",
      "train loss:1.3793691834105002\n",
      "train loss:1.2212978427622794\n",
      "train loss:1.2976440414635002\n",
      "train loss:1.2118417776319348\n",
      "train loss:1.1796146524943736\n",
      "train loss:1.2775518486797026\n",
      "train loss:1.2875175578339644\n",
      "train loss:1.2685242366248088\n",
      "train loss:1.3169204196508784\n",
      "train loss:1.3741605083021053\n",
      "train loss:1.2237014202223178\n",
      "train loss:1.25523235426159\n",
      "train loss:1.1873871612390907\n",
      "train loss:1.3238218812775024\n",
      "train loss:1.3885646340612545\n",
      "train loss:1.1130724108821661\n",
      "train loss:1.37852975721231\n",
      "train loss:1.300367157939539\n",
      "train loss:1.1214442155754463\n",
      "train loss:1.2547205527188066\n",
      "train loss:1.1835103281323913\n",
      "train loss:1.2378808902458092\n",
      "train loss:1.2087626468532864\n",
      "train loss:1.2410845714663967\n",
      "train loss:1.345512404909247\n",
      "train loss:1.1876607058676028\n",
      "train loss:1.3684575124421394\n",
      "train loss:1.2894554639504943\n",
      "train loss:1.3468042966284615\n",
      "train loss:1.0262367065475555\n",
      "train loss:1.210568275674474\n",
      "train loss:1.5076833211329954\n",
      "train loss:1.2712786631730117\n",
      "train loss:1.3895239159606958\n",
      "train loss:1.2500451381591566\n",
      "train loss:1.2429506296021235\n",
      "train loss:1.2525995949280995\n",
      "train loss:1.300878007678856\n",
      "train loss:1.2706468208382256\n",
      "train loss:1.374513242062959\n",
      "train loss:1.219460162232355\n",
      "train loss:1.3314506344377408\n",
      "train loss:1.5489798042664378\n",
      "train loss:1.1867961374588436\n",
      "train loss:1.4781229547155421\n",
      "train loss:0.98363927861176\n",
      "train loss:1.295711857140957\n",
      "train loss:1.3304544686093254\n",
      "train loss:1.2558604390612977\n",
      "train loss:1.0466884051527885\n",
      "train loss:1.1887080495556406\n",
      "train loss:1.3387443831082138\n",
      "train loss:1.2075841512873853\n",
      "train loss:1.2698429062006489\n",
      "train loss:1.3347280035915787\n",
      "train loss:1.2577970358692967\n",
      "train loss:1.358770896967534\n",
      "train loss:1.2028791683456148\n",
      "train loss:1.313837003716898\n",
      "train loss:1.452614790570095\n",
      "train loss:1.1881921653708403\n",
      "train loss:1.1826281103785643\n",
      "train loss:1.3131175689029038\n",
      "train loss:1.2767160530727766\n",
      "train loss:1.1391977922173784\n",
      "train loss:1.1781943773371113\n",
      "train loss:1.2457697465226527\n",
      "train loss:1.0964196888489592\n",
      "train loss:1.40039253783416\n",
      "train loss:1.287003599842111\n",
      "train loss:1.1937422469676635\n",
      "train loss:1.1561411575555218\n",
      "train loss:1.5213378131675575\n",
      "train loss:1.1713405954639584\n",
      "train loss:1.2219835254622469\n",
      "train loss:1.3046984313017775\n",
      "train loss:1.2055940021410132\n",
      "train loss:1.2006196466616899\n",
      "train loss:1.284715659264544\n",
      "train loss:1.2572848150937568\n",
      "train loss:1.2139836461428133\n",
      "train loss:1.14488193860462\n",
      "train loss:1.1828099114332786\n",
      "train loss:1.2455120095937813\n",
      "train loss:1.2937011433173757\n",
      "train loss:1.2216058650368773\n",
      "train loss:1.0575988210800389\n",
      "train loss:1.0437616199481636\n",
      "train loss:1.2634785613700488\n",
      "train loss:1.2404141928405286\n",
      "train loss:1.304757829304809\n",
      "train loss:1.1427461185310834\n",
      "train loss:1.3396208175143476\n",
      "train loss:1.3439525535472152\n",
      "train loss:1.226690651183774\n",
      "train loss:1.2804999987393522\n",
      "train loss:1.3907273068131973\n",
      "train loss:1.171388814390112\n",
      "train loss:1.1964874629705025\n",
      "train loss:1.2472238120140993\n",
      "train loss:1.1040374332196918\n",
      "train loss:1.39502456908631\n",
      "train loss:1.2053816957558494\n",
      "train loss:1.2187936879571575\n",
      "train loss:1.1571734342373985\n",
      "train loss:1.2107591799646829\n",
      "train loss:1.1629268413701777\n",
      "train loss:1.1103482732687597\n",
      "train loss:1.1970009550302323\n",
      "train loss:1.495903315178168\n",
      "train loss:0.9962473972863982\n",
      "train loss:1.1546382744164587\n",
      "train loss:1.2374252919723254\n",
      "train loss:1.0543027147147763\n",
      "train loss:1.0191590891450526\n",
      "train loss:1.1846760650381167\n",
      "train loss:1.3029425639896752\n",
      "train loss:0.9969501493992144\n",
      "train loss:1.1854062807644583\n",
      "train loss:1.1274607728228658\n",
      "train loss:1.1595338003184879\n",
      "train loss:1.1628894439162203\n",
      "train loss:1.2285329242604592\n",
      "train loss:1.215918581832242\n",
      "train loss:1.281450474696423\n",
      "train loss:1.1934854460407596\n",
      "train loss:1.2954548183157033\n",
      "train loss:1.2676289264091825\n",
      "train loss:1.0773191779304796\n",
      "train loss:1.1068510689818114\n",
      "train loss:1.119755594730879\n",
      "train loss:1.1664497014833604\n",
      "train loss:1.0142310443820035\n",
      "train loss:1.1694379342624595\n",
      "train loss:1.1645324720534584\n",
      "train loss:1.0598110986887812\n",
      "train loss:1.2469740033925907\n",
      "train loss:1.0327668021816216\n",
      "train loss:0.971313501507206\n",
      "train loss:1.029243718821414\n",
      "train loss:1.2497514163562993\n",
      "train loss:1.1003336502265149\n",
      "train loss:1.078424095669936\n",
      "train loss:1.0387271945142904\n",
      "train loss:1.2228347359126712\n",
      "train loss:0.933198419341895\n",
      "train loss:0.9464140217994134\n",
      "train loss:1.1896596660338037\n",
      "train loss:1.1370447092579257\n",
      "train loss:1.401922211862612\n",
      "train loss:1.1266838135801285\n",
      "train loss:1.1633743349163164\n",
      "train loss:1.2404953918673995\n",
      "train loss:1.0929788168836534\n",
      "train loss:1.0017182158370055\n",
      "train loss:1.0707607714069605\n",
      "train loss:0.9880444312251967\n",
      "train loss:1.1886305913519257\n",
      "train loss:1.1753910313978602\n",
      "train loss:1.2290172254521274\n",
      "train loss:1.1721705294269456\n",
      "train loss:1.089771882192508\n",
      "train loss:1.22052720274302\n",
      "train loss:1.16552544942627\n",
      "train loss:1.2224696600233784\n",
      "train loss:1.190655911950875\n",
      "train loss:1.1169314384588325\n",
      "train loss:1.077876084889586\n",
      "train loss:1.1500703529544116\n",
      "train loss:1.2031909872463749\n",
      "train loss:1.2368469382857308\n",
      "train loss:1.0958027390479361\n",
      "train loss:0.9627968965962223\n",
      "train loss:1.06819790566486\n",
      "train loss:1.2375842658353933\n",
      "train loss:1.3064769521520458\n",
      "train loss:0.777419244286932\n",
      "train loss:1.1147932074450033\n",
      "train loss:1.2079383979894316\n",
      "train loss:1.156694652691625\n",
      "train loss:1.2675266913579322\n",
      "train loss:1.004291469467556\n",
      "train loss:1.0970781608546158\n",
      "train loss:1.209034922682217\n",
      "train loss:1.04873102449758\n",
      "train loss:1.0095037093414485\n",
      "train loss:1.0082614325260295\n",
      "train loss:1.2851907699357161\n",
      "train loss:1.0990566736375729\n",
      "train loss:0.9909266219880997\n",
      "train loss:1.2458117668634061\n",
      "train loss:1.1704631749830496\n",
      "train loss:1.1476661621463757\n",
      "train loss:1.138065553206309\n",
      "train loss:0.9894383802035159\n",
      "train loss:1.1511326474414973\n",
      "train loss:1.1332754644647083\n",
      "train loss:0.9517637671061777\n",
      "train loss:0.9251317546896458\n",
      "train loss:1.1605637413285388\n",
      "train loss:0.8308186862028603\n",
      "train loss:1.220372935011616\n",
      "train loss:0.9731899669685704\n",
      "train loss:1.0757574562848355\n",
      "train loss:1.1407842799040135\n",
      "train loss:1.07911363279243\n",
      "train loss:1.2298872658084534\n",
      "train loss:1.3304276003145605\n",
      "train loss:1.048261362993328\n",
      "train loss:1.1265159053853193\n",
      "train loss:1.1540325576970683\n",
      "train loss:0.9377204315817272\n",
      "train loss:1.198286573762562\n",
      "train loss:1.1860653361112017\n",
      "train loss:1.204647195148166\n",
      "train loss:1.253304713767726\n",
      "train loss:1.2026701944249254\n",
      "train loss:1.1721800807579579\n",
      "train loss:1.0890619147190288\n",
      "train loss:1.0833522824784625\n",
      "train loss:1.1849052584037054\n",
      "train loss:1.1828650855815976\n",
      "train loss:1.1837504497638967\n",
      "train loss:1.180359538198277\n",
      "train loss:1.2091662494712554\n",
      "train loss:1.0594202713258776\n",
      "train loss:1.1508094403584384\n",
      "train loss:1.1841172820818258\n",
      "train loss:1.0550958769516816\n",
      "train loss:1.1152593046106087\n",
      "train loss:1.2509957251819221\n",
      "train loss:1.2838363792298322\n",
      "train loss:1.0114715885046255\n",
      "train loss:1.1709927998742216\n",
      "train loss:1.0937890304791067\n",
      "train loss:1.1752388141161758\n",
      "train loss:1.3163503971054757\n",
      "train loss:1.0978446195219353\n",
      "train loss:1.3553567112005498\n",
      "train loss:1.1157204249243347\n",
      "train loss:1.1772669442320867\n",
      "train loss:1.1720605471913703\n",
      "train loss:1.0702969977026773\n",
      "train loss:1.088833510635441\n",
      "train loss:0.9964797444116481\n",
      "train loss:1.094948428660068\n",
      "train loss:1.021068510988259\n",
      "train loss:1.0252560454014326\n",
      "train loss:1.111867987251902\n",
      "train loss:1.1280545230995573\n",
      "train loss:1.0917018891761734\n",
      "train loss:0.8819264377820453\n",
      "train loss:1.31448627653506\n",
      "train loss:1.106389895304971\n",
      "train loss:1.0988531341936127\n",
      "train loss:1.0002084977012462\n",
      "train loss:1.0556586726621373\n",
      "train loss:1.1489256794491083\n",
      "train loss:1.0622143923603315\n",
      "train loss:0.9815393279373743\n",
      "train loss:1.1919907758771684\n",
      "train loss:0.9973914117377306\n",
      "train loss:0.8745268732138283\n",
      "train loss:1.0397973331216217\n",
      "train loss:1.0481547652002947\n",
      "train loss:0.9619358034691827\n",
      "train loss:1.0194129577986095\n",
      "train loss:1.1033098676099746\n",
      "train loss:1.0458718194865526\n",
      "train loss:1.0433019284046363\n",
      "train loss:1.0712253781762442\n",
      "train loss:1.0774362035682508\n",
      "train loss:1.2060557308153887\n",
      "train loss:0.9032018410935927\n",
      "train loss:1.1189319823100397\n",
      "train loss:1.000258642731821\n",
      "train loss:1.0407288456695232\n",
      "train loss:1.1856118422630246\n",
      "train loss:1.3391144595173872\n",
      "train loss:1.093049698182205\n",
      "train loss:0.9601145159082694\n",
      "train loss:0.9702640569561782\n",
      "train loss:1.1446896910831281\n",
      "train loss:1.0230986038210408\n",
      "train loss:1.0864075436658844\n",
      "train loss:1.1524740198952932\n",
      "train loss:0.9799815892102332\n",
      "train loss:1.0683249479385137\n",
      "train loss:0.9824508955911562\n",
      "train loss:0.9279154595986789\n",
      "train loss:1.031069843873276\n",
      "train loss:1.117461902281113\n",
      "train loss:1.04337451701707\n",
      "train loss:1.1824844278516395\n",
      "train loss:0.9483014556637431\n",
      "train loss:1.289292093253815\n",
      "train loss:0.9783190508006113\n",
      "train loss:1.0722116644268147\n",
      "train loss:1.0319177202122414\n",
      "train loss:1.1424628113426534\n",
      "train loss:1.0805841422849511\n",
      "train loss:0.9417824274906227\n",
      "train loss:0.976807885755806\n",
      "train loss:0.9176682432640638\n",
      "train loss:1.038441664053435\n",
      "train loss:1.124362140032716\n",
      "train loss:1.009989471066934\n",
      "train loss:0.883828777586724\n",
      "train loss:1.097328614666442\n",
      "train loss:1.0981833247135269\n",
      "train loss:0.7996508196189275\n",
      "train loss:1.157164476823298\n",
      "train loss:1.131949395141628\n",
      "train loss:1.1347008394971285\n",
      "train loss:1.0483089601645807\n",
      "train loss:1.1234125919284388\n",
      "train loss:0.954262411351834\n",
      "train loss:1.0682164562911725\n",
      "train loss:0.9954855997819666\n",
      "train loss:0.9668185964185642\n",
      "train loss:1.1029149984540842\n",
      "train loss:0.8886550039302081\n",
      "train loss:1.1389599551537275\n",
      "train loss:1.1945663068015253\n",
      "train loss:0.9445517154887243\n",
      "train loss:1.0916114523576246\n",
      "train loss:0.9330947555761929\n",
      "train loss:1.1304173965209923\n",
      "train loss:1.0210828264607184\n",
      "train loss:1.1018631342606888\n",
      "train loss:1.0074279625835514\n",
      "train loss:0.8517827179813879\n",
      "train loss:1.0840678191876683\n",
      "train loss:0.9179475242567514\n",
      "train loss:1.1952711151182585\n",
      "train loss:1.0274971600710554\n",
      "train loss:1.0416854796452704\n",
      "train loss:0.7921127791414306\n",
      "train loss:1.2047798704603447\n",
      "train loss:1.0549894455641151\n",
      "train loss:1.1077616754271684\n",
      "train loss:1.174511920709152\n",
      "train loss:1.108517157992678\n",
      "train loss:1.0649218661724051\n",
      "train loss:1.0844690167636333\n",
      "train loss:1.0948276389233786\n",
      "train loss:0.8593935056522354\n",
      "train loss:0.9352407753396953\n",
      "train loss:0.9270926974516546\n",
      "train loss:1.0653179488519977\n",
      "train loss:1.1946664928052066\n",
      "train loss:1.0565460087673308\n",
      "train loss:1.0319924303556136\n",
      "train loss:1.1125970781299999\n",
      "train loss:1.091906388132476\n",
      "train loss:1.11460553753103\n",
      "train loss:1.2317644643781003\n",
      "train loss:1.0394812700787113\n",
      "train loss:1.0627025656982472\n",
      "train loss:0.9360002913681774\n",
      "train loss:1.1047016905414744\n",
      "train loss:1.0116997195708188\n",
      "train loss:1.00108031040844\n",
      "train loss:1.1069421328943614\n",
      "train loss:1.001716777752697\n",
      "=== epoch:2, train acc:0.974, test acc:0.979 ===\n",
      "train loss:0.981557767795542\n",
      "train loss:1.009183034506038\n",
      "train loss:1.034419722233967\n",
      "train loss:0.9985088381929422\n",
      "train loss:1.169712612459537\n",
      "train loss:0.8877500057425946\n",
      "train loss:1.2125094970341337\n",
      "train loss:1.0327014724281411\n",
      "train loss:0.953814930585061\n",
      "train loss:1.1031998658890287\n",
      "train loss:1.2455052039574699\n",
      "train loss:1.0835822866207416\n",
      "train loss:0.9752584505746469\n",
      "train loss:1.1266997314095075\n",
      "train loss:0.9451515156251932\n",
      "train loss:1.1719914521800272\n",
      "train loss:1.2430637956790982\n",
      "train loss:0.9184081715448629\n",
      "train loss:1.00487030418185\n",
      "train loss:1.163933148067336\n",
      "train loss:1.218891138292247\n",
      "train loss:1.0888628000208906\n",
      "train loss:1.2428568300517064\n",
      "train loss:0.9290056152783989\n",
      "train loss:1.1116246346159653\n",
      "train loss:0.8510658175237027\n",
      "train loss:1.21911938040068\n",
      "train loss:0.9551384598666333\n",
      "train loss:0.9456945809093681\n",
      "train loss:0.9900696035427186\n",
      "train loss:1.0264198860402762\n",
      "train loss:1.203457321629342\n",
      "train loss:1.0490652215074456\n",
      "train loss:1.0276408404078543\n",
      "train loss:0.9965096512640231\n",
      "train loss:1.1266219663297332\n",
      "train loss:1.1285007530105018\n",
      "train loss:1.0948398404821176\n",
      "train loss:0.9221962818715128\n",
      "train loss:0.9039350824663953\n",
      "train loss:0.8751029038688042\n",
      "train loss:0.9729367463081043\n",
      "train loss:1.196796699715335\n",
      "train loss:1.0946594175528879\n",
      "train loss:0.9012379008538237\n",
      "train loss:1.2755066818790515\n",
      "train loss:1.0887779436196154\n",
      "train loss:1.0702387938609006\n",
      "train loss:1.1516689517735106\n",
      "train loss:1.0199299628998317\n",
      "train loss:1.0851050799193842\n",
      "train loss:0.9800094009895944\n",
      "train loss:1.0783214161525898\n",
      "train loss:1.1331795305649057\n",
      "train loss:0.9933061595549004\n",
      "train loss:0.9606400405191401\n",
      "train loss:0.9681327306586155\n",
      "train loss:0.7608087634959212\n",
      "train loss:1.0327676220306425\n",
      "train loss:1.0238782467232967\n",
      "train loss:0.9022885778653965\n",
      "train loss:0.9289804531546905\n",
      "train loss:1.155735111244867\n",
      "train loss:0.8240993970927462\n",
      "train loss:0.9878283788194399\n",
      "train loss:0.9582059699723452\n",
      "train loss:1.011824789792067\n",
      "train loss:1.144761667205442\n",
      "train loss:1.0518160463118778\n",
      "train loss:1.0610695144924747\n",
      "train loss:0.977260616855562\n",
      "train loss:1.069495210861056\n",
      "train loss:0.917334537438803\n",
      "train loss:0.8802713873966769\n",
      "train loss:0.9845289418613433\n",
      "train loss:0.8667836615521487\n",
      "train loss:1.1830784344288632\n",
      "train loss:0.9493300103663093\n",
      "train loss:0.8511363770000243\n",
      "train loss:1.0310408454081423\n",
      "train loss:0.9520530608307044\n",
      "train loss:1.0485974616228484\n",
      "train loss:1.067754728482445\n",
      "train loss:1.0405922186693528\n",
      "train loss:1.1661674515059168\n",
      "train loss:0.9969120366217784\n",
      "train loss:1.0429435646516994\n",
      "train loss:1.0424586399077063\n",
      "train loss:0.8894294359365423\n",
      "train loss:0.9106500397438705\n",
      "train loss:1.1278426237241141\n",
      "train loss:0.995716263276319\n",
      "train loss:0.9865665695199963\n",
      "train loss:1.1896547553132029\n",
      "train loss:1.0787003026270312\n",
      "train loss:1.0564196244559652\n",
      "train loss:0.924193817959326\n",
      "train loss:1.0744187095770157\n",
      "train loss:0.8563099236235662\n",
      "train loss:0.9621805332303371\n",
      "train loss:0.9391402959569125\n",
      "train loss:0.9487518348001894\n",
      "train loss:1.126754965803625\n",
      "train loss:1.063182771227404\n",
      "train loss:1.0526962989670587\n",
      "train loss:0.913126769597563\n",
      "train loss:1.0362730843186494\n",
      "train loss:0.9998998056308968\n",
      "train loss:1.1212480048572555\n",
      "train loss:1.0547013835202108\n",
      "train loss:0.9025697878385239\n",
      "train loss:1.1117230109460705\n",
      "train loss:0.9885690198454051\n",
      "train loss:0.9538332462566861\n",
      "train loss:1.1369996162208225\n",
      "train loss:1.0664607859612694\n",
      "train loss:1.1075857027559792\n",
      "train loss:1.0504196205088747\n",
      "train loss:1.0334616011422713\n",
      "train loss:0.9640470480392817\n",
      "train loss:0.9981586975345002\n",
      "train loss:0.8718986529962908\n",
      "train loss:1.0264567210168958\n",
      "train loss:0.8292962468444365\n",
      "train loss:0.9696343448544091\n",
      "train loss:0.9537617700740694\n",
      "train loss:0.8624570304594915\n",
      "train loss:1.0821397908503385\n",
      "train loss:1.1144472318786491\n",
      "train loss:0.9461379102893042\n",
      "train loss:0.8963089502487324\n",
      "train loss:1.035529610436162\n",
      "train loss:1.0437182287428568\n",
      "train loss:0.8774337655259714\n",
      "train loss:0.9811631890723923\n",
      "train loss:0.9942162607507647\n",
      "train loss:0.9992941065363651\n",
      "train loss:1.0646687790168623\n",
      "train loss:1.0249597935422186\n",
      "train loss:0.8109969355388149\n",
      "train loss:1.0629274284650037\n",
      "train loss:0.9722862051668486\n",
      "train loss:1.0869131647397083\n",
      "train loss:1.0748782680740185\n",
      "train loss:0.9891057040597708\n",
      "train loss:0.9188014339785946\n",
      "train loss:1.1425034303129054\n",
      "train loss:1.1064932544114858\n",
      "train loss:0.9699012088551133\n",
      "train loss:1.1220670066481566\n",
      "train loss:0.9429021309216924\n",
      "train loss:1.0360617239122432\n",
      "train loss:0.9667661459567698\n",
      "train loss:0.932355887542218\n",
      "train loss:0.9122098057897025\n",
      "train loss:1.0330433324449735\n",
      "train loss:1.1612948307435638\n",
      "train loss:1.031600794868203\n",
      "train loss:1.0290404900217762\n",
      "train loss:0.9165923134119666\n",
      "train loss:1.185673093134342\n",
      "train loss:1.0526166673638029\n",
      "train loss:0.9542451081458432\n",
      "train loss:1.0623441487459746\n",
      "train loss:1.1332138727788155\n",
      "train loss:0.8310664431573198\n",
      "train loss:1.1645240521466353\n",
      "train loss:1.028738028530766\n",
      "train loss:1.1881933698631506\n",
      "train loss:1.0562580110040662\n",
      "train loss:1.1621969646698342\n",
      "train loss:1.0248096636504849\n",
      "train loss:0.9498214432269788\n",
      "train loss:0.9086432788524119\n",
      "train loss:0.9810247294329286\n",
      "train loss:1.010277265094803\n",
      "train loss:1.1763871393363694\n",
      "train loss:0.9387649756574146\n",
      "train loss:1.050704518508081\n",
      "train loss:0.8617646077765593\n",
      "train loss:1.1634375976181177\n",
      "train loss:1.0294526608938623\n",
      "train loss:1.0660265375908984\n",
      "train loss:0.935050599211905\n",
      "train loss:1.272674659161512\n",
      "train loss:0.9340459425662673\n",
      "train loss:1.0527737205300525\n",
      "train loss:1.137698995041719\n",
      "train loss:1.0700001863765287\n",
      "train loss:1.177219980296277\n",
      "train loss:1.1160412226621077\n",
      "train loss:0.995820829157865\n",
      "train loss:1.0916772783703044\n",
      "train loss:0.8882460862421598\n",
      "train loss:0.9762649324196044\n",
      "train loss:1.0431319357082562\n",
      "train loss:1.02873345738511\n",
      "train loss:1.0084040232742348\n",
      "train loss:1.2361795547951389\n",
      "train loss:0.9558322787556306\n",
      "train loss:0.9798282408742083\n",
      "train loss:0.9315485699627425\n",
      "train loss:1.0032777397344137\n",
      "train loss:0.9641319692750463\n",
      "train loss:1.0397107508945598\n",
      "train loss:1.0676274378613968\n",
      "train loss:1.1775748234729004\n",
      "train loss:1.178715489297466\n",
      "train loss:1.0272292808977537\n",
      "train loss:1.029335674601037\n",
      "train loss:1.1332191978474464\n",
      "train loss:0.9371532969566101\n",
      "train loss:0.8446001499890649\n",
      "train loss:1.0829380900934682\n",
      "train loss:1.1021005536313344\n",
      "train loss:1.1586742102684027\n",
      "train loss:1.0767388508755513\n",
      "train loss:0.8307646003782895\n",
      "train loss:0.9850678094608495\n",
      "train loss:0.935241740340003\n",
      "train loss:1.1706664810694456\n",
      "train loss:0.8024746743246246\n",
      "train loss:1.1111534181605778\n",
      "train loss:1.074032763614491\n",
      "train loss:0.9599405543645974\n",
      "train loss:1.1802451047026248\n",
      "train loss:1.035687694827213\n",
      "train loss:0.9726539732931018\n",
      "train loss:0.9362978945151582\n",
      "train loss:0.9268968144800875\n",
      "train loss:0.8323083081202399\n",
      "train loss:1.0972839777390844\n",
      "train loss:1.0418907419109453\n",
      "train loss:0.8635574480030113\n",
      "train loss:0.8942572834265645\n",
      "train loss:0.9999011639049776\n",
      "train loss:0.9444708589602498\n",
      "train loss:0.8353161747140676\n",
      "train loss:1.0750911734166195\n",
      "train loss:1.110216855772781\n",
      "train loss:1.0129551976538376\n",
      "train loss:1.0294415945336197\n",
      "train loss:0.9308931429833153\n",
      "train loss:1.1319605905553751\n",
      "train loss:1.039413144493262\n",
      "train loss:0.8865383501366609\n",
      "train loss:0.899752018741524\n",
      "train loss:0.9703179218256675\n",
      "train loss:1.1165901176470476\n",
      "train loss:1.0840152689136235\n",
      "train loss:0.9705239103864589\n",
      "train loss:0.9099023913528391\n",
      "train loss:0.869778416308682\n",
      "train loss:0.8046380681447525\n",
      "train loss:1.000913936252893\n",
      "train loss:0.9163737913898312\n",
      "train loss:1.0105949294296683\n",
      "train loss:1.049279887881327\n",
      "train loss:1.0115130035293232\n",
      "train loss:0.9696885670605968\n",
      "train loss:1.0083686162698096\n",
      "train loss:1.0651185671063461\n",
      "train loss:0.8710140909152898\n",
      "train loss:0.9907200637664114\n",
      "train loss:1.0078936877668918\n",
      "train loss:1.0671027936541573\n",
      "train loss:0.9558149476063165\n",
      "train loss:1.1043453257383968\n",
      "train loss:1.0295202738652534\n",
      "train loss:0.9352737261952613\n",
      "train loss:0.9229619076482675\n",
      "train loss:0.9969198830927071\n",
      "train loss:1.0240441078800213\n",
      "train loss:1.005441525353803\n",
      "train loss:0.8295064998933906\n",
      "train loss:0.9905067970904762\n",
      "train loss:0.9321909929391998\n",
      "train loss:1.0700652300829738\n",
      "train loss:0.7981292360095077\n",
      "train loss:0.9986186410927088\n",
      "train loss:1.0775743916122567\n",
      "train loss:0.9329551664706729\n",
      "train loss:1.0077072442260222\n",
      "train loss:0.9048085256530104\n",
      "train loss:0.8501796294771049\n",
      "train loss:1.007678490727339\n",
      "train loss:0.9011044752368298\n",
      "train loss:1.001671756738324\n",
      "train loss:1.027556921035017\n",
      "train loss:1.0142896734685964\n",
      "train loss:1.142156699506669\n",
      "train loss:0.8917639264749541\n",
      "train loss:0.7940685372428307\n",
      "train loss:1.21831642931009\n",
      "train loss:0.9182594419895324\n",
      "train loss:1.0419615750606326\n",
      "train loss:1.0059748872175016\n",
      "train loss:0.9534139758739038\n",
      "train loss:0.9409133399674096\n",
      "train loss:0.8748017205173652\n",
      "train loss:1.0505037653846743\n",
      "train loss:0.983773668568703\n",
      "train loss:1.0776686547999081\n",
      "train loss:1.0570805941372559\n",
      "train loss:0.9508548100138533\n",
      "train loss:1.0023870021063612\n",
      "train loss:1.0755263725494861\n",
      "train loss:0.9856131890262145\n",
      "train loss:1.0806104951204276\n",
      "train loss:1.071743028951131\n",
      "train loss:0.9799362576415451\n",
      "train loss:0.9612109611780612\n",
      "train loss:0.8525913993793728\n",
      "train loss:1.1663025710415291\n",
      "train loss:0.923440090455205\n",
      "train loss:1.1472227223480274\n",
      "train loss:0.973413533319636\n",
      "train loss:1.175084754517372\n",
      "train loss:1.0921544207872484\n",
      "train loss:0.9879148672893667\n",
      "train loss:1.036897232977689\n",
      "train loss:0.9664998935206297\n",
      "train loss:1.2655920466196502\n",
      "train loss:0.9896890413126849\n",
      "train loss:1.0451127341103306\n",
      "train loss:0.849555851122826\n",
      "train loss:1.2646501784750221\n",
      "train loss:0.8885124760760038\n",
      "train loss:0.9671302711415334\n",
      "train loss:0.8968682336639857\n",
      "train loss:0.9135079399846378\n",
      "train loss:1.0973710167793653\n",
      "train loss:1.049957201831478\n",
      "train loss:1.047514071955663\n",
      "train loss:1.1060219650364247\n",
      "train loss:0.7028679181970969\n",
      "train loss:0.974228265771378\n",
      "train loss:1.0414326812997425\n",
      "train loss:1.1108535877644043\n",
      "train loss:0.9696541331546122\n",
      "train loss:0.957141931629553\n",
      "train loss:1.0272175498351965\n",
      "train loss:0.945724946242354\n",
      "train loss:0.9128368649871912\n",
      "train loss:0.8204729024101661\n",
      "train loss:1.0965265748736748\n",
      "train loss:1.0863613597597082\n",
      "train loss:1.0129495738555756\n",
      "train loss:0.8993597078527842\n",
      "train loss:0.8401526231764717\n",
      "train loss:0.9190032922624997\n",
      "train loss:1.0634751664907662\n",
      "train loss:1.0701624642809207\n",
      "train loss:1.095506924307123\n",
      "train loss:0.9309673651760769\n",
      "train loss:0.869347250347613\n",
      "train loss:0.8880401450958958\n",
      "train loss:0.9200243997468295\n",
      "train loss:0.9057695185144529\n",
      "train loss:0.9351915947266609\n",
      "train loss:0.8682975650568509\n",
      "train loss:0.9446885476934545\n",
      "train loss:1.0519967903211092\n",
      "train loss:1.0006244190271616\n",
      "train loss:0.8217413283426623\n",
      "train loss:1.0115421484289064\n",
      "train loss:1.0667946330501288\n",
      "train loss:0.896883254193143\n",
      "train loss:0.9827922506708624\n",
      "train loss:1.0600394718953614\n",
      "train loss:0.8239207033120813\n",
      "train loss:1.2530435039938619\n",
      "train loss:1.146783576982748\n",
      "train loss:0.8863953005620562\n",
      "train loss:1.0483463076681607\n",
      "train loss:0.8752122152482826\n",
      "train loss:0.8982968983091772\n",
      "train loss:0.875269402491405\n",
      "train loss:0.9326015942461215\n",
      "train loss:0.9928532208462434\n",
      "train loss:0.8942098425341453\n",
      "train loss:0.9518088891278168\n",
      "train loss:0.9012040645583005\n",
      "train loss:0.9872687339107528\n",
      "train loss:0.9995715786201523\n",
      "train loss:1.099024631665423\n",
      "train loss:0.8416914844994343\n",
      "train loss:1.1384821449185054\n",
      "train loss:0.9531277030892262\n",
      "train loss:1.0249425023961085\n",
      "train loss:0.921096980220284\n",
      "train loss:0.7732454683528772\n",
      "train loss:0.9885151880071197\n",
      "train loss:0.9556230647464004\n",
      "train loss:0.9798942009565792\n",
      "train loss:0.991381266835971\n",
      "train loss:0.9783069566822357\n",
      "train loss:0.9201633417658097\n",
      "train loss:0.9880139245477172\n",
      "train loss:0.8651140294298817\n",
      "train loss:1.133839860764439\n",
      "train loss:1.0684569406321216\n",
      "train loss:1.008777203952447\n",
      "train loss:0.9971200116523555\n",
      "train loss:0.8751464408458888\n",
      "train loss:1.2096344704892301\n",
      "train loss:1.083499170462236\n",
      "train loss:0.9198507069231668\n",
      "train loss:0.7712224956493593\n",
      "train loss:0.9558246969015853\n",
      "train loss:0.9771741289086913\n",
      "train loss:1.0385494476624373\n",
      "train loss:1.0801247934300051\n",
      "train loss:1.0275035440696987\n",
      "train loss:0.8676832150771935\n",
      "train loss:0.9136434459450478\n",
      "train loss:0.8933385812340624\n",
      "train loss:0.9664410206095679\n",
      "train loss:1.0154736980316301\n",
      "train loss:0.9732534940663379\n",
      "train loss:0.9973150869099388\n",
      "train loss:0.8011705299587137\n",
      "train loss:1.1325110147894886\n",
      "train loss:0.8867905540603376\n",
      "train loss:0.9699696752580665\n",
      "train loss:1.0149062259157278\n",
      "train loss:1.0296503396233532\n",
      "train loss:0.9940705079491284\n",
      "train loss:1.0844671282720375\n",
      "train loss:1.0098225563531764\n",
      "train loss:0.8399169921121674\n",
      "train loss:0.8983018857213854\n",
      "train loss:1.2368020542232188\n",
      "train loss:1.0805469381358193\n",
      "train loss:0.7937507823221077\n",
      "train loss:1.0768724635286562\n",
      "train loss:1.1710961328241403\n",
      "train loss:1.0235812745985187\n",
      "train loss:1.0776187150648993\n",
      "train loss:0.9196841405165317\n",
      "train loss:1.0190635000726638\n",
      "train loss:0.9734720157739158\n",
      "train loss:1.0376367661044603\n",
      "train loss:0.8432853870273722\n",
      "train loss:1.072346807439\n",
      "train loss:0.8416191225379717\n",
      "train loss:1.0943307618062699\n",
      "train loss:1.0536810111417956\n",
      "train loss:0.949076077421268\n",
      "train loss:1.059140350304191\n",
      "train loss:1.0563187452967444\n",
      "train loss:1.0720396210845218\n",
      "train loss:0.9065398386421886\n",
      "train loss:1.049320733953551\n",
      "train loss:1.1458521205180021\n",
      "train loss:1.0133604712141762\n",
      "train loss:1.0498510165629276\n",
      "train loss:0.9849771563234018\n",
      "train loss:1.0291097029041987\n",
      "train loss:1.0253395864923882\n",
      "train loss:1.0134622177807682\n",
      "train loss:0.9365679497445385\n",
      "train loss:1.0735248244505053\n",
      "train loss:0.9168060067870246\n",
      "train loss:1.013859840262446\n",
      "train loss:0.996293472793878\n",
      "train loss:0.9131837393540435\n",
      "train loss:1.043768782159732\n",
      "train loss:1.1349390268264083\n",
      "train loss:1.0455376738000604\n",
      "train loss:1.0811761662957344\n",
      "train loss:1.1329611845716925\n",
      "train loss:0.8813254399892494\n",
      "train loss:1.0262928179013215\n",
      "train loss:1.040499319471755\n",
      "train loss:0.9030831188858689\n",
      "train loss:1.171488083850053\n",
      "train loss:1.1835678145122408\n",
      "train loss:0.8861223874082066\n",
      "train loss:1.0905926021824182\n",
      "train loss:1.0319164769437585\n",
      "train loss:1.077813622855544\n",
      "train loss:1.0388158921132218\n",
      "train loss:1.1182966558006986\n",
      "train loss:0.8753742222676787\n",
      "train loss:0.8633142837787515\n",
      "train loss:0.921840644802017\n",
      "train loss:1.03635724053729\n",
      "train loss:1.0644268916645323\n",
      "train loss:1.069933089221622\n",
      "train loss:1.0188005951182384\n",
      "train loss:0.9283454153045047\n",
      "train loss:1.1461641999935364\n",
      "train loss:0.9564955930796153\n",
      "train loss:1.005304150927876\n",
      "train loss:0.9623600695553424\n",
      "train loss:1.0579384848547944\n",
      "train loss:0.93656454885665\n",
      "train loss:1.019593819875864\n",
      "train loss:0.867747847685833\n",
      "train loss:0.9166035903239593\n",
      "train loss:1.1119464920305007\n",
      "train loss:0.896529522153443\n",
      "train loss:1.1275944149974064\n",
      "train loss:0.9870045090581405\n",
      "train loss:1.1088224654364112\n",
      "train loss:1.1218984214227035\n",
      "train loss:0.9984758326501338\n",
      "train loss:0.9870248588733455\n",
      "train loss:1.1240817789962725\n",
      "train loss:0.9043700735364345\n",
      "train loss:0.9194550155694751\n",
      "train loss:0.9705638829580441\n",
      "train loss:0.9227644017590133\n",
      "train loss:0.9706331348431024\n",
      "train loss:0.8753427301542793\n",
      "train loss:1.0162343868995796\n",
      "train loss:0.8646313134647207\n",
      "train loss:0.9939774567670092\n",
      "train loss:1.0180461690225249\n",
      "train loss:0.9831702505633253\n",
      "train loss:0.8263703688063362\n",
      "train loss:0.9719863356960264\n",
      "train loss:0.9080403484514542\n",
      "train loss:1.1038839219781738\n",
      "train loss:0.8603639530295443\n",
      "train loss:0.8507314461310627\n",
      "train loss:1.0936795533113526\n",
      "train loss:0.9960217064302319\n",
      "train loss:0.9657571566398753\n",
      "train loss:0.8547504899446997\n",
      "train loss:0.8358580987662152\n",
      "train loss:0.8208058444690419\n",
      "train loss:0.8744584116805716\n",
      "train loss:0.9702976020901163\n",
      "train loss:1.0762604270878413\n",
      "train loss:0.9516471306822385\n",
      "train loss:1.1000927051411002\n",
      "train loss:0.9441379123261894\n",
      "train loss:1.0933781190040839\n",
      "train loss:0.9323114064250265\n",
      "train loss:1.0413982880828467\n",
      "train loss:1.106635697110462\n",
      "train loss:0.8490919147386713\n",
      "train loss:0.967186190938792\n",
      "train loss:0.8936635756318418\n",
      "train loss:0.9795892431940607\n",
      "train loss:1.1028009745756062\n",
      "train loss:0.9867617922587147\n",
      "train loss:0.9629023991585953\n",
      "train loss:0.955247353552746\n",
      "train loss:0.9259659646196424\n",
      "train loss:1.1409962044757613\n",
      "train loss:1.0550973032382023\n",
      "train loss:0.9731395237930662\n",
      "train loss:0.9255410694681181\n",
      "train loss:0.8843063805207605\n",
      "train loss:0.9442611794011105\n",
      "train loss:0.9868120329411713\n",
      "train loss:1.0781976720808057\n",
      "train loss:0.8959582841834296\n",
      "train loss:0.9496789072935178\n",
      "train loss:0.94088649366163\n",
      "train loss:0.8599399188187472\n",
      "train loss:1.033353568306139\n",
      "train loss:0.9423587347236456\n",
      "train loss:1.035174850568605\n",
      "train loss:1.0560317092987939\n",
      "train loss:0.8769508402303017\n",
      "train loss:0.9175646544400736\n",
      "train loss:0.923879857579851\n",
      "train loss:0.9663786913530791\n",
      "train loss:0.8956754736391994\n",
      "train loss:0.9576031117492328\n",
      "train loss:0.9527356126374048\n",
      "train loss:1.0024392651019614\n",
      "train loss:0.9402274887318808\n",
      "train loss:1.0309529106507098\n",
      "train loss:0.967037323173835\n",
      "train loss:0.836363148395637\n",
      "train loss:1.136175139111763\n",
      "train loss:0.9636794531455338\n",
      "train loss:0.8214107182467779\n",
      "train loss:1.170133981290702\n",
      "train loss:0.8231089691844425\n",
      "train loss:0.9897088141643969\n",
      "train loss:0.9816448942811654\n",
      "train loss:1.2186866890031114\n",
      "train loss:0.8277608490431926\n",
      "train loss:0.9001313960426994\n",
      "train loss:0.9480918137999488\n",
      "train loss:1.0572465608007038\n",
      "train loss:0.9436346405811336\n",
      "train loss:0.9613925706421935\n",
      "train loss:0.9515752888766268\n",
      "train loss:0.9157343395858546\n",
      "train loss:0.8306323548148278\n",
      "train loss:0.7952968618102527\n",
      "train loss:1.073704686379943\n",
      "train loss:1.0603924579981931\n",
      "=== epoch:3, train acc:0.987, test acc:0.982 ===\n",
      "train loss:0.9199345559955739\n",
      "train loss:0.9014662951500284\n",
      "train loss:0.824952557505777\n",
      "train loss:1.0332207026436668\n",
      "train loss:1.0607422930660966\n",
      "train loss:1.204587991322689\n",
      "train loss:1.0268629428409057\n",
      "train loss:1.0925255708929682\n",
      "train loss:0.9169924830792046\n",
      "train loss:0.8576091018833827\n",
      "train loss:1.00455782518139\n",
      "train loss:0.9664772316663708\n",
      "train loss:1.1900627575624156\n",
      "train loss:1.0389866573958826\n",
      "train loss:1.0428431746860298\n",
      "train loss:0.935795947364647\n",
      "train loss:1.1470768398214561\n",
      "train loss:0.8916545283526591\n",
      "train loss:0.8966051437310996\n",
      "train loss:0.9184694074926135\n",
      "train loss:0.8921167251843972\n",
      "train loss:0.8699221397108883\n",
      "train loss:0.8851640518154386\n",
      "train loss:0.8965643126999154\n",
      "train loss:1.19050771639862\n",
      "train loss:0.8664470087031213\n",
      "train loss:0.9491869658528721\n",
      "train loss:1.012712369525339\n",
      "train loss:0.9886364606948169\n",
      "train loss:1.0039604414627419\n",
      "train loss:0.931384370014935\n",
      "train loss:0.9565323393097196\n",
      "train loss:0.8817769931258281\n",
      "train loss:0.8512576422584287\n",
      "train loss:0.8872653876860697\n",
      "train loss:0.8514842524504439\n",
      "train loss:1.0866662172302382\n",
      "train loss:1.0553439529568953\n",
      "train loss:1.0932053732665563\n",
      "train loss:0.9488950246523544\n",
      "train loss:0.8923840371923228\n",
      "train loss:0.7637604114095429\n",
      "train loss:0.967988745717016\n",
      "train loss:0.7940003722989162\n",
      "train loss:0.8149723373365443\n",
      "train loss:1.0715228087990414\n",
      "train loss:1.100443872174202\n",
      "train loss:0.9586419844305398\n",
      "train loss:0.9628135538640699\n",
      "train loss:0.9691094890798854\n",
      "train loss:0.8675986025094405\n",
      "train loss:0.9707177514802214\n",
      "train loss:1.0801517806716268\n",
      "train loss:0.960072137797079\n",
      "train loss:1.0167034272711541\n",
      "train loss:0.8632698702166556\n",
      "train loss:0.8211485222911797\n",
      "train loss:0.8519583464621402\n",
      "train loss:1.149496765855912\n",
      "train loss:1.0331221538089788\n",
      "train loss:1.1720226793872248\n",
      "train loss:1.0172435296327342\n",
      "train loss:1.0341954923807783\n",
      "train loss:0.8336953893260141\n",
      "train loss:1.008497070383186\n",
      "train loss:1.0209102350943107\n",
      "train loss:1.0698094925607016\n",
      "train loss:0.8924202068663597\n",
      "train loss:0.9873076792216311\n",
      "train loss:1.0453660762216934\n",
      "train loss:0.9313824420307862\n",
      "train loss:1.0755075224583504\n",
      "train loss:0.9361653298786998\n",
      "train loss:0.9322312035259557\n",
      "train loss:1.1195884855985272\n",
      "train loss:0.8996282125899226\n",
      "train loss:0.9396698097334582\n",
      "train loss:0.9079739163307198\n",
      "train loss:0.7371846395494112\n",
      "train loss:1.0026757764642467\n",
      "train loss:0.9838481456097498\n",
      "train loss:0.9883979222342509\n",
      "train loss:0.9639527193002457\n",
      "train loss:0.9942262875150268\n",
      "train loss:1.045303822000653\n",
      "train loss:1.1406202348613608\n",
      "train loss:1.0271694064252974\n",
      "train loss:1.1206922424657066\n",
      "train loss:0.9708819214940801\n",
      "train loss:0.9275126463213236\n",
      "train loss:0.9653265430143098\n",
      "train loss:0.9290770990486655\n",
      "train loss:0.9424439107852028\n",
      "train loss:0.9633299724736623\n",
      "train loss:0.7068234201969126\n",
      "train loss:0.9180372674770207\n",
      "train loss:0.988849578044511\n",
      "train loss:0.8611455671725144\n",
      "train loss:0.8502137272462917\n",
      "train loss:0.9345337968357144\n",
      "train loss:0.9990461878096789\n",
      "train loss:0.9670475821617935\n",
      "train loss:1.0970095395539012\n",
      "train loss:1.0326540814918823\n",
      "train loss:0.9207015148390244\n",
      "train loss:1.0739251700887187\n",
      "train loss:0.8478437556176894\n",
      "train loss:1.0349340796218331\n",
      "train loss:1.1617579214810896\n",
      "train loss:0.9840856775740147\n",
      "train loss:0.9953167311236121\n",
      "train loss:0.9171621811195628\n",
      "train loss:0.7295188078443771\n",
      "train loss:0.9452906365193706\n",
      "train loss:0.9232944578622542\n",
      "train loss:0.8598794124915726\n",
      "train loss:0.9391384736108705\n",
      "train loss:1.1718618365771787\n",
      "train loss:1.010594471811504\n",
      "train loss:0.9463530320602842\n",
      "train loss:0.8487201177978786\n",
      "train loss:1.0675049533786125\n",
      "train loss:1.035111865640233\n",
      "train loss:0.7640459096827685\n",
      "train loss:0.8952863048781208\n",
      "train loss:0.9479282341313484\n",
      "train loss:0.9381639811558351\n",
      "train loss:0.9937088175206108\n",
      "train loss:1.0520557282425471\n",
      "train loss:1.057922218495451\n",
      "train loss:0.9739576285233185\n",
      "train loss:0.9370068418766464\n",
      "train loss:0.9469861746537569\n",
      "train loss:0.9793985849906133\n",
      "train loss:0.9164099593168129\n",
      "train loss:1.0353139265769702\n",
      "train loss:1.0200744758340023\n",
      "train loss:1.1559067510472238\n",
      "train loss:0.9828022997377048\n",
      "train loss:0.914710933138553\n",
      "train loss:0.8563630702198556\n",
      "train loss:0.8848748728301057\n",
      "train loss:0.8811704037381395\n",
      "train loss:0.9806930799685469\n",
      "train loss:0.9730450132493162\n",
      "train loss:1.055432644924189\n",
      "train loss:0.9048979282737699\n",
      "train loss:1.0791772648362568\n",
      "train loss:0.8601532872358217\n",
      "train loss:1.0324552312721627\n",
      "train loss:1.0254695881845557\n",
      "train loss:0.8693285229770895\n",
      "train loss:0.9546430561048419\n",
      "train loss:0.9952240955942118\n",
      "train loss:0.7969153716303126\n",
      "train loss:0.9286344356036499\n",
      "train loss:0.8968849512456839\n",
      "train loss:1.0856246949278252\n",
      "train loss:0.998279679892104\n",
      "train loss:0.9547510403639223\n",
      "train loss:0.8722487822279374\n",
      "train loss:0.9273905035498277\n",
      "train loss:0.8776152807969716\n",
      "train loss:0.8709944118222559\n",
      "train loss:0.9783554426726045\n",
      "train loss:0.9404530433633369\n",
      "train loss:0.9150460102917002\n",
      "train loss:0.8109186206836634\n",
      "train loss:1.0828279810611718\n",
      "train loss:1.028483724306036\n",
      "train loss:0.7825049531987756\n",
      "train loss:0.9131020050455539\n",
      "train loss:0.970844281032077\n",
      "train loss:0.9647429283250538\n",
      "train loss:0.8096900372798427\n",
      "train loss:0.9051457201801664\n",
      "train loss:1.0722568782486779\n",
      "train loss:0.9308724710454598\n",
      "train loss:0.8781733593681541\n",
      "train loss:0.8894881000934625\n",
      "train loss:1.014778106740805\n",
      "train loss:1.04237345526937\n",
      "train loss:0.8873396183418314\n",
      "train loss:0.9481415558921351\n",
      "train loss:0.9827787567006141\n",
      "train loss:0.9270401009366658\n",
      "train loss:1.1579232301421911\n",
      "train loss:0.9919210955590035\n",
      "train loss:0.9747298258429486\n",
      "train loss:1.0195960788754699\n",
      "train loss:0.973726210615983\n",
      "train loss:0.9408621811685552\n",
      "train loss:0.7283288996282486\n",
      "train loss:0.9668319549312243\n",
      "train loss:0.9352311250183757\n",
      "train loss:1.200687645875514\n",
      "train loss:0.8555811531924611\n",
      "train loss:0.9463799616752719\n",
      "train loss:0.9961223918018857\n",
      "train loss:0.8046947758412825\n",
      "train loss:0.7577602790104716\n",
      "train loss:0.9950470755319997\n",
      "train loss:0.9493919479572008\n",
      "train loss:0.8887231973741981\n",
      "train loss:0.8273813007771196\n",
      "train loss:1.134804875290432\n",
      "train loss:1.0873566297531827\n",
      "train loss:0.8222476151270489\n",
      "train loss:1.01740508099761\n",
      "train loss:0.8822811224977091\n",
      "train loss:0.9494462931975436\n",
      "train loss:1.004960910108863\n",
      "train loss:0.9583467652887556\n",
      "train loss:0.9586536444026014\n",
      "train loss:1.016195096450452\n",
      "train loss:0.8610644222005348\n",
      "train loss:0.9595876692862915\n",
      "train loss:1.262907090097967\n",
      "train loss:1.128582012360973\n",
      "train loss:1.0665638443573073\n",
      "train loss:1.0154714386530097\n",
      "train loss:0.9867767067345585\n",
      "train loss:1.032102512250051\n",
      "train loss:0.9427443711372314\n",
      "train loss:0.9887560445240475\n",
      "train loss:0.9219229553916533\n",
      "train loss:0.856869411881468\n",
      "train loss:1.1647544608856641\n",
      "train loss:1.182975542945592\n",
      "train loss:0.990675630590125\n",
      "train loss:0.8498635531620289\n",
      "train loss:0.8495071128506144\n",
      "train loss:1.1047825297253042\n",
      "train loss:1.0990246160275596\n",
      "train loss:1.0224900885272212\n",
      "train loss:0.8715985757813476\n",
      "train loss:0.8211654368128795\n",
      "train loss:0.8768868990180296\n",
      "train loss:1.0743898594962338\n",
      "train loss:1.1239464911852846\n",
      "train loss:0.8413185223722788\n",
      "train loss:1.1198980439118598\n",
      "train loss:0.930630671158831\n",
      "train loss:0.9345699149902991\n",
      "train loss:0.747544213602249\n",
      "train loss:1.0718695506998073\n",
      "train loss:0.8748377916453305\n",
      "train loss:0.8547799191259817\n",
      "train loss:1.0295274770647211\n",
      "train loss:1.0716082180081203\n",
      "train loss:1.1175004072089998\n",
      "train loss:0.8775814827551681\n",
      "train loss:1.062430756929126\n",
      "train loss:1.0707976139020225\n",
      "train loss:1.0525943595571252\n",
      "train loss:1.0310963156772426\n",
      "train loss:0.9570896895218993\n",
      "train loss:1.0846130239395058\n",
      "train loss:0.8201390317470925\n",
      "train loss:0.9914315791387396\n",
      "train loss:0.9014687112341477\n",
      "train loss:0.9369000001248254\n",
      "train loss:0.8636935903544298\n",
      "train loss:0.9121288308430775\n",
      "train loss:0.9691772211045828\n",
      "train loss:0.8727425676019371\n",
      "train loss:1.0848310662297884\n",
      "train loss:0.8154346398022061\n",
      "train loss:0.8733455314713942\n",
      "train loss:0.9488640714291944\n",
      "train loss:0.9968904390120851\n",
      "train loss:0.90777158529753\n",
      "train loss:1.073090242274001\n",
      "train loss:0.8083265392633462\n",
      "train loss:1.2326455639389469\n",
      "train loss:0.9279138450236966\n",
      "train loss:0.92655859684183\n",
      "train loss:0.9880589748474051\n",
      "train loss:1.076183502276412\n",
      "train loss:1.0850357988762007\n",
      "train loss:1.0183555150731132\n",
      "train loss:0.8908842540952974\n",
      "train loss:1.0569351747363827\n",
      "train loss:0.9486449619181718\n",
      "train loss:0.8568029184222122\n",
      "train loss:0.9043927514144332\n",
      "train loss:0.8701073206329797\n",
      "train loss:1.0148835978575055\n",
      "train loss:0.9442754994808144\n",
      "train loss:0.8334717298588044\n",
      "train loss:1.119064968948921\n",
      "train loss:1.0824922158892398\n",
      "train loss:0.8855128614884127\n",
      "train loss:0.9777322235412913\n",
      "train loss:0.854033854980369\n",
      "train loss:0.7866618557658489\n",
      "train loss:0.76003793377677\n",
      "train loss:0.8472217078834903\n",
      "train loss:1.1695543554028414\n",
      "train loss:0.7820967673871705\n",
      "train loss:1.011654811552523\n",
      "train loss:0.9139855769620299\n",
      "train loss:0.92079781192211\n",
      "train loss:1.077117410445458\n",
      "train loss:1.0952364349114994\n",
      "train loss:1.1888366885333155\n",
      "train loss:0.87422127324548\n",
      "train loss:0.9399366610123167\n",
      "train loss:0.9947058063820539\n",
      "train loss:1.0351417771164708\n",
      "train loss:0.8773512478334464\n",
      "train loss:0.8440470251486509\n",
      "train loss:1.0130054025111344\n",
      "train loss:1.0131697739819507\n",
      "train loss:0.9743517721706618\n",
      "train loss:1.1172174054073234\n",
      "train loss:1.0882574735477073\n",
      "train loss:0.8344864389172497\n",
      "train loss:1.231043547657012\n",
      "train loss:0.8706286905996548\n",
      "train loss:1.0373511042417298\n",
      "train loss:0.8938043424521968\n",
      "train loss:0.8772601863566859\n",
      "train loss:1.0940299732049714\n",
      "train loss:0.9834072190794119\n",
      "train loss:0.935591431452975\n",
      "train loss:0.9493928587189383\n",
      "train loss:0.9217088210182665\n",
      "train loss:0.9371074809552499\n",
      "train loss:1.0580580667636514\n",
      "train loss:1.0367965411681492\n",
      "train loss:0.9244125679208135\n",
      "train loss:1.0028566236173575\n",
      "train loss:0.8105138738328073\n",
      "train loss:0.7917358821914918\n",
      "train loss:0.907608099747302\n",
      "train loss:0.9784419549300581\n",
      "train loss:0.8514837575802591\n",
      "train loss:0.9776378402897932\n",
      "train loss:1.0498751375027437\n",
      "train loss:0.9982465828886196\n",
      "train loss:0.8172689825151334\n",
      "train loss:0.985117590283573\n",
      "train loss:1.0739604286274014\n",
      "train loss:1.0723622825517243\n",
      "train loss:0.8631112906731893\n",
      "train loss:0.8546168920796036\n",
      "train loss:0.8847220023783063\n",
      "train loss:0.7958293751324551\n",
      "train loss:0.9550921240683674\n",
      "train loss:1.0146301545942145\n",
      "train loss:0.9319212965221172\n",
      "train loss:0.9159349812454898\n",
      "train loss:0.956963347258832\n",
      "train loss:0.8499688910259795\n",
      "train loss:0.9729833447428018\n",
      "train loss:0.8051043951521598\n",
      "train loss:0.9599134156113823\n",
      "train loss:1.0259837591611851\n",
      "train loss:0.8444230824696278\n",
      "train loss:1.0076883625102893\n",
      "train loss:0.987736880490485\n",
      "train loss:0.8490121514965853\n",
      "train loss:0.8550799750380171\n",
      "train loss:0.9922931692913448\n",
      "train loss:1.0064945616918022\n",
      "train loss:0.9101221849143684\n",
      "train loss:0.9986740821131993\n",
      "train loss:1.0477902759796454\n",
      "train loss:0.8636816693219425\n",
      "train loss:1.0422739630290558\n",
      "train loss:0.8524545567984524\n",
      "train loss:1.0202778541641795\n",
      "train loss:0.8947235507108275\n",
      "train loss:0.9318332555083829\n",
      "train loss:0.846440088551405\n",
      "train loss:1.0987756209898407\n",
      "train loss:1.0892019550202539\n",
      "train loss:1.1459785950097048\n",
      "train loss:0.9155441886715137\n",
      "train loss:0.9616832884523119\n",
      "train loss:0.9668482351737795\n",
      "train loss:1.0367026826295171\n",
      "train loss:0.9385666844086576\n",
      "train loss:0.8841060021770173\n",
      "train loss:0.8331238887778928\n",
      "train loss:0.9816112311780641\n",
      "train loss:0.8907837323021038\n",
      "train loss:1.0136794948642807\n",
      "train loss:0.9783791122618556\n",
      "train loss:0.8136041387299232\n",
      "train loss:0.9547474353102462\n",
      "train loss:1.016210963783069\n",
      "train loss:0.9401820594505358\n",
      "train loss:0.9031107043336436\n",
      "train loss:0.9987104030710964\n",
      "train loss:0.9901433122953683\n",
      "train loss:1.1400290949840934\n",
      "train loss:1.0766672735834626\n",
      "train loss:0.9373742852215443\n",
      "train loss:0.9999274386422603\n",
      "train loss:0.9237479961742264\n",
      "train loss:0.9478651455219647\n",
      "train loss:0.9507862977156448\n",
      "train loss:0.9174720301215414\n",
      "train loss:0.9560516804172576\n",
      "train loss:0.9620910590230208\n",
      "train loss:1.0453397131442141\n",
      "train loss:1.112180298655996\n",
      "train loss:0.9125528957768377\n",
      "train loss:1.0132555088660384\n",
      "train loss:0.8435107041091456\n",
      "train loss:0.9078074578603278\n",
      "train loss:0.9800352484473052\n",
      "train loss:0.8890351906222932\n",
      "train loss:0.9665205610481132\n",
      "train loss:1.13813390252451\n",
      "train loss:0.9721216567595772\n",
      "train loss:1.0601889427673272\n",
      "train loss:0.8871594182548825\n",
      "train loss:1.0246249023075773\n",
      "train loss:0.9795236297555071\n",
      "train loss:0.7113495478808852\n",
      "train loss:0.9672954156615056\n",
      "train loss:0.8613110631709158\n",
      "train loss:0.8596884274014687\n",
      "train loss:0.9611551115032267\n",
      "train loss:0.9875068550450945\n",
      "train loss:0.955406769651099\n",
      "train loss:1.1586999245228464\n",
      "train loss:0.9497617742007272\n",
      "train loss:1.0352633742951154\n",
      "train loss:0.9566025596121998\n",
      "train loss:0.8995944416339805\n",
      "train loss:1.0095983876760746\n",
      "train loss:0.8745855901010756\n",
      "train loss:0.9161590129396958\n",
      "train loss:0.9607173587112117\n",
      "train loss:0.9687617408357282\n",
      "train loss:1.1073697203870192\n",
      "train loss:0.8807481470750881\n",
      "train loss:0.9363169232361834\n",
      "train loss:1.034210689890858\n",
      "train loss:1.0692823228199821\n",
      "train loss:0.9168192413937044\n",
      "train loss:0.7560439103702139\n",
      "train loss:0.858754813667122\n",
      "train loss:0.9446352297056635\n",
      "train loss:1.09379923673314\n",
      "train loss:0.8295582388750062\n",
      "train loss:0.7484020998970233\n",
      "train loss:0.779850576750301\n",
      "train loss:0.9313733918977062\n",
      "train loss:1.0530814377822195\n",
      "train loss:0.9343809602552301\n",
      "train loss:0.8134415733355941\n",
      "train loss:0.9035666025664397\n",
      "train loss:0.8442062647070258\n",
      "train loss:0.904266524146855\n",
      "train loss:1.0047785984335176\n",
      "train loss:0.8923257029804406\n",
      "train loss:1.0335117022087195\n",
      "train loss:1.0955287753384597\n",
      "train loss:0.8381456499467232\n",
      "train loss:1.1317339422834012\n",
      "train loss:0.9320454701798636\n",
      "train loss:0.869350787249371\n",
      "train loss:1.0481166858053932\n",
      "train loss:0.8973255243279403\n",
      "train loss:0.9822536259454888\n",
      "train loss:0.951463031528904\n",
      "train loss:0.8870725539450537\n",
      "train loss:0.9154865718161962\n",
      "train loss:0.9507093473937179\n",
      "train loss:0.9752226143186106\n",
      "train loss:1.0233669557786724\n",
      "train loss:1.007294343327833\n",
      "train loss:0.9981186945454472\n",
      "train loss:0.8644031038987563\n",
      "train loss:0.8953139656007064\n",
      "train loss:0.9924601728943785\n",
      "train loss:1.177088672829069\n",
      "train loss:0.9667087529630329\n",
      "train loss:0.9502318165368706\n",
      "train loss:1.0029068998427315\n",
      "train loss:0.8841988144898336\n",
      "train loss:1.0356705625155684\n",
      "train loss:1.0394941354790599\n",
      "train loss:0.9738817224511856\n",
      "train loss:0.8152076269889839\n",
      "train loss:0.9497535773139479\n",
      "train loss:1.0111811923603653\n",
      "train loss:0.8999404116547507\n",
      "train loss:0.7913247240130974\n",
      "train loss:0.9616875598132925\n",
      "train loss:0.9805298562129743\n",
      "train loss:0.8576602920697125\n",
      "train loss:1.0405607050949994\n",
      "train loss:1.217765311746821\n",
      "train loss:1.0385971020870624\n",
      "train loss:1.0572847686794524\n",
      "train loss:1.010804640183352\n",
      "train loss:0.9539858215240292\n",
      "train loss:1.0451503459582432\n",
      "train loss:0.9603327115123476\n",
      "train loss:1.1401775186160747\n",
      "train loss:0.9990400582436559\n",
      "train loss:0.9324432950232567\n",
      "train loss:0.9960774573711129\n",
      "train loss:0.8527764713403978\n",
      "train loss:0.9102183818719028\n",
      "train loss:1.052623685143861\n",
      "train loss:1.0853854507834766\n",
      "train loss:1.0001023415313883\n",
      "train loss:0.9076706525949315\n",
      "train loss:0.8370887814148302\n",
      "train loss:0.9913795761107039\n",
      "train loss:1.1361804040339312\n",
      "train loss:1.10975592856864\n",
      "train loss:0.975507301694899\n",
      "train loss:1.0608208028928312\n",
      "train loss:0.9757877642707841\n",
      "train loss:1.0092056775075808\n",
      "train loss:0.8439241283115809\n",
      "train loss:1.0352611615910166\n",
      "train loss:0.9577434496130591\n",
      "train loss:1.1411216345916184\n",
      "train loss:0.8456712396148882\n",
      "train loss:0.9501194462150874\n",
      "train loss:0.9905384188220284\n",
      "train loss:0.8005440729223381\n",
      "train loss:0.9101744754700967\n",
      "train loss:0.941131576292831\n",
      "train loss:0.9557295334187961\n",
      "train loss:0.9596649965549559\n",
      "train loss:1.1161067326137177\n",
      "train loss:0.9646391401581168\n",
      "train loss:0.9950939790091619\n",
      "train loss:1.1594882466642658\n",
      "train loss:0.9205267198138736\n",
      "train loss:0.9641247658064366\n",
      "train loss:1.0143899521075979\n",
      "train loss:1.1841321874313346\n",
      "train loss:0.914433023043755\n",
      "train loss:0.8391501954924737\n",
      "train loss:0.9236548067608351\n",
      "train loss:0.8642039600194278\n",
      "train loss:1.0690177585706466\n",
      "train loss:0.7847134548117474\n",
      "train loss:1.0158925177173999\n",
      "train loss:0.9883557126541558\n",
      "train loss:0.9073147345530697\n",
      "train loss:0.9652374815273924\n",
      "train loss:0.7594166139043722\n",
      "train loss:1.021840093143225\n",
      "train loss:1.0487522219558878\n",
      "train loss:0.8745679876652466\n",
      "train loss:0.9113343989851179\n",
      "train loss:0.912528841330314\n",
      "train loss:0.9406712227579497\n",
      "train loss:0.8065010543812113\n",
      "train loss:0.9445425451570135\n",
      "train loss:0.8620878480955387\n",
      "train loss:0.7656516688986524\n",
      "train loss:0.9413773152541488\n",
      "train loss:0.9740293254823789\n",
      "train loss:1.0766539717499943\n",
      "train loss:1.0495567227704568\n",
      "train loss:0.811063368805498\n",
      "train loss:0.9669290919446724\n",
      "train loss:0.873150863185439\n",
      "train loss:0.9109962681200543\n",
      "train loss:1.2671832925871827\n",
      "train loss:1.0210380567101311\n",
      "train loss:0.8572876112887226\n",
      "train loss:0.9318848134492725\n",
      "train loss:1.0611420638603253\n",
      "train loss:0.8239981462379566\n",
      "train loss:0.9627991328162021\n",
      "train loss:0.9166905109323342\n",
      "train loss:0.8896670959834106\n",
      "train loss:0.901797774774422\n",
      "train loss:0.9925400109030944\n",
      "train loss:1.141369090434702\n",
      "train loss:0.980139068701944\n",
      "train loss:0.9287515592549636\n",
      "train loss:0.8825794833858791\n",
      "train loss:1.1127553389930187\n",
      "train loss:1.0549237933810662\n",
      "train loss:0.8595481507259719\n",
      "train loss:1.1634403860455453\n",
      "train loss:0.9821427893519382\n",
      "train loss:0.8770525899285411\n",
      "train loss:0.948073935571654\n",
      "train loss:0.9347097111219823\n",
      "train loss:0.9949937428052509\n",
      "train loss:0.8146904300396013\n",
      "train loss:0.9447789896932952\n",
      "train loss:1.0150020038841645\n",
      "train loss:0.9514446491207262\n",
      "=== epoch:4, train acc:0.984, test acc:0.991 ===\n",
      "train loss:0.9689076425817243\n",
      "train loss:0.9201471965830642\n",
      "train loss:0.9049915128556083\n",
      "train loss:0.8784145903089106\n",
      "train loss:0.8380204309174063\n",
      "train loss:0.9873751848488239\n",
      "train loss:1.0477621073703813\n",
      "train loss:0.7704094847103662\n",
      "train loss:0.9565170453648308\n",
      "train loss:0.9213681240255903\n",
      "train loss:1.154887468809787\n",
      "train loss:1.0243110391392254\n",
      "train loss:0.8478951562538058\n",
      "train loss:0.8357243181330821\n",
      "train loss:1.186445092283738\n",
      "train loss:1.0024341587199954\n",
      "train loss:0.9274622248246958\n",
      "train loss:0.9112102280987222\n",
      "train loss:1.0443839426582868\n",
      "train loss:0.8622709509835849\n",
      "train loss:0.9577819492494595\n",
      "train loss:1.0809174515119195\n",
      "train loss:1.0147296292463384\n",
      "train loss:1.0155228548909065\n",
      "train loss:0.8515323184295401\n",
      "train loss:0.8640537614621493\n",
      "train loss:1.0138433939383358\n",
      "train loss:1.0675285453466448\n",
      "train loss:0.9108259318090621\n",
      "train loss:0.836108640274356\n",
      "train loss:0.7842752002237384\n",
      "train loss:0.9433417775522872\n",
      "train loss:0.7376372075197258\n",
      "train loss:0.9024586260799587\n",
      "train loss:0.9503169643990586\n",
      "train loss:0.9855723562716381\n",
      "train loss:1.0859426583574614\n",
      "train loss:0.8555772447022811\n",
      "train loss:1.0147116225394492\n",
      "train loss:1.0688945648413195\n",
      "train loss:0.9513413013438101\n",
      "train loss:0.7783829732754229\n",
      "train loss:1.0200767978204437\n",
      "train loss:0.9634368273365611\n",
      "train loss:0.881089884149011\n",
      "train loss:0.9936557124035472\n",
      "train loss:1.0612797243813816\n",
      "train loss:0.8187376259088127\n",
      "train loss:0.9150258280279434\n",
      "train loss:0.962243547049443\n",
      "train loss:0.9116160514688003\n",
      "train loss:0.9902107868457048\n",
      "train loss:0.9279693246952185\n",
      "train loss:1.0499563174587232\n",
      "train loss:1.1382092874611092\n",
      "train loss:0.9884898228304038\n",
      "train loss:0.9263797721028378\n",
      "train loss:1.0023598270373066\n",
      "train loss:0.9648146773514472\n",
      "train loss:0.838756665355091\n",
      "train loss:1.0019380019591886\n",
      "train loss:0.9771886434230889\n",
      "train loss:1.1048248672916083\n",
      "train loss:0.9586407369769971\n",
      "train loss:0.9614531572385129\n",
      "train loss:0.9491191951708106\n",
      "train loss:1.06742840700084\n",
      "train loss:1.0374274766315832\n",
      "train loss:1.0433415657890834\n",
      "train loss:0.876858782784339\n",
      "train loss:1.2200097823964955\n",
      "train loss:0.9162235574912482\n",
      "train loss:0.7935244418751429\n",
      "train loss:1.0589031302347807\n",
      "train loss:0.8927998543746454\n",
      "train loss:0.8733355554362393\n",
      "train loss:0.9115348022274208\n",
      "train loss:1.0630579209487327\n",
      "train loss:1.0055493116256213\n",
      "train loss:1.0436705785232112\n",
      "train loss:0.8359715753442856\n",
      "train loss:0.9784373265866535\n",
      "train loss:0.9340582853182368\n",
      "train loss:0.9196023249973324\n",
      "train loss:0.882797723638169\n",
      "train loss:0.9749059900320082\n",
      "train loss:0.9830502320727529\n",
      "train loss:0.8595058685798054\n",
      "train loss:0.9278319709977234\n",
      "train loss:0.9499347140828575\n",
      "train loss:0.7576459437826247\n",
      "train loss:1.0948743876970572\n",
      "train loss:0.9060846728390846\n",
      "train loss:0.9493959380774768\n",
      "train loss:1.0386754412099843\n",
      "train loss:0.9452347145610617\n",
      "train loss:0.9248940389334179\n",
      "train loss:0.980323325353968\n",
      "train loss:1.0367312644890092\n",
      "train loss:0.9843681518338725\n",
      "train loss:1.0510839454057606\n",
      "train loss:0.9253624014343481\n",
      "train loss:0.8314490577605721\n",
      "train loss:1.1533378081385008\n",
      "train loss:0.9376240594186491\n",
      "train loss:0.9958037912450596\n",
      "train loss:0.9649085091217505\n",
      "train loss:1.0093816472637291\n",
      "train loss:0.9753765086502095\n",
      "train loss:0.9971153778867445\n",
      "train loss:0.9080031028250153\n",
      "train loss:0.8685986328639004\n",
      "train loss:1.0118432935305646\n",
      "train loss:1.0310623691103158\n",
      "train loss:0.8779166091711724\n",
      "train loss:1.113418674811515\n",
      "train loss:0.9393101010182049\n",
      "train loss:0.9251613780182422\n",
      "train loss:0.9920450009489744\n",
      "train loss:0.8021647132820425\n",
      "train loss:0.7400916347898704\n",
      "train loss:0.8811415716198921\n",
      "train loss:0.8813067146124064\n",
      "train loss:1.1141791319388012\n",
      "train loss:0.9530839788151881\n",
      "train loss:0.8582937017304734\n",
      "train loss:0.9739857904798653\n",
      "train loss:0.8578322171168206\n",
      "train loss:1.0263551190636435\n",
      "train loss:0.9520064837280005\n",
      "train loss:0.8953280643776914\n",
      "train loss:0.9418285040844693\n",
      "train loss:1.0353208273487042\n",
      "train loss:1.014324265742789\n",
      "train loss:0.9392304885592161\n",
      "train loss:0.8291566712763698\n",
      "train loss:0.85726519250849\n",
      "train loss:0.8374556754423952\n",
      "train loss:1.044486774779199\n",
      "train loss:1.0076518153082512\n",
      "train loss:0.9245588836917915\n",
      "train loss:0.9516530830242257\n",
      "train loss:1.0253426810782094\n",
      "train loss:0.9373572749489806\n",
      "train loss:0.9772497203035584\n",
      "train loss:0.9743461969431904\n",
      "train loss:0.923731272439005\n",
      "train loss:0.9959792087826672\n",
      "train loss:0.8478488278104896\n",
      "train loss:0.9520579788458042\n",
      "train loss:0.9231570764873777\n",
      "train loss:0.9603140682239432\n",
      "train loss:0.9639515426026232\n",
      "train loss:0.9741977768553691\n",
      "train loss:0.9596252107554102\n",
      "train loss:0.9953430754609752\n",
      "train loss:0.8141301951779155\n",
      "train loss:0.9429185334128696\n",
      "train loss:1.1150535699381334\n",
      "train loss:0.9608548890001972\n",
      "train loss:0.9937062659015894\n",
      "train loss:1.0081982669092442\n",
      "train loss:1.0377767793655317\n",
      "train loss:1.0593841952556762\n",
      "train loss:0.9387697911497805\n",
      "train loss:0.9874018451225406\n",
      "train loss:1.0787120240597383\n",
      "train loss:1.0440959327860018\n",
      "train loss:1.011066874519959\n",
      "train loss:0.9612552142415315\n",
      "train loss:0.9075819868826873\n",
      "train loss:0.7448469786453882\n",
      "train loss:1.0867116348891122\n",
      "train loss:1.0254349259721454\n",
      "train loss:0.9309945687590525\n",
      "train loss:0.8908191433858589\n",
      "train loss:0.9757706102819824\n",
      "train loss:0.9407494774415257\n",
      "train loss:1.1585255403977102\n",
      "train loss:1.1570065291106204\n",
      "train loss:0.888681806066544\n",
      "train loss:0.9617354508858782\n",
      "train loss:0.8990092655004429\n",
      "train loss:1.1279492122361388\n",
      "train loss:0.9404998552451937\n",
      "train loss:1.0312152074643692\n",
      "train loss:0.8551836813681232\n",
      "train loss:0.9048075524425377\n",
      "train loss:1.0496650149439462\n",
      "train loss:0.9584099306155827\n",
      "train loss:1.100225921646708\n",
      "train loss:1.021426613544089\n",
      "train loss:0.9994076493596016\n",
      "train loss:0.8691617890885458\n",
      "train loss:1.1149760144879735\n",
      "train loss:0.8988842265179072\n",
      "train loss:0.8606046153924584\n",
      "train loss:0.9933659184259223\n",
      "train loss:1.0274158867851697\n",
      "train loss:0.9549323246718454\n",
      "train loss:0.9335180428472718\n",
      "train loss:1.0363176251975543\n",
      "train loss:0.9309569423440023\n",
      "train loss:0.9575028573292806\n",
      "train loss:0.9909048786972986\n",
      "train loss:0.9667823679686266\n",
      "train loss:0.7731913884739199\n",
      "train loss:0.9774095814447409\n",
      "train loss:0.9233505896486294\n",
      "train loss:1.017129136083407\n",
      "train loss:0.9481744779335329\n",
      "train loss:0.8315640045221864\n",
      "train loss:1.0268973320610675\n",
      "train loss:0.8584593808809395\n",
      "train loss:0.9048340296871568\n",
      "train loss:0.801528176680707\n",
      "train loss:0.9300038137725777\n",
      "train loss:1.0624511831828511\n",
      "train loss:0.9126647152172797\n",
      "train loss:1.1117798602804756\n",
      "train loss:0.8436851649503296\n",
      "train loss:0.9300958327419572\n",
      "train loss:0.9423717454465121\n",
      "train loss:0.6809457456500811\n",
      "train loss:1.030788680826306\n",
      "train loss:0.966717707727488\n",
      "train loss:1.01769410092311\n",
      "train loss:0.9652262804194449\n",
      "train loss:0.8385710961361424\n",
      "train loss:0.8865487933775202\n",
      "train loss:1.0180974824677025\n",
      "train loss:1.0167829273157956\n",
      "train loss:0.8356499563786427\n",
      "train loss:1.0017544334009907\n",
      "train loss:1.0052489382863465\n",
      "train loss:0.9622497631850331\n",
      "train loss:0.8120796364339814\n",
      "train loss:0.8258551966965892\n",
      "train loss:0.9649369187958888\n",
      "train loss:0.922490139755854\n",
      "train loss:0.7463879469992973\n",
      "train loss:0.961794431467426\n",
      "train loss:0.9249572710902327\n",
      "train loss:1.0027509714999958\n",
      "train loss:0.9414945626459428\n",
      "train loss:0.9451088205320033\n",
      "train loss:0.9485178181552398\n",
      "train loss:0.9971647896719429\n",
      "train loss:0.99092621666008\n",
      "train loss:0.7960389443400548\n",
      "train loss:1.0124701373373715\n",
      "train loss:1.007214910260612\n",
      "train loss:0.9610511418486157\n",
      "train loss:0.7842715592117401\n",
      "train loss:0.8829537198703111\n",
      "train loss:0.8797066163242065\n",
      "train loss:1.0872160852328734\n",
      "train loss:0.7552206985712592\n",
      "train loss:0.9744410374260231\n",
      "train loss:0.9283719399117818\n",
      "train loss:0.8387721014167417\n",
      "train loss:0.987996349785546\n",
      "train loss:0.9039966005704666\n",
      "train loss:1.1559426408779676\n",
      "train loss:0.8433152329737963\n",
      "train loss:0.887232902698628\n",
      "train loss:0.825229378560338\n",
      "train loss:0.7308989796169832\n",
      "train loss:0.9688692190385964\n",
      "train loss:0.9414444906306427\n",
      "train loss:0.9593835600381734\n",
      "train loss:0.8668026730621908\n",
      "train loss:1.0521929180997063\n",
      "train loss:1.0003618495433422\n",
      "train loss:0.9060925316439288\n",
      "train loss:0.9357107618891297\n",
      "train loss:1.0233968286483375\n",
      "train loss:0.992099108699587\n",
      "train loss:0.8820241452642896\n",
      "train loss:0.9769908406577612\n",
      "train loss:0.759131906898171\n",
      "train loss:0.8384814519206426\n",
      "train loss:1.0736541912184965\n",
      "train loss:1.1964972760724115\n",
      "train loss:0.9739804792502773\n",
      "train loss:0.8928519908516406\n",
      "train loss:1.0188837813059402\n",
      "train loss:0.9205405532145708\n",
      "train loss:0.9324019810588909\n",
      "train loss:1.0549459572606033\n",
      "train loss:0.9081973489934257\n",
      "train loss:1.0856211633884412\n",
      "train loss:0.823452300032963\n",
      "train loss:0.8791475636135193\n",
      "train loss:0.9729168767763369\n",
      "train loss:0.8681679482644006\n",
      "train loss:1.0049376337203682\n",
      "train loss:1.0687203842432091\n",
      "train loss:0.9311744253731203\n",
      "train loss:1.0184532675533222\n",
      "train loss:0.9893813520411899\n",
      "train loss:0.8928944847389004\n",
      "train loss:0.9969726640037204\n",
      "train loss:0.9139491245392777\n",
      "train loss:0.8844380895399704\n",
      "train loss:0.8284223004480377\n",
      "train loss:1.0308053554172438\n",
      "train loss:0.9099600772891587\n",
      "train loss:0.8108063598533946\n",
      "train loss:1.017030875560893\n",
      "train loss:0.9689011770476574\n",
      "train loss:0.8732878548847333\n",
      "train loss:0.9033539211988868\n",
      "train loss:0.8465119185930144\n",
      "train loss:0.9460782595158445\n",
      "train loss:0.9443725208051623\n",
      "train loss:0.599804044704973\n",
      "train loss:0.8489232043563907\n",
      "train loss:0.9816998578562902\n",
      "train loss:1.07694177549464\n",
      "train loss:0.8548987226650374\n",
      "train loss:0.8842180034459918\n",
      "train loss:0.9361715929215231\n",
      "train loss:0.9135654153720129\n",
      "train loss:0.8054666557925426\n",
      "train loss:0.9223453213495297\n",
      "train loss:0.8654351398899826\n",
      "train loss:0.9689827512218534\n",
      "train loss:0.84185272546061\n",
      "train loss:0.7859901421861715\n",
      "train loss:0.9516441491846463\n",
      "train loss:0.7642795346239072\n",
      "train loss:0.8646058544346055\n",
      "train loss:0.9024964185762481\n",
      "train loss:0.9958118499809956\n",
      "train loss:0.9213083973762203\n",
      "train loss:0.947619896966145\n",
      "train loss:0.9167968425769543\n",
      "train loss:0.9413231339873274\n",
      "train loss:0.9078889752412536\n",
      "train loss:0.9243052535713683\n",
      "train loss:0.9820628699872569\n",
      "train loss:0.8532152059371875\n",
      "train loss:1.0252763271177734\n",
      "train loss:0.8738438028640142\n",
      "train loss:0.8425209862244148\n",
      "train loss:1.0983807992544348\n",
      "train loss:1.1983715142273588\n",
      "train loss:0.941480527621248\n",
      "train loss:0.941543725510132\n",
      "train loss:1.1066456938970948\n",
      "train loss:0.8671318781266929\n",
      "train loss:1.0485742498038888\n",
      "train loss:0.9007914846459204\n",
      "train loss:1.0420217995066645\n",
      "train loss:0.9667456222636472\n",
      "train loss:0.9567319925542997\n",
      "train loss:1.0844911077270831\n",
      "train loss:0.9438148394203885\n",
      "train loss:0.8169930383285776\n",
      "train loss:0.9759968324938416\n",
      "train loss:0.8347847145757473\n",
      "train loss:1.0022771115430644\n",
      "train loss:1.0949681649635552\n",
      "train loss:1.1521669073802518\n",
      "train loss:0.7982648937523349\n",
      "train loss:0.8786958864671804\n",
      "train loss:0.9495558483656475\n",
      "train loss:1.0469983835247896\n",
      "train loss:0.8929317374855162\n",
      "train loss:0.9509782474988512\n",
      "train loss:1.0362519248230242\n",
      "train loss:0.9533792295378481\n",
      "train loss:1.03480881232275\n",
      "train loss:0.8722714261698888\n",
      "train loss:1.1023360592266216\n",
      "train loss:0.8333339480450347\n",
      "train loss:0.9449451604749707\n",
      "train loss:1.0378105763498315\n",
      "train loss:0.8603742884100307\n",
      "train loss:0.9854887845058403\n",
      "train loss:0.8529015071105768\n",
      "train loss:0.8857266414316279\n",
      "train loss:0.9261394248433582\n",
      "train loss:0.8871169629200929\n",
      "train loss:0.9214423529144306\n",
      "train loss:0.9540009472493503\n",
      "train loss:0.9707483291529959\n",
      "train loss:1.0021965670237583\n",
      "train loss:1.1565231685058917\n",
      "train loss:0.9224756076800549\n",
      "train loss:0.946482141079096\n",
      "train loss:0.7667279833352767\n",
      "train loss:0.9883521927680781\n",
      "train loss:0.9077750206691829\n",
      "train loss:0.8433610935634274\n",
      "train loss:0.9955783262763955\n",
      "train loss:1.0748234337852423\n",
      "train loss:0.9762042507585081\n",
      "train loss:0.9960241649101402\n",
      "train loss:0.9973671871974468\n",
      "train loss:0.802697989501898\n",
      "train loss:1.1548099485607546\n",
      "train loss:0.9243320668220512\n",
      "train loss:0.8467436031763143\n",
      "train loss:0.8595890252666963\n",
      "train loss:1.1218137749151067\n",
      "train loss:1.0510002971680037\n",
      "train loss:1.1609542875762222\n",
      "train loss:0.8887048151075799\n",
      "train loss:1.07971067189519\n",
      "train loss:1.1334632210432012\n",
      "train loss:0.9532972900713846\n",
      "train loss:0.7402853906979917\n",
      "train loss:0.9965138111028957\n",
      "train loss:0.9246006812151967\n",
      "train loss:1.0002183395969408\n",
      "train loss:0.9899792847751262\n",
      "train loss:0.8679205242171855\n",
      "train loss:0.9804130313013201\n",
      "train loss:1.0585313655143984\n",
      "train loss:0.8196104684570338\n",
      "train loss:0.9767589485467808\n",
      "train loss:1.07235928582388\n",
      "train loss:1.012829565524749\n",
      "train loss:1.0522955285090072\n",
      "train loss:0.9146471140107902\n",
      "train loss:0.8813808690071719\n",
      "train loss:1.089563061666466\n",
      "train loss:0.7722993394918806\n",
      "train loss:1.109329099768275\n",
      "train loss:1.0257902800458223\n",
      "train loss:0.9767991570474025\n",
      "train loss:0.84613861937791\n",
      "train loss:0.8256555644738424\n",
      "train loss:1.0642258549964578\n",
      "train loss:0.8700000001165037\n",
      "train loss:0.9908674876422378\n",
      "train loss:0.8832609524263855\n",
      "train loss:0.893986335469304\n",
      "train loss:0.9048958610450017\n",
      "train loss:0.9976018027871015\n",
      "train loss:0.9605567236578232\n",
      "train loss:0.8725863173836117\n",
      "train loss:0.951212730429376\n",
      "train loss:1.0934131632453055\n",
      "train loss:0.8922829384494646\n",
      "train loss:1.0599590625817603\n",
      "train loss:0.9545827102288534\n",
      "train loss:1.03056052151417\n",
      "train loss:0.9909074874248068\n",
      "train loss:1.0189132003687\n",
      "train loss:1.0359630393768806\n",
      "train loss:0.9436940373242183\n",
      "train loss:0.9926970159959906\n",
      "train loss:0.9144066805522051\n",
      "train loss:0.8979510519597846\n",
      "train loss:0.7675092856337011\n",
      "train loss:0.8368577643016498\n",
      "train loss:0.7447094952773503\n",
      "train loss:0.9734456277132518\n",
      "train loss:0.8661556588788061\n",
      "train loss:0.9833498631096576\n",
      "train loss:0.9024442104143381\n",
      "train loss:0.8950042390057348\n",
      "train loss:0.8724243448780536\n",
      "train loss:0.8185257609813483\n",
      "train loss:0.9800159700415755\n",
      "train loss:0.9758541313972259\n",
      "train loss:0.9272943146049879\n",
      "train loss:0.9080096204089196\n",
      "train loss:1.1175700934427204\n",
      "train loss:0.9627044086663291\n",
      "train loss:0.9510819117045329\n",
      "train loss:0.9669367743360227\n",
      "train loss:0.9414238081344515\n",
      "train loss:0.9460208387926898\n",
      "train loss:0.8377806332278707\n",
      "train loss:0.7556046996101735\n",
      "train loss:0.9342919351593211\n",
      "train loss:0.8309481291452459\n",
      "train loss:0.7863567742801421\n",
      "train loss:0.9674944679330194\n",
      "train loss:1.0062218633826843\n",
      "train loss:0.8790643899799364\n",
      "train loss:0.8774461390023239\n",
      "train loss:1.0703637502906\n",
      "train loss:1.0742228212158387\n",
      "train loss:0.908779185968571\n",
      "train loss:0.823615538098323\n",
      "train loss:0.8667803892643281\n",
      "train loss:1.0122012141717514\n",
      "train loss:0.8333154363845813\n",
      "train loss:0.7576828919804082\n",
      "train loss:0.9860402808372319\n",
      "train loss:0.9843585854996842\n",
      "train loss:1.0088757262296648\n",
      "train loss:0.9869873187285954\n",
      "train loss:0.7146071402278729\n",
      "train loss:1.0948922897562354\n",
      "train loss:0.918929579739389\n",
      "train loss:0.9508235409876998\n",
      "train loss:0.8378518532598906\n",
      "train loss:0.9891079321905538\n",
      "train loss:0.7958142852594007\n",
      "train loss:0.8957225595400683\n",
      "train loss:0.9591782543712821\n",
      "train loss:0.8757244688800577\n",
      "train loss:0.9534065987671491\n",
      "train loss:0.9379314956028344\n",
      "train loss:0.8140057837338158\n",
      "train loss:0.9822387454476703\n",
      "train loss:0.9214972834885589\n",
      "train loss:0.8548026540467564\n",
      "train loss:0.9263813220611687\n",
      "train loss:1.0878732398544178\n",
      "train loss:0.9112827685555251\n",
      "train loss:0.9951274870360041\n",
      "train loss:0.8208432295145444\n",
      "train loss:1.063087685991139\n",
      "train loss:0.8482025935390528\n",
      "train loss:1.07057613901034\n",
      "train loss:0.8073828284301786\n",
      "train loss:1.11322961057075\n",
      "train loss:0.823134252051581\n",
      "train loss:0.919904553381387\n",
      "train loss:0.767742982336688\n",
      "train loss:1.09075612554664\n",
      "train loss:0.9419998696621943\n",
      "train loss:0.9288390433859895\n",
      "train loss:0.9099585188282299\n",
      "train loss:0.7541334563842291\n",
      "train loss:0.9453041511654615\n",
      "train loss:0.8696682197809354\n",
      "train loss:0.9611330293127797\n",
      "train loss:1.0296204932256876\n",
      "train loss:0.9192966439291296\n",
      "train loss:0.742076884233515\n",
      "train loss:0.9373897591222204\n",
      "train loss:0.9578832652293626\n",
      "train loss:0.9182137762267538\n",
      "train loss:0.8606784346013986\n",
      "train loss:0.7594372282519931\n",
      "train loss:0.9493153214945113\n",
      "train loss:1.2104799169596665\n",
      "train loss:0.6812111297767554\n",
      "train loss:0.87416331148811\n",
      "train loss:1.1554056616987314\n",
      "train loss:0.9709646721518819\n",
      "train loss:1.0215862484738334\n",
      "train loss:0.9806970218336004\n",
      "train loss:1.044162732834806\n",
      "train loss:1.0037748786297855\n",
      "train loss:0.8627811872022155\n",
      "train loss:0.8995264791291544\n",
      "train loss:1.1318719583065093\n",
      "train loss:0.9589695274151329\n",
      "train loss:0.949259872672374\n",
      "train loss:1.1836361261302104\n",
      "train loss:0.8998195652110027\n",
      "train loss:0.908088173422876\n",
      "train loss:0.9512606881784533\n",
      "train loss:1.0759573860687746\n",
      "train loss:0.9916599763701636\n",
      "train loss:0.9514341314161119\n",
      "train loss:0.8571141378134115\n",
      "train loss:0.9586109491950745\n",
      "train loss:1.0084478959083654\n",
      "train loss:1.089424998414573\n",
      "train loss:0.9583519157313032\n",
      "train loss:1.0505214176507938\n",
      "train loss:0.9659202595356315\n",
      "train loss:0.7824283981587974\n",
      "train loss:0.896834656265615\n",
      "train loss:1.008589355070051\n",
      "train loss:0.9149657601884849\n",
      "train loss:0.9473853707131024\n",
      "train loss:0.9205206688537594\n",
      "train loss:0.9164352493299645\n",
      "train loss:0.8995506699220661\n",
      "train loss:0.9028343554427175\n",
      "train loss:0.8093985589064058\n",
      "train loss:0.9779041014009192\n",
      "train loss:1.1050060436778628\n",
      "train loss:0.9164903857959429\n",
      "train loss:1.010595726040341\n",
      "train loss:0.9100160909786976\n",
      "train loss:0.8501160746912827\n",
      "train loss:0.9938287260540838\n",
      "train loss:0.9341512938297325\n",
      "train loss:0.9696418559363503\n",
      "train loss:0.9048494977183684\n",
      "train loss:0.7749807179410181\n",
      "train loss:0.8528882974386992\n",
      "train loss:1.0259549056165447\n",
      "train loss:1.0686257248000068\n",
      "train loss:1.0321574332175747\n",
      "train loss:0.9893510611974196\n",
      "train loss:0.8343325454876092\n",
      "train loss:1.022782895165458\n",
      "=== epoch:5, train acc:0.992, test acc:0.989 ===\n",
      "train loss:1.0296860145800202\n",
      "train loss:0.7125544224320371\n",
      "train loss:0.8135298332574302\n",
      "train loss:0.9387964610273692\n",
      "train loss:1.1144309295342265\n",
      "train loss:0.93567723594979\n",
      "train loss:0.8952380685543868\n",
      "train loss:0.949998391592274\n",
      "train loss:0.9782978496920594\n",
      "train loss:0.9179662565576885\n",
      "train loss:0.9050363846395502\n",
      "train loss:0.999825447085618\n",
      "train loss:1.0434559278757378\n",
      "train loss:1.0224112691220328\n",
      "train loss:0.8865619750181457\n",
      "train loss:0.8584143699205703\n",
      "train loss:0.9087887541387386\n",
      "train loss:1.016540920310726\n",
      "train loss:0.6854181023189939\n",
      "train loss:0.9352671596688457\n",
      "train loss:0.9736347858500742\n",
      "train loss:0.9027896283916301\n",
      "train loss:0.8208361524204739\n",
      "train loss:0.931847080122576\n",
      "train loss:0.9471303107362813\n",
      "train loss:0.8591293648532745\n",
      "train loss:1.0362004560277993\n",
      "train loss:0.8356508556939076\n",
      "train loss:0.8063380919962374\n",
      "train loss:1.068831737668283\n",
      "train loss:0.9787648259097489\n",
      "train loss:1.0704375559228236\n",
      "train loss:0.991496375488646\n",
      "train loss:1.0454200421180238\n",
      "train loss:0.9491912003559797\n",
      "train loss:0.8991754955175151\n",
      "train loss:0.9499317153905408\n",
      "train loss:0.8128406804637649\n",
      "train loss:0.8456512665023684\n",
      "train loss:0.9933331043034738\n",
      "train loss:0.9820979406032143\n",
      "train loss:0.8030202888189543\n",
      "train loss:0.7446723587438359\n",
      "train loss:0.8160414277683208\n",
      "train loss:0.9313873762006007\n",
      "train loss:0.9483870756783362\n",
      "train loss:0.8658724170822607\n",
      "train loss:0.7522777978931662\n",
      "train loss:1.009568461191\n",
      "train loss:1.012861392386071\n",
      "train loss:1.060258222373064\n",
      "train loss:0.7656073927189158\n",
      "train loss:0.9305564441867422\n",
      "train loss:0.8980332220715266\n",
      "train loss:0.9580474507234205\n",
      "train loss:0.7167107853955741\n",
      "train loss:0.8702995799404952\n",
      "train loss:0.9280370820760362\n",
      "train loss:0.8412228085168789\n",
      "train loss:0.784196649562026\n",
      "train loss:0.8375591314960903\n",
      "train loss:0.9121271118869007\n",
      "train loss:1.2791499491091847\n",
      "train loss:0.9395053494009382\n",
      "train loss:1.0398339292950356\n",
      "train loss:0.9143120820968902\n",
      "train loss:1.0167021340884435\n",
      "train loss:1.0437916774017244\n",
      "train loss:0.9906841436507476\n",
      "train loss:0.9953518261520324\n",
      "train loss:0.8908067994732859\n",
      "train loss:0.9638343781655196\n",
      "train loss:0.7115692132910908\n",
      "train loss:0.9061834939756157\n",
      "train loss:0.9754318868952667\n",
      "train loss:0.8303498412222997\n",
      "train loss:0.8490778255699221\n",
      "train loss:0.9532540318795495\n",
      "train loss:0.7847728676577199\n",
      "train loss:0.9043374141540517\n",
      "train loss:0.9091012755578967\n",
      "train loss:0.8138076802515066\n",
      "train loss:0.8796647428569195\n",
      "train loss:0.9553663060147614\n",
      "train loss:1.1073827700544934\n",
      "train loss:0.8796880488457716\n",
      "train loss:0.8725411317949824\n",
      "train loss:0.912298403225553\n",
      "train loss:0.9208180120446978\n",
      "train loss:0.9532282864328938\n",
      "train loss:0.9746025358599798\n",
      "train loss:0.9469522495537042\n",
      "train loss:0.9822171052215594\n",
      "train loss:0.8706777573083093\n",
      "train loss:0.9448820907952163\n",
      "train loss:1.0594442381318585\n",
      "train loss:0.91648349022197\n",
      "train loss:0.8230771736145541\n",
      "train loss:0.958489230305673\n",
      "train loss:1.0073282120789373\n",
      "train loss:1.1078832512742811\n",
      "train loss:0.9743307220988524\n",
      "train loss:0.9664586765904289\n",
      "train loss:1.087165250839711\n",
      "train loss:1.034381647024589\n",
      "train loss:1.060956229432184\n",
      "train loss:0.9739130306322678\n",
      "train loss:1.071240193343507\n",
      "train loss:0.7754614059620707\n",
      "train loss:1.009462679741737\n",
      "train loss:0.9885938197228376\n",
      "train loss:0.839877033753435\n",
      "train loss:0.8421920896953479\n",
      "train loss:0.8661613735383301\n",
      "train loss:0.6685660767136794\n",
      "train loss:0.9406112629305217\n",
      "train loss:1.0008628798397128\n",
      "train loss:1.0582349369197284\n",
      "train loss:1.1723128684654822\n",
      "train loss:0.9433727136553626\n",
      "train loss:0.8229872705284279\n",
      "train loss:0.8401546796091934\n",
      "train loss:0.9632286794780285\n",
      "train loss:0.9889021932604644\n",
      "train loss:1.090410343223254\n",
      "train loss:0.9527235653132539\n",
      "train loss:0.9204137931687555\n",
      "train loss:0.9165859945463899\n",
      "train loss:0.7901891001912924\n",
      "train loss:0.9745027749435081\n",
      "train loss:0.8013591107422385\n",
      "train loss:1.010026621560986\n",
      "train loss:0.7349097943313615\n",
      "train loss:1.0977540763593345\n",
      "train loss:0.8746359504810934\n",
      "train loss:0.786028080662248\n",
      "train loss:1.1272105202735154\n",
      "train loss:0.9399368162671277\n",
      "train loss:0.8996762856644458\n",
      "train loss:0.8414544806490496\n",
      "train loss:0.9269979680017582\n",
      "train loss:0.9185443504550588\n",
      "train loss:0.8291503108460683\n",
      "train loss:0.9288982804228673\n",
      "train loss:1.0064779295435766\n",
      "train loss:1.0149806055545074\n",
      "train loss:0.8934923162570327\n",
      "train loss:1.0870081065058288\n",
      "train loss:0.9848327705571837\n",
      "train loss:1.0607701076266316\n",
      "train loss:0.8338973326368495\n",
      "train loss:0.8002654350459635\n",
      "train loss:0.9704237146468648\n",
      "train loss:0.8235487500648787\n",
      "train loss:1.0128153989518487\n",
      "train loss:0.8434371867186755\n",
      "train loss:0.7821799585733455\n",
      "train loss:0.7629395955290016\n",
      "train loss:0.9281569518272803\n",
      "train loss:0.9357321511325597\n",
      "train loss:0.9451723640206204\n",
      "train loss:1.1568364915694465\n",
      "train loss:0.879990664462713\n",
      "train loss:0.9800616968415862\n",
      "train loss:0.6897446530124697\n",
      "train loss:0.8010709630915759\n",
      "train loss:0.9194596770371387\n",
      "train loss:0.8605023184853617\n",
      "train loss:1.0082566111871503\n",
      "train loss:0.8452326279566316\n",
      "train loss:0.9084044206984351\n",
      "train loss:0.8938685936399654\n",
      "train loss:0.9552190782798534\n",
      "train loss:0.7999162188112477\n",
      "train loss:0.8123246315159435\n",
      "train loss:0.7434210034032843\n",
      "train loss:0.9016013591218607\n",
      "train loss:0.9805049123939983\n",
      "train loss:0.9656075275711581\n",
      "train loss:0.9934384661840159\n",
      "train loss:0.9741236423495278\n",
      "train loss:0.846814770215049\n",
      "train loss:0.9235466541379007\n",
      "train loss:0.9431642382114116\n",
      "train loss:0.8892593965662204\n",
      "train loss:0.8874343874332447\n",
      "train loss:0.959540076715523\n",
      "train loss:0.9269195458968437\n",
      "train loss:0.9209320983193702\n",
      "train loss:0.8110662691253213\n",
      "train loss:1.2094764401563476\n",
      "train loss:0.9248104437228716\n",
      "train loss:0.9355317478105585\n",
      "train loss:1.048651217275719\n",
      "train loss:1.0057334791638939\n",
      "train loss:0.923624213614826\n",
      "train loss:0.9230295782884546\n",
      "train loss:0.944794429726375\n",
      "train loss:0.8720681302539495\n",
      "train loss:0.773595233200421\n",
      "train loss:0.8119383792947447\n",
      "train loss:1.057292241495273\n",
      "train loss:1.040968101443337\n",
      "train loss:1.0527063130288716\n",
      "train loss:0.7927681359898594\n",
      "train loss:0.9740791547209712\n",
      "train loss:0.9521273039811824\n",
      "train loss:0.8725533985628791\n",
      "train loss:0.8656131989742926\n",
      "train loss:1.0272997471024632\n",
      "train loss:0.8864202140413052\n",
      "train loss:0.9438886605850009\n",
      "train loss:1.088587762170724\n",
      "train loss:0.9624341168311636\n",
      "train loss:0.9439853165857133\n",
      "train loss:1.0131331176443625\n",
      "train loss:0.9572003059404972\n",
      "train loss:1.0995785982426487\n",
      "train loss:1.029702497924858\n",
      "train loss:0.9625032558641118\n",
      "train loss:0.8876501338204617\n",
      "train loss:0.9493640155671649\n",
      "train loss:0.8757853954452135\n",
      "train loss:0.8796183307768211\n",
      "train loss:0.9549993043217816\n",
      "train loss:0.9533973701876057\n",
      "train loss:1.0622599138562665\n",
      "train loss:1.0075832534377511\n",
      "train loss:0.9750293670032655\n",
      "train loss:0.8378048481742391\n",
      "train loss:1.0453100453822866\n",
      "train loss:0.9755112260060443\n",
      "train loss:0.8644238291657358\n",
      "train loss:0.921998771138415\n",
      "train loss:0.8590209155432182\n",
      "train loss:0.9479005439521001\n",
      "train loss:0.9570827629433218\n",
      "train loss:0.8837668462182967\n",
      "train loss:0.9957725913873\n",
      "train loss:0.9871381855767446\n",
      "train loss:0.6933913000969325\n",
      "train loss:1.034189870894354\n",
      "train loss:0.8609683358735293\n",
      "train loss:1.1598129849678716\n",
      "train loss:0.791870015829336\n",
      "train loss:0.8782840464518901\n",
      "train loss:0.969578470051438\n",
      "train loss:0.8078886395425415\n",
      "train loss:0.9009935928477311\n",
      "train loss:1.0012275146858176\n",
      "train loss:1.0486412761719812\n",
      "train loss:0.8330166060082728\n",
      "train loss:0.8857111583493491\n",
      "train loss:0.9074504623007094\n",
      "train loss:0.8759598928767731\n",
      "train loss:1.0070794961275513\n",
      "train loss:1.084561944985961\n",
      "train loss:0.9269094065598368\n",
      "train loss:1.0022578456610145\n",
      "train loss:0.8541439450276158\n",
      "train loss:0.806643898168335\n",
      "train loss:0.962357471112718\n",
      "train loss:1.053401535494784\n",
      "train loss:0.7884165792345966\n",
      "train loss:0.8704101771035658\n",
      "train loss:0.9256683995684121\n",
      "train loss:0.7223611392769189\n",
      "train loss:1.0475457615952763\n",
      "train loss:0.8329787944926258\n",
      "train loss:0.8467085420599695\n",
      "train loss:0.9163652338995667\n",
      "train loss:0.6377984047468446\n",
      "train loss:1.1348874537998492\n",
      "train loss:0.9345147341517813\n",
      "train loss:0.8742787343731939\n",
      "train loss:1.0028832661812426\n",
      "train loss:0.9249484358760847\n",
      "train loss:0.928506075936842\n",
      "train loss:1.0043879994900098\n",
      "train loss:0.7921088556463188\n",
      "train loss:1.070627860438093\n",
      "train loss:1.005759248040383\n",
      "train loss:0.9596624243340898\n",
      "train loss:0.7070503812485994\n",
      "train loss:1.0449855724048702\n",
      "train loss:0.9785101887437985\n",
      "train loss:0.8777772123374136\n",
      "train loss:0.6938361642940891\n",
      "train loss:0.9838300047149375\n",
      "train loss:0.9551609399114267\n",
      "train loss:0.8215395873154202\n",
      "train loss:0.8114655321866421\n",
      "train loss:0.9814170436970082\n",
      "train loss:1.0064872964479243\n",
      "train loss:0.8488138722738722\n",
      "train loss:0.9526954749578838\n",
      "train loss:0.8098431762613575\n",
      "train loss:0.9540203400412254\n",
      "train loss:0.9290338582656099\n",
      "train loss:0.8479267424578911\n",
      "train loss:0.9283017175598625\n",
      "train loss:0.9050502904469847\n",
      "train loss:0.8530065008836781\n",
      "train loss:0.8838332620776136\n",
      "train loss:0.8681515294518601\n",
      "train loss:0.8252234848372585\n",
      "train loss:0.9131457342149177\n",
      "train loss:1.0508690446298363\n",
      "train loss:1.1768479378905952\n",
      "train loss:0.7206845308399659\n",
      "train loss:1.1643845611542143\n",
      "train loss:0.8380042994241435\n",
      "train loss:0.9465685221802846\n",
      "train loss:0.8967272982085764\n",
      "train loss:0.9270312365381318\n",
      "train loss:1.0180402835044935\n",
      "train loss:0.9718862747498993\n",
      "train loss:0.9396870856427394\n",
      "train loss:0.8242718614178834\n",
      "train loss:0.9235172005556429\n",
      "train loss:0.9631173872182128\n",
      "train loss:0.9203400986098764\n",
      "train loss:1.0844622025906203\n",
      "train loss:0.841927114161976\n",
      "train loss:0.802648692963615\n",
      "train loss:0.8645547608644396\n",
      "train loss:0.9759669526612655\n",
      "train loss:0.9971018941113979\n",
      "train loss:0.8592246460873753\n",
      "train loss:0.7961653177815572\n",
      "train loss:0.7755810323615574\n",
      "train loss:0.841260826859857\n",
      "train loss:1.0371171788219748\n",
      "train loss:0.9572052695963231\n",
      "train loss:1.0859741769377558\n",
      "train loss:0.9804917800490821\n",
      "train loss:0.9696503914223891\n",
      "train loss:0.8798990489296563\n",
      "train loss:0.9118679590448012\n",
      "train loss:0.9670785698656108\n",
      "train loss:1.0627620112266065\n",
      "train loss:1.0650578206559995\n",
      "train loss:0.8443646265660597\n",
      "train loss:0.8394532259506288\n",
      "train loss:0.9851899577532347\n",
      "train loss:0.8781264418439774\n",
      "train loss:1.207150266529844\n",
      "train loss:0.8086307301026533\n",
      "train loss:0.8950717112907276\n",
      "train loss:0.8797779314738666\n",
      "train loss:1.0363172277217865\n",
      "train loss:0.9381914621341874\n",
      "train loss:1.1086105495163365\n",
      "train loss:0.9897539956015459\n",
      "train loss:0.9561955432950611\n",
      "train loss:0.8347653793793998\n",
      "train loss:1.1260918427015607\n",
      "train loss:0.9875536351772638\n",
      "train loss:0.9152099958837673\n",
      "train loss:0.8073839202556706\n",
      "train loss:0.7641432916320019\n",
      "train loss:0.9773444152053683\n",
      "train loss:1.005982172462689\n",
      "train loss:0.9411291399727945\n",
      "train loss:0.8612147534821196\n",
      "train loss:0.8003035215568637\n",
      "train loss:0.7218815543460179\n",
      "train loss:0.933416611281627\n",
      "train loss:0.8094783229964245\n",
      "train loss:0.9069781012712032\n",
      "train loss:0.7445169097627733\n",
      "train loss:0.8481194125905204\n",
      "train loss:0.8816568133448348\n",
      "train loss:0.9055829619355977\n",
      "train loss:0.9246753153585131\n",
      "train loss:0.9543774362058479\n",
      "train loss:0.7020270447716029\n",
      "train loss:0.9815632721917291\n",
      "train loss:0.7379625829897057\n",
      "train loss:0.9745055483984946\n",
      "train loss:0.906837578499007\n",
      "train loss:0.8889069188543892\n",
      "train loss:0.7546867193524269\n",
      "train loss:0.8274324208567927\n",
      "train loss:0.8851750890646118\n",
      "train loss:0.9146824012671136\n",
      "train loss:1.1123928019912008\n",
      "train loss:1.107656760170744\n",
      "train loss:0.8847048262764632\n",
      "train loss:0.8390780053365277\n",
      "train loss:0.9888901129877464\n",
      "train loss:0.9608941144754667\n",
      "train loss:0.7828791350598812\n",
      "train loss:0.8900100899990274\n",
      "train loss:0.8899838611202007\n",
      "train loss:0.9782627041003299\n",
      "train loss:0.8986831731478305\n",
      "train loss:0.8839334614438836\n",
      "train loss:0.9016249478293292\n",
      "train loss:0.9035810149981424\n",
      "train loss:1.1057950440459243\n",
      "train loss:0.812780704465942\n",
      "train loss:0.9143908283929738\n",
      "train loss:0.9488632010987513\n",
      "train loss:0.9892706792214541\n",
      "train loss:0.8656967567559022\n",
      "train loss:0.9700855411205918\n",
      "train loss:0.8068456133324275\n",
      "train loss:0.7930906709425557\n",
      "train loss:0.9390032358255408\n",
      "train loss:0.8838108136996322\n",
      "train loss:0.9167693359329454\n",
      "train loss:0.9290740972568493\n",
      "train loss:0.845438159996467\n",
      "train loss:0.7371071068277647\n",
      "train loss:0.844568621413456\n",
      "train loss:0.8831297113037047\n",
      "train loss:0.9939122009242228\n",
      "train loss:0.9076221526673995\n",
      "train loss:0.9804781428922551\n",
      "train loss:1.0114761839600477\n",
      "train loss:1.0424280082116824\n",
      "train loss:0.9483586287918471\n",
      "train loss:0.9468876566840018\n",
      "train loss:1.010491918326928\n",
      "train loss:0.790048820898804\n",
      "train loss:0.9555023191046902\n",
      "train loss:1.119579549772575\n",
      "train loss:1.0793546185808656\n",
      "train loss:1.0837717282339014\n",
      "train loss:0.8568888159486836\n",
      "train loss:0.9823701849041153\n",
      "train loss:1.069301910750337\n",
      "train loss:1.0534129367120943\n",
      "train loss:0.8346954376961164\n",
      "train loss:0.8567397560441787\n",
      "train loss:0.798481606503532\n",
      "train loss:0.7551105477918536\n",
      "train loss:0.982340434187148\n",
      "train loss:0.9546285790407782\n",
      "train loss:0.8363997801775795\n",
      "train loss:0.7444300897133403\n",
      "train loss:0.9436318655927617\n",
      "train loss:0.8994971230305069\n",
      "train loss:1.0941468025932835\n",
      "train loss:0.8851555582233511\n",
      "train loss:0.9041279569791404\n",
      "train loss:0.8179276221330372\n",
      "train loss:1.0056896005449918\n",
      "train loss:0.7733018264724011\n",
      "train loss:0.7757677817573547\n",
      "train loss:0.8120947912250946\n",
      "train loss:0.9069217582732181\n",
      "train loss:0.8797410093399936\n",
      "train loss:0.9711255023799804\n",
      "train loss:0.822737592292066\n",
      "train loss:0.7129492345134187\n",
      "train loss:0.9878555645372299\n",
      "train loss:0.9096360598123551\n",
      "train loss:0.9614244428972958\n",
      "train loss:0.8079553641605842\n",
      "train loss:0.8793221965010648\n",
      "train loss:1.0109666897135856\n",
      "train loss:0.9115909098758718\n",
      "train loss:0.8609906026441037\n",
      "train loss:0.9101487780923583\n",
      "train loss:0.7435309380073706\n",
      "train loss:1.0091631177977687\n",
      "train loss:0.8098771305414025\n",
      "train loss:0.8361791667534949\n",
      "train loss:0.9404733594306203\n",
      "train loss:0.8641843986730692\n",
      "train loss:0.9640638708921397\n",
      "train loss:0.9753499007550299\n",
      "train loss:1.0033987293574127\n",
      "train loss:0.9452014464563476\n",
      "train loss:1.0022375598842652\n",
      "train loss:1.018031580494411\n",
      "train loss:0.9380157118169831\n",
      "train loss:0.8722671005925794\n",
      "train loss:0.9198835086778948\n",
      "train loss:0.8218679039184607\n",
      "train loss:0.8741639040808653\n",
      "train loss:0.8358703173515645\n",
      "train loss:0.8069890379371649\n",
      "train loss:1.1681501164086738\n",
      "train loss:0.9745954566882992\n",
      "train loss:1.0174587711066172\n",
      "train loss:0.9479630015130601\n",
      "train loss:0.8093301004964111\n",
      "train loss:0.8704917783173625\n",
      "train loss:0.9561899754947554\n",
      "train loss:0.7605435012452054\n",
      "train loss:0.8974197693216098\n",
      "train loss:0.9182670563597506\n",
      "train loss:0.997503035044036\n",
      "train loss:0.9824232915597342\n",
      "train loss:0.8981016969260324\n",
      "train loss:0.9145516633777848\n",
      "train loss:0.891676659617035\n",
      "train loss:0.902015738929635\n",
      "train loss:0.9889829226765768\n",
      "train loss:0.9969232621466889\n",
      "train loss:0.9441975673393347\n",
      "train loss:0.9862895064786714\n",
      "train loss:0.9109703381857828\n",
      "train loss:0.7040692497202435\n",
      "train loss:0.9358833675543282\n",
      "train loss:0.9877835840536281\n",
      "train loss:0.9212615365069636\n",
      "train loss:1.0401220000288809\n",
      "train loss:0.9497733750049646\n",
      "train loss:0.9865272695425628\n",
      "train loss:0.807572473051636\n",
      "train loss:0.8587550219755076\n",
      "train loss:1.0016512919301832\n",
      "train loss:0.9157748622057007\n",
      "train loss:0.8437481711103174\n",
      "train loss:0.7682626358298049\n",
      "train loss:0.8324887503764518\n",
      "train loss:0.9632395276556496\n",
      "train loss:0.9293403508587254\n",
      "train loss:0.8485320270256816\n",
      "train loss:0.8540611463235463\n",
      "train loss:0.9349485743285674\n",
      "train loss:0.883747893211394\n",
      "train loss:0.9735569582360257\n",
      "train loss:0.9248849543998715\n",
      "train loss:0.9102250231256293\n",
      "train loss:0.8651304627928453\n",
      "train loss:0.9332400964758973\n",
      "train loss:0.9806809133427842\n",
      "train loss:1.0710929860936063\n",
      "train loss:0.8907200385438584\n",
      "train loss:0.8586461265196956\n",
      "train loss:0.8910866525294473\n",
      "train loss:0.9084499569946751\n",
      "train loss:0.890132105708751\n",
      "train loss:0.87742656345059\n",
      "train loss:0.9201452528301556\n",
      "train loss:0.9116551211681236\n",
      "train loss:0.8360158575683541\n",
      "train loss:0.9623720196423006\n",
      "train loss:0.8518085726624407\n",
      "train loss:0.8303322919498223\n",
      "train loss:0.881862524013348\n",
      "train loss:0.7247934275495576\n",
      "train loss:0.8623602026322126\n",
      "train loss:0.9605737145015968\n",
      "train loss:1.0328231765815665\n",
      "train loss:1.1486817499395787\n",
      "train loss:0.9496618136319758\n",
      "train loss:0.950888129120808\n",
      "train loss:0.8462232817098214\n",
      "train loss:0.8543052156871955\n",
      "train loss:0.9783861008675743\n",
      "train loss:0.9178214564710224\n",
      "train loss:0.7666040221586802\n",
      "train loss:0.8396498697110808\n",
      "train loss:0.8302274261420467\n",
      "train loss:0.964234288328306\n",
      "train loss:0.9575030265510484\n",
      "train loss:0.9634269367241819\n",
      "train loss:0.9373807646485522\n",
      "train loss:0.8608366668572923\n",
      "train loss:0.7877922526795246\n",
      "train loss:0.9053940117055373\n",
      "train loss:0.9350763288263968\n",
      "train loss:0.8915300708417903\n",
      "train loss:0.9168488670024455\n",
      "train loss:0.8657815703404598\n",
      "train loss:0.9891623909952433\n",
      "train loss:0.8756043325707562\n",
      "train loss:1.11968180130001\n",
      "train loss:0.8722634973586689\n",
      "train loss:0.9866906561674604\n",
      "train loss:0.9543230921341563\n",
      "train loss:0.9015440746050563\n",
      "train loss:0.9477592108243819\n",
      "train loss:0.8653272709164564\n",
      "train loss:0.865385681846789\n",
      "train loss:0.822831580691725\n",
      "train loss:0.8153872718143608\n",
      "train loss:0.9882985287009718\n",
      "train loss:1.0298439299868165\n",
      "train loss:0.8710441671834016\n",
      "train loss:0.8265038707441795\n",
      "train loss:0.8725486411822767\n",
      "train loss:0.9099772772418858\n",
      "train loss:0.935508966217467\n",
      "train loss:0.8828379311173697\n",
      "train loss:0.9825859707753274\n",
      "train loss:0.9179192827164446\n",
      "train loss:0.9297365508746758\n",
      "train loss:0.8465586869953639\n",
      "train loss:0.8060717018861745\n",
      "train loss:0.9255881003226846\n",
      "train loss:0.9481537472870322\n",
      "train loss:0.8721779736600901\n",
      "train loss:0.9475752590493591\n",
      "=== epoch:6, train acc:0.992, test acc:0.99 ===\n",
      "train loss:0.880189840178768\n",
      "train loss:0.9609334682145981\n",
      "train loss:1.0281089014076272\n",
      "train loss:0.7887642271630324\n",
      "train loss:0.8891399425935929\n",
      "train loss:0.7778612532576659\n",
      "train loss:0.9240272731015573\n",
      "train loss:0.8889186244332353\n",
      "train loss:0.7174686472099835\n",
      "train loss:0.9375516236814337\n",
      "train loss:0.9206688834716573\n",
      "train loss:0.8380374719695953\n",
      "train loss:0.9750576642995032\n",
      "train loss:0.9688053183832794\n",
      "train loss:0.8249576909359158\n",
      "train loss:0.8346120266096626\n",
      "train loss:1.0100674102884055\n",
      "train loss:0.8678189384534086\n",
      "train loss:0.9864218850453251\n",
      "train loss:0.9760435044793744\n",
      "train loss:0.9015208866481339\n",
      "train loss:1.0277355338531722\n",
      "train loss:0.8402503255719442\n",
      "train loss:0.8340623842589692\n",
      "train loss:0.9424785693590375\n",
      "train loss:0.9589735433932498\n",
      "train loss:1.028663490370447\n",
      "train loss:0.924197932108332\n",
      "train loss:0.9314494364925376\n",
      "train loss:1.0123860980149537\n",
      "train loss:0.8246192410723894\n",
      "train loss:0.9491245930699644\n",
      "train loss:0.8099940037110757\n",
      "train loss:0.9470985760020636\n",
      "train loss:0.9954892944206034\n",
      "train loss:1.0335329686358705\n",
      "train loss:0.7741453576411433\n",
      "train loss:0.8047553814140689\n",
      "train loss:0.8716161776301005\n",
      "train loss:0.9241474731112339\n",
      "train loss:1.0694462340817301\n",
      "train loss:0.91690266585435\n",
      "train loss:0.9519431281185856\n",
      "train loss:0.8495543295853929\n",
      "train loss:0.9089236759573367\n",
      "train loss:0.9185751635891599\n",
      "train loss:0.8477551677379754\n",
      "train loss:0.8934847105441845\n",
      "train loss:0.9783715044291443\n",
      "train loss:1.0673372947669924\n",
      "train loss:0.8802974143281318\n",
      "train loss:0.8915442094782731\n",
      "train loss:0.8918516509803289\n",
      "train loss:0.9954543009372775\n",
      "train loss:0.820434148297873\n",
      "train loss:0.9017707878426602\n",
      "train loss:0.811019947958818\n",
      "train loss:0.8248313731008912\n",
      "train loss:0.9616453364571869\n",
      "train loss:0.9008856991075639\n",
      "train loss:0.9330659365515082\n",
      "train loss:0.9408607763557134\n",
      "train loss:0.830029160030963\n",
      "train loss:0.9804986218347375\n",
      "train loss:0.926484318204786\n",
      "train loss:0.8769224191219598\n",
      "train loss:0.9268244659866016\n",
      "train loss:0.8648455136783271\n",
      "train loss:0.9624189113959645\n",
      "train loss:0.780737538057034\n",
      "train loss:0.891053510098903\n",
      "train loss:0.8680606978112033\n",
      "train loss:0.9197073904475315\n",
      "train loss:0.8432597545119138\n",
      "train loss:0.7980878232367239\n",
      "train loss:0.8935971840234955\n",
      "train loss:1.0568139689508844\n",
      "train loss:0.8513575587932319\n",
      "train loss:0.8790790190850255\n",
      "train loss:0.9520784592590538\n",
      "train loss:0.6995935419977815\n",
      "train loss:0.8392166217114512\n",
      "train loss:0.7158202063029087\n",
      "train loss:0.8928670260316685\n",
      "train loss:0.7891548072929427\n",
      "train loss:1.1078082377915732\n",
      "train loss:0.9699873979014082\n",
      "train loss:0.9929930711217034\n",
      "train loss:0.8245496479784806\n",
      "train loss:0.8581375323083762\n",
      "train loss:0.9088210660540769\n",
      "train loss:0.8842493198665872\n",
      "train loss:0.8974460818029898\n",
      "train loss:0.9178830204950539\n",
      "train loss:1.0441510670315297\n",
      "train loss:0.9643266041278608\n",
      "train loss:1.0385707329310505\n",
      "train loss:0.8849329318764687\n",
      "train loss:0.7818662662255589\n",
      "train loss:0.9045745015333801\n",
      "train loss:0.9471795126521276\n",
      "train loss:1.0627056189437127\n",
      "train loss:0.9744727937161608\n",
      "train loss:0.9110941282167475\n",
      "train loss:0.9409102342742669\n",
      "train loss:0.9437082952574057\n",
      "train loss:1.0028576577844464\n",
      "train loss:0.9670363942091541\n",
      "train loss:0.8995013630679461\n",
      "train loss:0.8583197813812261\n",
      "train loss:0.9126605522729483\n",
      "train loss:0.9848332510119553\n",
      "train loss:0.9648202498999221\n",
      "train loss:1.2225951091539498\n",
      "train loss:1.0214143918084297\n",
      "train loss:0.8942394299560991\n",
      "train loss:0.758064636132136\n",
      "train loss:0.8534060986466305\n",
      "train loss:1.0749455193066104\n",
      "train loss:0.8444704649647473\n",
      "train loss:0.8962155461430095\n",
      "train loss:0.771271891681866\n",
      "train loss:0.8523794289316649\n",
      "train loss:0.9284382153020048\n",
      "train loss:0.8283556535665744\n",
      "train loss:0.9720651722373078\n",
      "train loss:1.0026145994498485\n",
      "train loss:0.7800375159150131\n",
      "train loss:1.0407772042696302\n",
      "train loss:0.7826747633413005\n",
      "train loss:1.0077845598449824\n",
      "train loss:0.7439446495537407\n",
      "train loss:0.9804340329106715\n",
      "train loss:0.8826847690768668\n",
      "train loss:0.8145187962166561\n",
      "train loss:1.048221110783864\n",
      "train loss:0.9974545698682228\n",
      "train loss:0.7429323067763787\n",
      "train loss:0.9721408549377654\n",
      "train loss:1.0380556742617002\n",
      "train loss:0.9340102980476409\n",
      "train loss:0.7931816974108387\n",
      "train loss:0.8469439343484989\n",
      "train loss:0.7936723087541416\n",
      "train loss:0.9688854113475891\n",
      "train loss:0.9840551584704399\n",
      "train loss:0.6811566817208049\n",
      "train loss:0.8121350970940494\n",
      "train loss:0.856608904160741\n",
      "train loss:0.9800068592685746\n",
      "train loss:0.7129254792603241\n",
      "train loss:0.8244394894675662\n",
      "train loss:0.9599525510218454\n",
      "train loss:0.8225607894126683\n",
      "train loss:0.9484606155489252\n",
      "train loss:0.8949500372438195\n",
      "train loss:1.0252397565632902\n",
      "train loss:0.9556127776222766\n",
      "train loss:0.9610321324174214\n",
      "train loss:1.0539493555326929\n",
      "train loss:0.8875495546855173\n",
      "train loss:0.8886567611371434\n",
      "train loss:0.8384474314512735\n",
      "train loss:0.9245851432991475\n",
      "train loss:0.650319645935435\n",
      "train loss:0.908472460769751\n",
      "train loss:0.9087844099513964\n",
      "train loss:0.7093976104093556\n",
      "train loss:0.9421929337009998\n",
      "train loss:0.9274286801929751\n",
      "train loss:1.027871631955664\n",
      "train loss:0.8634312822031811\n",
      "train loss:1.027047638238688\n",
      "train loss:0.8007187088615864\n",
      "train loss:0.7677714305874506\n",
      "train loss:0.8849446728196055\n",
      "train loss:0.9346220550769435\n",
      "train loss:0.7085047008336224\n",
      "train loss:1.0632235625714623\n",
      "train loss:1.013925815756727\n",
      "train loss:0.9599101869759863\n",
      "train loss:0.8210381827313309\n",
      "train loss:0.8794035471938969\n",
      "train loss:0.8281504700387553\n",
      "train loss:0.7748116890454577\n",
      "train loss:0.9018181133323008\n",
      "train loss:0.9938101340077466\n",
      "train loss:0.8784612299382908\n",
      "train loss:0.8952459145826994\n",
      "train loss:1.0437146833747477\n",
      "train loss:0.9772192257568623\n",
      "train loss:1.029470445502554\n",
      "train loss:0.9310033232016149\n",
      "train loss:0.6703680788825548\n",
      "train loss:0.8969006373031994\n",
      "train loss:0.6943007977541008\n",
      "train loss:0.8414988438770314\n",
      "train loss:0.9612611584142094\n",
      "train loss:0.8465429469350501\n",
      "train loss:0.7935258505038933\n",
      "train loss:0.8553317278793179\n",
      "train loss:0.9067015458084313\n",
      "train loss:0.9728122290531189\n",
      "train loss:0.8798973149949814\n",
      "train loss:0.9144766134411642\n",
      "train loss:0.9240110010101494\n",
      "train loss:0.9139612950122724\n",
      "train loss:0.8740315714296756\n",
      "train loss:0.8563807850291432\n",
      "train loss:0.6872598508638729\n",
      "train loss:0.993882459793649\n",
      "train loss:1.0189111508053252\n",
      "train loss:0.9112957320548365\n",
      "train loss:0.9085893115308766\n",
      "train loss:0.8567499324635119\n",
      "train loss:0.731046848267308\n",
      "train loss:0.8337989891934637\n",
      "train loss:0.7722374692446147\n",
      "train loss:1.052195962312559\n",
      "train loss:0.9619845540757236\n",
      "train loss:0.9335685873085734\n",
      "train loss:0.9740878556759358\n",
      "train loss:1.0593214614149447\n",
      "train loss:1.0414938880965137\n",
      "train loss:0.7660779701160759\n",
      "train loss:0.7233930206739057\n",
      "train loss:0.8973428926899217\n",
      "train loss:0.9229692792176784\n",
      "train loss:1.0271338799416923\n",
      "train loss:1.0207242686990268\n",
      "train loss:0.9583085013245837\n",
      "train loss:0.8227780781387333\n",
      "train loss:0.9112773283228495\n",
      "train loss:0.9983693573828516\n",
      "train loss:0.8793674224315287\n",
      "train loss:0.8214853775472142\n",
      "train loss:0.9714053398811937\n",
      "train loss:0.9431224877488389\n",
      "train loss:1.0593566686754092\n",
      "train loss:0.9234939304429974\n",
      "train loss:0.9506473772461745\n",
      "train loss:0.9116016714276474\n",
      "train loss:0.9014974618520951\n",
      "train loss:0.8044565390073892\n",
      "train loss:0.8095138101484777\n",
      "train loss:0.7900183490808225\n",
      "train loss:1.1590560663925202\n",
      "train loss:0.9705261109323383\n",
      "train loss:0.8951908994552004\n",
      "train loss:0.8788897473174302\n",
      "train loss:0.9506832247642142\n",
      "train loss:0.9165253704433192\n",
      "train loss:0.9174380333162653\n",
      "train loss:0.9362590690205789\n",
      "train loss:0.9892945431569862\n",
      "train loss:0.9377233690925151\n",
      "train loss:0.9416350124851748\n",
      "train loss:0.8308728673862273\n",
      "train loss:1.0451872981467094\n",
      "train loss:0.9473738234314766\n",
      "train loss:0.8764045034111132\n",
      "train loss:0.7810137292409849\n",
      "train loss:0.9550297047581319\n",
      "train loss:0.882040934114602\n",
      "train loss:0.9317704921344252\n",
      "train loss:1.0209026623612423\n",
      "train loss:0.8464807585784426\n",
      "train loss:0.7806661188050075\n",
      "train loss:0.7843465800038589\n",
      "train loss:0.7425456813391206\n",
      "train loss:0.9855620202959224\n",
      "train loss:0.9628145427370502\n",
      "train loss:0.7551268444559281\n",
      "train loss:0.8828664617908014\n",
      "train loss:0.9550513888273693\n",
      "train loss:0.9246977762150415\n",
      "train loss:0.8542590936288399\n",
      "train loss:0.8397225690256097\n",
      "train loss:0.8510638964914218\n",
      "train loss:0.8008998700140705\n",
      "train loss:1.0398301831762273\n",
      "train loss:0.739273816430855\n",
      "train loss:1.0175020677297364\n",
      "train loss:0.8579624600355871\n",
      "train loss:0.9331112179821046\n",
      "train loss:0.945618631914334\n",
      "train loss:0.7249248389616142\n",
      "train loss:0.7712053314652416\n",
      "train loss:0.8539656109047415\n",
      "train loss:0.9541664872563405\n",
      "train loss:0.896379776188695\n",
      "train loss:0.8776741659393704\n",
      "train loss:0.8641151679645944\n",
      "train loss:0.9719567200225405\n",
      "train loss:0.9230693730261007\n",
      "train loss:0.9052799251887923\n",
      "train loss:0.8719864349716623\n",
      "train loss:0.929038493874901\n",
      "train loss:0.7050908644637388\n",
      "train loss:0.8503415972965965\n",
      "train loss:0.8101252112795391\n",
      "train loss:0.8119814818806566\n",
      "train loss:1.0093334511681031\n",
      "train loss:0.951310392464832\n",
      "train loss:0.7845385996527021\n",
      "train loss:0.9186280952474783\n",
      "train loss:1.0233237482833468\n",
      "train loss:0.9753475696032038\n",
      "train loss:0.859270841157104\n",
      "train loss:0.9007462830868908\n",
      "train loss:0.8047050830454738\n",
      "train loss:0.7602353976226895\n",
      "train loss:1.073355196524764\n",
      "train loss:0.872271995080636\n",
      "train loss:0.9526339644413129\n",
      "train loss:0.9522529576491615\n",
      "train loss:0.8706012659952823\n",
      "train loss:0.8613382258490908\n",
      "train loss:0.9415432265624866\n",
      "train loss:1.0042516007204818\n",
      "train loss:0.7990141668989108\n",
      "train loss:0.9276822882802409\n",
      "train loss:1.009759671602855\n",
      "train loss:0.8031712253733068\n",
      "train loss:0.8090480404344906\n",
      "train loss:0.8104511235523257\n",
      "train loss:1.008671848707777\n",
      "train loss:0.9361230246805731\n",
      "train loss:0.9283869716914834\n",
      "train loss:0.9367193767647396\n",
      "train loss:0.9168855515825223\n",
      "train loss:0.8417450857634485\n",
      "train loss:0.9728481742357451\n",
      "train loss:0.9295500997454758\n",
      "train loss:0.8829352591826892\n",
      "train loss:1.0083272818770244\n",
      "train loss:0.9510000760274869\n",
      "train loss:0.9701247989547652\n",
      "train loss:0.8911721723361232\n",
      "train loss:0.8064716742662407\n",
      "train loss:0.9322096620989675\n",
      "train loss:0.9213168547973848\n",
      "train loss:1.1080769131328296\n",
      "train loss:0.9139247462571555\n",
      "train loss:0.8925421180099989\n",
      "train loss:0.8550441613763458\n",
      "train loss:0.8471695306970025\n",
      "train loss:0.9793505730479667\n",
      "train loss:0.8530456941691373\n",
      "train loss:0.7614651453587379\n",
      "train loss:0.7348549424436785\n",
      "train loss:0.8951739826050884\n",
      "train loss:1.1262276358781171\n",
      "train loss:0.7618200587290456\n",
      "train loss:0.7412197487397105\n",
      "train loss:0.8788245406659815\n",
      "train loss:1.024956513150825\n",
      "train loss:0.7557024354284155\n",
      "train loss:0.8254648596407135\n",
      "train loss:1.2632380895558368\n",
      "train loss:1.0265448122475305\n",
      "train loss:0.7910365976462495\n",
      "train loss:0.8524538536116947\n",
      "train loss:0.9656866566591986\n",
      "train loss:1.0238924139372865\n",
      "train loss:0.9685024650042573\n",
      "train loss:1.0653084073538055\n",
      "train loss:0.9802857136012584\n",
      "train loss:0.9492542041882424\n",
      "train loss:1.067353210777746\n",
      "train loss:0.8048393721219314\n",
      "train loss:0.855025716909168\n",
      "train loss:1.0885958749424416\n",
      "train loss:0.863038137143329\n",
      "train loss:0.926867749177628\n",
      "train loss:0.9670883135991479\n",
      "train loss:0.8601159023206998\n",
      "train loss:0.8856191833877207\n",
      "train loss:0.8965332241242314\n",
      "train loss:0.8375055124750129\n",
      "train loss:0.9075305538228262\n",
      "train loss:0.8873311765933306\n",
      "train loss:0.832690925017014\n",
      "train loss:0.9662067552480497\n",
      "train loss:0.8953907228030643\n",
      "train loss:0.8349558078137213\n",
      "train loss:1.0018755141857891\n",
      "train loss:0.9132488145877179\n",
      "train loss:0.9397094205372168\n",
      "train loss:0.8557384181286962\n",
      "train loss:0.8792628069364439\n",
      "train loss:0.7670241675576467\n",
      "train loss:0.8867668840933538\n",
      "train loss:0.6844915306255802\n",
      "train loss:0.8050618622934546\n",
      "train loss:0.9476004950002473\n",
      "train loss:0.8051302812789922\n",
      "train loss:0.995353841540204\n",
      "train loss:1.1457661656348501\n",
      "train loss:0.7695740943751584\n",
      "train loss:0.8398992494695458\n",
      "train loss:1.093290260964805\n",
      "train loss:0.8177514900288159\n",
      "train loss:0.830085814597994\n",
      "train loss:0.9092088155232139\n",
      "train loss:0.9489646975365852\n",
      "train loss:0.9499161443316113\n",
      "train loss:0.9021901918545645\n",
      "train loss:0.9176158234444739\n",
      "train loss:1.0354897733106825\n",
      "train loss:0.8630425831824483\n",
      "train loss:0.8671687398748166\n",
      "train loss:1.0808028226289403\n",
      "train loss:0.8862281771733019\n",
      "train loss:0.8076670732963741\n",
      "train loss:0.9073767726530408\n",
      "train loss:0.7912391748013722\n",
      "train loss:0.8278061501685267\n",
      "train loss:0.8805069846308784\n",
      "train loss:0.8446850156134463\n",
      "train loss:0.8785754390251864\n",
      "train loss:0.8296149468775798\n",
      "train loss:0.9285833880709522\n",
      "train loss:0.853488296503319\n",
      "train loss:1.043510761750814\n",
      "train loss:0.8660067344988126\n",
      "train loss:0.7767750185384504\n",
      "train loss:0.7734560446158042\n",
      "train loss:0.9789661021879755\n",
      "train loss:1.0706613259761353\n",
      "train loss:0.9931464479142305\n",
      "train loss:0.9025703858791934\n",
      "train loss:0.9810814562231962\n",
      "train loss:1.0731570285640935\n",
      "train loss:0.9399925909302711\n",
      "train loss:0.9264847210013933\n",
      "train loss:0.7963718746191403\n",
      "train loss:0.8891214125954581\n",
      "train loss:1.037013532002541\n",
      "train loss:0.8211048939012053\n",
      "train loss:0.8807033575890547\n",
      "train loss:1.0691797152917097\n",
      "train loss:1.0251853961014765\n",
      "train loss:1.0271010306381163\n",
      "train loss:1.0859783456260663\n",
      "train loss:0.7112834413277627\n",
      "train loss:0.7369401825470225\n",
      "train loss:0.849204162017223\n",
      "train loss:0.7904459251494074\n",
      "train loss:0.8712629382743023\n",
      "train loss:0.8719639076018544\n",
      "train loss:0.981858620904692\n",
      "train loss:0.8754796340071641\n",
      "train loss:1.0888799669164484\n",
      "train loss:0.8616373019933077\n",
      "train loss:0.7812893888394236\n",
      "train loss:0.8801844648397059\n",
      "train loss:0.7863262775139518\n",
      "train loss:0.9223475240552531\n",
      "train loss:0.9585043090303316\n",
      "train loss:0.8579122095376421\n",
      "train loss:0.8219526970613015\n",
      "train loss:0.6242381340643117\n",
      "train loss:0.8632746879738746\n",
      "train loss:0.9569374153896352\n",
      "train loss:0.9813537395901994\n",
      "train loss:0.9081120599831022\n",
      "train loss:0.9546090100414333\n",
      "train loss:0.7375526578138554\n",
      "train loss:0.8788177017512971\n",
      "train loss:0.75608068360726\n",
      "train loss:0.8886252779785129\n",
      "train loss:0.8364194934218662\n",
      "train loss:0.9581733099486045\n",
      "train loss:0.9426807289624718\n",
      "train loss:0.8779569699918558\n",
      "train loss:0.7561533175112295\n",
      "train loss:0.773247886797437\n",
      "train loss:0.6755999534239259\n",
      "train loss:0.9296234461375069\n",
      "train loss:0.9256714008005921\n",
      "train loss:0.9949482680183932\n",
      "train loss:0.8813901123754466\n",
      "train loss:0.8308409744325778\n",
      "train loss:1.1093966595028066\n",
      "train loss:0.841383507194099\n",
      "train loss:1.0876592566950662\n",
      "train loss:0.8060502231008261\n",
      "train loss:0.7167040444485484\n",
      "train loss:0.8492210508336002\n",
      "train loss:0.9288324881192949\n",
      "train loss:0.967067817472172\n",
      "train loss:0.7972400679812941\n",
      "train loss:0.8035141468964488\n",
      "train loss:0.9662414359146063\n",
      "train loss:0.9395572845098391\n",
      "train loss:0.7669800820873595\n",
      "train loss:0.8982482991712348\n",
      "train loss:1.0253733421208566\n",
      "train loss:0.9445585181606375\n",
      "train loss:0.7494604089738539\n",
      "train loss:0.8546069680744786\n",
      "train loss:0.9338673679563365\n",
      "train loss:0.7982880672907483\n",
      "train loss:0.939714924422734\n",
      "train loss:0.9612392409019063\n",
      "train loss:0.8376657100491659\n",
      "train loss:0.8810648502509002\n",
      "train loss:0.8565030357950593\n",
      "train loss:0.8808946628627392\n",
      "train loss:0.8888567446319517\n",
      "train loss:0.8050648658623406\n",
      "train loss:0.8431745079691059\n",
      "train loss:0.9343872153482264\n",
      "train loss:0.9163674358524659\n",
      "train loss:0.9814743274483404\n",
      "train loss:0.8462384444834593\n",
      "train loss:0.9127870686829114\n",
      "train loss:0.8830430166015705\n",
      "train loss:0.8810527626060869\n",
      "train loss:0.9034684244708887\n",
      "train loss:0.9656673616913988\n",
      "train loss:0.9494140374647513\n",
      "train loss:0.877003398784036\n",
      "train loss:0.8840444739619541\n",
      "train loss:0.9698357323228235\n",
      "train loss:0.7614784279641517\n",
      "train loss:0.8950094996144977\n",
      "train loss:1.0045936776333617\n",
      "train loss:1.1472453292857028\n",
      "train loss:0.8611789556785735\n",
      "train loss:0.8876062929256076\n",
      "train loss:1.0353692213127843\n",
      "train loss:1.027146280803069\n",
      "train loss:0.987522874297004\n",
      "train loss:0.93453940604172\n",
      "train loss:0.8330361747661713\n",
      "train loss:0.8392666902952404\n",
      "train loss:0.9100475717312163\n",
      "train loss:1.0035230669797257\n",
      "train loss:1.2041068585513157\n",
      "train loss:0.9310924389904999\n",
      "train loss:0.8961027721408271\n",
      "train loss:0.8596553506787504\n",
      "train loss:0.9523799758850076\n",
      "train loss:0.9802819914398438\n",
      "train loss:0.9248345476175162\n",
      "train loss:0.8307760501803461\n",
      "train loss:0.8264118637312143\n",
      "train loss:0.8839975425284835\n",
      "train loss:0.8957795019880269\n",
      "train loss:0.8458818031905752\n",
      "train loss:0.9187722756412436\n",
      "train loss:0.9389134553174086\n",
      "train loss:0.9604154558301555\n",
      "train loss:0.9862055897897397\n",
      "train loss:0.7806747868737343\n",
      "train loss:0.8366520361253071\n",
      "train loss:0.8874886710042108\n",
      "train loss:0.9968396105149381\n",
      "train loss:0.946677979924056\n",
      "train loss:0.8202122129839889\n",
      "train loss:0.862338150016524\n",
      "train loss:0.8096085374238536\n",
      "train loss:0.9375072825573819\n",
      "train loss:1.1241331582126006\n",
      "train loss:0.9261325981138003\n",
      "train loss:0.8915127417996176\n",
      "train loss:0.898206008408553\n",
      "train loss:0.7339794517747993\n",
      "train loss:0.8828518161654898\n",
      "train loss:0.8988371312606251\n",
      "train loss:0.9231955176655454\n",
      "train loss:0.7657089243500872\n",
      "train loss:0.9921624852735161\n",
      "train loss:1.0935508125006101\n",
      "train loss:0.9388149609874964\n",
      "train loss:0.7779825235267193\n",
      "train loss:0.8995962744814296\n",
      "train loss:0.847228079844563\n",
      "train loss:0.8628877516784832\n",
      "train loss:1.0045881448903125\n",
      "train loss:0.7496257846798526\n",
      "train loss:0.7270904716459574\n",
      "train loss:0.8743061566290571\n",
      "train loss:1.0374234692730637\n",
      "train loss:0.8439731451835929\n",
      "train loss:0.8562045804295321\n",
      "train loss:0.8095223566951225\n",
      "train loss:0.9941780272491908\n",
      "train loss:0.8132625805263405\n",
      "train loss:0.7912821050328889\n",
      "train loss:1.1703038589707127\n",
      "train loss:0.7606741284258696\n",
      "train loss:0.8279397476126924\n",
      "train loss:0.7209445632188016\n",
      "train loss:0.9632213778365336\n",
      "train loss:0.9440910848828536\n",
      "train loss:1.0144365524792003\n",
      "train loss:0.9855600016098757\n",
      "=== epoch:7, train acc:0.991, test acc:0.989 ===\n",
      "train loss:0.8617043288873397\n",
      "train loss:0.7960791775834889\n",
      "train loss:0.8965022347711123\n",
      "train loss:0.9952957581205452\n",
      "train loss:0.7925092567623331\n",
      "train loss:0.7805966630345388\n",
      "train loss:0.9119478893035342\n",
      "train loss:0.8378353656636736\n",
      "train loss:0.8671784666674729\n",
      "train loss:0.7741359079504574\n",
      "train loss:0.8728892160565646\n",
      "train loss:0.8641123327620016\n",
      "train loss:0.9286639244235666\n",
      "train loss:0.9981377692488705\n",
      "train loss:0.9961390981360801\n",
      "train loss:0.7465272638333283\n",
      "train loss:0.9714489153879466\n",
      "train loss:0.9364885625647621\n",
      "train loss:0.9560766698382591\n",
      "train loss:1.10755167499154\n",
      "train loss:0.9360132058475966\n",
      "train loss:0.6999156911760187\n",
      "train loss:1.050760532989321\n",
      "train loss:0.9166251833070659\n",
      "train loss:0.8578785838667844\n",
      "train loss:0.8280699002459012\n",
      "train loss:0.9456686683856806\n",
      "train loss:0.9452167781953755\n",
      "train loss:0.8820332904236747\n",
      "train loss:0.7501571553655935\n",
      "train loss:0.9258000190127466\n",
      "train loss:0.9092403372193918\n",
      "train loss:0.8512350396480456\n",
      "train loss:0.9286207546555267\n",
      "train loss:0.8927067300840216\n",
      "train loss:1.1245064394163842\n",
      "train loss:1.0022541111623635\n",
      "train loss:0.820712841874507\n",
      "train loss:1.0002908668862827\n",
      "train loss:0.7727424188130047\n",
      "train loss:0.8706777531698816\n",
      "train loss:0.894141081368849\n",
      "train loss:0.8515877574471524\n",
      "train loss:0.9887262329797\n",
      "train loss:0.8181598367421884\n",
      "train loss:1.0554984865812245\n",
      "train loss:0.9122910425876707\n",
      "train loss:1.0558881761660275\n",
      "train loss:0.8074487461269604\n",
      "train loss:0.8569008319642607\n",
      "train loss:1.011100598999964\n",
      "train loss:1.1082444768714521\n",
      "train loss:0.852173078652818\n",
      "train loss:0.988318663365112\n",
      "train loss:0.8496079655378549\n",
      "train loss:1.0220105653013818\n",
      "train loss:0.7397446706322403\n",
      "train loss:0.8724357544140156\n",
      "train loss:0.9805326979444249\n",
      "train loss:0.9010384940657157\n",
      "train loss:1.0202916746025383\n",
      "train loss:0.8811135098326364\n",
      "train loss:0.8910736148718713\n",
      "train loss:0.9418195655340013\n",
      "train loss:1.0627241843109305\n",
      "train loss:0.967773623924916\n",
      "train loss:0.7998615412244433\n",
      "train loss:0.9603129210947731\n",
      "train loss:0.929642854728531\n",
      "train loss:0.8181041453420822\n",
      "train loss:0.8980742085196605\n",
      "train loss:0.8165513137916782\n",
      "train loss:0.8629781135106281\n",
      "train loss:0.8546611106077926\n",
      "train loss:0.9019142475964741\n",
      "train loss:0.897717591495987\n",
      "train loss:0.793836571286064\n",
      "train loss:0.8936873513727552\n",
      "train loss:1.1407002690090082\n",
      "train loss:1.1475451918140163\n",
      "train loss:1.0867733106196524\n",
      "train loss:0.736307927876179\n",
      "train loss:0.9254160095793575\n",
      "train loss:1.027417540413082\n",
      "train loss:1.0610181107927772\n",
      "train loss:0.9660768063222379\n",
      "train loss:0.8336667588690141\n",
      "train loss:0.8551043002295536\n",
      "train loss:0.8938559473716882\n",
      "train loss:0.9074859841794652\n",
      "train loss:0.6647891193252594\n",
      "train loss:0.7881392884982565\n",
      "train loss:0.8726602376100768\n",
      "train loss:0.9124381312505355\n",
      "train loss:1.0667672504936703\n",
      "train loss:0.9607577297099471\n",
      "train loss:1.0136763072946942\n",
      "train loss:0.8788887247757553\n",
      "train loss:0.9798065808319986\n",
      "train loss:1.036412668797623\n",
      "train loss:0.8649201812135457\n",
      "train loss:0.8940810045243454\n",
      "train loss:0.7475637429598901\n",
      "train loss:0.9173184516636317\n",
      "train loss:1.0097487841928163\n",
      "train loss:0.823381606090459\n",
      "train loss:0.875545984030356\n",
      "train loss:0.8354327809193576\n",
      "train loss:0.949388067856691\n",
      "train loss:0.9890196776425827\n",
      "train loss:0.8194883101357675\n",
      "train loss:0.8333702301983006\n",
      "train loss:0.7079044832560955\n",
      "train loss:0.7848040546679479\n",
      "train loss:0.8729589309590357\n",
      "train loss:0.9032909615807327\n",
      "train loss:0.8332532521653568\n",
      "train loss:1.1001678816252132\n",
      "train loss:0.9941207224927101\n",
      "train loss:0.8500506734461044\n",
      "train loss:1.0374874937414658\n",
      "train loss:0.8289214243278816\n",
      "train loss:0.8637843674243791\n",
      "train loss:0.9151597721327337\n",
      "train loss:0.883463784992959\n",
      "train loss:1.0414912411746853\n",
      "train loss:0.7909610560686389\n",
      "train loss:0.7894604754145896\n",
      "train loss:0.9157461699532157\n",
      "train loss:0.9969449573551795\n",
      "train loss:0.9591899604868732\n",
      "train loss:1.000546088065763\n",
      "train loss:1.0550432770684604\n",
      "train loss:0.8796466891410474\n",
      "train loss:0.9697833718424209\n",
      "train loss:0.8504435830229236\n",
      "train loss:0.8499641711746464\n",
      "train loss:0.9383641700077751\n",
      "train loss:0.8542362824510257\n",
      "train loss:0.8197848031738271\n",
      "train loss:0.702797324822919\n",
      "train loss:0.8433795402427584\n",
      "train loss:0.9392718272487747\n",
      "train loss:0.8217995555854798\n",
      "train loss:0.8915279588971708\n",
      "train loss:1.0236617261337593\n",
      "train loss:0.9761883906634219\n",
      "train loss:0.8804220303198584\n",
      "train loss:0.9229418283909991\n",
      "train loss:0.8884769617845479\n",
      "train loss:0.885330266384059\n",
      "train loss:0.8934369652160266\n",
      "train loss:0.9783777098668864\n",
      "train loss:0.9130407192313464\n",
      "train loss:0.9321441479275415\n",
      "train loss:0.8305891141799602\n",
      "train loss:0.8842669029766591\n",
      "train loss:0.947939751556534\n",
      "train loss:0.8896904574253364\n",
      "train loss:0.9798789435819799\n",
      "train loss:0.9026304062991367\n",
      "train loss:0.9853273126630829\n",
      "train loss:0.9512760656679076\n",
      "train loss:0.7790205275834917\n",
      "train loss:1.006780545244305\n",
      "train loss:1.0099076911306617\n",
      "train loss:0.8200347541919472\n",
      "train loss:0.965027035287267\n",
      "train loss:0.9647870564221291\n",
      "train loss:0.9781974865366414\n",
      "train loss:0.8316632186805195\n",
      "train loss:0.878076234325739\n",
      "train loss:0.864758653222871\n",
      "train loss:0.8966559276781036\n",
      "train loss:0.743343577094828\n",
      "train loss:0.9399565762290172\n",
      "train loss:1.1094867225420604\n",
      "train loss:0.9846521825128672\n",
      "train loss:0.854617755078015\n",
      "train loss:0.8166812831070339\n",
      "train loss:1.0278951643909178\n",
      "train loss:1.0731777058025027\n",
      "train loss:0.9311897170409408\n",
      "train loss:0.9465114146889849\n",
      "train loss:0.82963422411044\n",
      "train loss:1.0218755591226387\n",
      "train loss:1.0219113494743264\n",
      "train loss:0.9169602700246219\n",
      "train loss:0.9226059852580338\n",
      "train loss:0.8164659167764599\n",
      "train loss:0.8334204410966654\n",
      "train loss:0.9647978376641752\n",
      "train loss:0.9686613060848128\n",
      "train loss:0.9901582627930092\n",
      "train loss:0.8376599199449376\n",
      "train loss:0.9038702998267955\n",
      "train loss:1.0220683355176876\n",
      "train loss:0.9522241752651783\n",
      "train loss:0.8178970866404599\n",
      "train loss:0.8459246014096666\n",
      "train loss:0.7729037734956482\n",
      "train loss:1.0568591850598512\n",
      "train loss:0.8285560619105544\n",
      "train loss:0.8673289846040853\n",
      "train loss:0.9424569362269822\n",
      "train loss:0.8471876267488042\n",
      "train loss:0.8474903345461119\n",
      "train loss:1.0639449799023144\n",
      "train loss:1.069924407234053\n",
      "train loss:0.9358748311251738\n",
      "train loss:0.9205088392720951\n",
      "train loss:0.9873040988877956\n",
      "train loss:0.8943587403386596\n",
      "train loss:0.8856377025106292\n",
      "train loss:0.9384326683253775\n",
      "train loss:0.8647898278620939\n",
      "train loss:1.0320754986585015\n",
      "train loss:0.920146302473459\n",
      "train loss:0.9635756034448774\n",
      "train loss:0.7472260118906827\n",
      "train loss:0.8842485309990082\n",
      "train loss:0.8590431040025688\n",
      "train loss:0.9649079988797571\n",
      "train loss:0.8605777243980088\n",
      "train loss:0.8334451073111676\n",
      "train loss:0.8569761964913749\n",
      "train loss:0.7955306628040687\n",
      "train loss:0.9510624482109955\n",
      "train loss:0.9478523729496736\n",
      "train loss:0.9024761762524772\n",
      "train loss:0.8549955047757762\n",
      "train loss:0.8800531695491582\n",
      "train loss:1.0666738502643083\n",
      "train loss:0.8784422302235145\n",
      "train loss:1.0526462591926944\n",
      "train loss:0.9005885846839231\n",
      "train loss:1.134113220228481\n",
      "train loss:0.9553370795703613\n",
      "train loss:0.9058957536337056\n",
      "train loss:0.9181270758478413\n",
      "train loss:0.9948082520691606\n",
      "train loss:0.9205142119584907\n",
      "train loss:1.004581021437844\n",
      "train loss:0.7864387232278502\n",
      "train loss:0.9322195854391011\n",
      "train loss:1.0149973721351007\n",
      "train loss:0.7165098811991839\n",
      "train loss:0.7897170149314127\n",
      "train loss:0.8186085542586474\n",
      "train loss:0.8867468963725762\n",
      "train loss:0.9643770571549339\n",
      "train loss:0.9486524387280386\n",
      "train loss:0.8960935737723544\n",
      "train loss:0.7740269945341857\n",
      "train loss:0.9549056333449951\n",
      "train loss:0.9567052648263126\n",
      "train loss:1.0044540308071683\n",
      "train loss:0.9067580096551473\n",
      "train loss:0.8755564253876202\n",
      "train loss:0.9256085588494275\n",
      "train loss:0.8338353892274593\n",
      "train loss:0.8370721027010696\n",
      "train loss:0.712973737160886\n",
      "train loss:0.8687175006677154\n",
      "train loss:0.8384786513810981\n",
      "train loss:0.8230460936402955\n",
      "train loss:0.9089490983136547\n",
      "train loss:0.8908465281460768\n",
      "train loss:0.8973437702858493\n",
      "train loss:0.885581414050372\n",
      "train loss:0.8477280695543056\n",
      "train loss:0.8636307598871658\n",
      "train loss:0.872206223160665\n",
      "train loss:0.8314285278942641\n",
      "train loss:0.8158280360039573\n",
      "train loss:0.8991904533681689\n",
      "train loss:0.8317801591180484\n",
      "train loss:0.9459218630907231\n",
      "train loss:0.8854593834535213\n",
      "train loss:0.9626840708248742\n",
      "train loss:0.8710392084909283\n",
      "train loss:0.7790588978465722\n",
      "train loss:0.9321968359734422\n",
      "train loss:0.7599993227373463\n",
      "train loss:0.9759329700121694\n",
      "train loss:0.7982376782738086\n",
      "train loss:0.9596588014553218\n",
      "train loss:0.8434154050324497\n",
      "train loss:0.9201193832256971\n",
      "train loss:0.797963894997207\n",
      "train loss:0.8772634349536605\n",
      "train loss:0.7992064659137637\n",
      "train loss:0.7471180963560129\n",
      "train loss:0.9987188648388815\n",
      "train loss:0.8791820959302182\n",
      "train loss:0.8194924386685716\n",
      "train loss:0.817192476820828\n",
      "train loss:0.8517364764789837\n",
      "train loss:0.9499029186591774\n",
      "train loss:0.8966273411397592\n",
      "train loss:0.8139706300008624\n",
      "train loss:0.9872398067998948\n",
      "train loss:0.9820773133846019\n",
      "train loss:1.0040678819594897\n",
      "train loss:0.8249783773098882\n",
      "train loss:0.8794294423396098\n",
      "train loss:0.8124700607402694\n",
      "train loss:0.9218014931098254\n",
      "train loss:1.1108958732905467\n",
      "train loss:0.8645019378752578\n",
      "train loss:0.924069700231536\n",
      "train loss:0.9795761770933402\n",
      "train loss:0.970384826462614\n",
      "train loss:0.7665397118950348\n",
      "train loss:0.9140084633438668\n",
      "train loss:0.8912668037875972\n",
      "train loss:0.8161188416349496\n",
      "train loss:1.129903926982021\n",
      "train loss:0.8136645740966436\n",
      "train loss:0.98219912716976\n",
      "train loss:0.8370373552006443\n",
      "train loss:0.8395139542827746\n",
      "train loss:0.793339142008314\n",
      "train loss:0.8339986583451529\n",
      "train loss:0.7948383515281434\n",
      "train loss:0.9833431293174647\n",
      "train loss:0.9072455052425568\n",
      "train loss:0.6522144647618776\n",
      "train loss:0.963141270200954\n",
      "train loss:0.9261979405776088\n",
      "train loss:0.9386694546798141\n",
      "train loss:0.954279846281884\n",
      "train loss:1.1113847731501634\n",
      "train loss:0.922528611601976\n",
      "train loss:0.8522019051751042\n",
      "train loss:0.7943142846617519\n",
      "train loss:1.0162148068728212\n",
      "train loss:0.9387282379004853\n",
      "train loss:0.8537311284033311\n",
      "train loss:0.8938996310604962\n",
      "train loss:0.8760627686021887\n",
      "train loss:0.8625883726103423\n",
      "train loss:0.7592275241421216\n",
      "train loss:1.0251784076683048\n",
      "train loss:0.8567682560497544\n",
      "train loss:0.7930678207477339\n",
      "train loss:0.7841199189284255\n",
      "train loss:0.9654628253738504\n",
      "train loss:0.8067950410670569\n",
      "train loss:0.9629151618609876\n",
      "train loss:0.9249516119240561\n",
      "train loss:0.8628726777731685\n",
      "train loss:1.0367224358556115\n",
      "train loss:0.8411147443990419\n",
      "train loss:0.8857361952445758\n",
      "train loss:1.0013753285792617\n",
      "train loss:0.9638471654371341\n",
      "train loss:0.8273612132345081\n",
      "train loss:0.876452979953472\n",
      "train loss:0.6898532118569569\n",
      "train loss:0.7880612742085993\n",
      "train loss:0.7212642894460869\n",
      "train loss:0.8819980480189308\n",
      "train loss:0.857710475147115\n",
      "train loss:0.7687295990602517\n",
      "train loss:0.7624596175129377\n",
      "train loss:1.0040696080756661\n",
      "train loss:0.7838556114533027\n",
      "train loss:0.9268432660265532\n",
      "train loss:0.8622208122294295\n",
      "train loss:1.0581189852776858\n",
      "train loss:1.028169323401724\n",
      "train loss:1.0346473821315227\n",
      "train loss:0.8610516180614914\n",
      "train loss:0.7903160777130193\n",
      "train loss:0.9870961550926846\n",
      "train loss:0.8424016163482966\n",
      "train loss:0.7562381124053202\n",
      "train loss:0.7588439268509463\n",
      "train loss:0.9531203084510181\n",
      "train loss:0.7915879014030108\n",
      "train loss:1.0847548276352148\n",
      "train loss:0.8840933466663317\n",
      "train loss:0.8223163454137993\n",
      "train loss:1.0901988765511965\n",
      "train loss:0.7057901316697993\n",
      "train loss:0.8823031056000916\n",
      "train loss:0.9209956337591911\n",
      "train loss:0.9409578936677043\n",
      "train loss:0.8748908420592688\n",
      "train loss:0.8866342392623876\n",
      "train loss:0.8651561302685526\n",
      "train loss:0.7453712256434523\n",
      "train loss:0.9307179592127113\n",
      "train loss:0.7792410077109444\n",
      "train loss:0.9562395451102708\n",
      "train loss:0.9302867175879844\n",
      "train loss:0.8544990378173545\n",
      "train loss:0.864675189214066\n",
      "train loss:0.8708809760437559\n",
      "train loss:0.9221837443905969\n",
      "train loss:1.0908283775009056\n",
      "train loss:0.928450197975277\n",
      "train loss:0.8091946406772967\n",
      "train loss:0.8674632171068326\n",
      "train loss:0.9567080231699882\n",
      "train loss:0.9912621902443837\n",
      "train loss:0.9198755525818966\n",
      "train loss:0.9355157753318322\n",
      "train loss:0.9260026481006542\n",
      "train loss:0.8235076613054119\n",
      "train loss:0.9024038798692328\n",
      "train loss:1.004730642254845\n",
      "train loss:0.9522408126445207\n",
      "train loss:0.8222313622078328\n",
      "train loss:0.9257485651101298\n",
      "train loss:1.0387202138856677\n",
      "train loss:0.9987885371520526\n",
      "train loss:0.809371737531271\n",
      "train loss:0.9150549473693428\n",
      "train loss:0.8555972224227443\n",
      "train loss:0.9397574077927996\n",
      "train loss:0.8548410349897384\n",
      "train loss:0.9047244657607015\n",
      "train loss:0.8141599197345343\n",
      "train loss:1.0659285441284467\n",
      "train loss:0.9182243670448021\n",
      "train loss:0.8703392574175716\n",
      "train loss:0.9072381344168172\n",
      "train loss:0.889062061044112\n",
      "train loss:0.9694887088080673\n",
      "train loss:0.8425205277604061\n",
      "train loss:0.9284855750793335\n",
      "train loss:0.8355156784521285\n",
      "train loss:0.9580543852780664\n",
      "train loss:0.8095623308353485\n",
      "train loss:0.8789078215908752\n",
      "train loss:1.2152299859533406\n",
      "train loss:1.034725042889813\n",
      "train loss:0.8214733852960577\n",
      "train loss:0.8606581187375031\n",
      "train loss:0.9643446092352179\n",
      "train loss:0.7514712754894964\n",
      "train loss:0.8094889689628408\n",
      "train loss:0.5411299545094298\n",
      "train loss:0.8610407354439953\n",
      "train loss:1.1323200410430732\n",
      "train loss:0.7985202600539617\n",
      "train loss:0.9139284008636953\n",
      "train loss:0.8582684316102432\n",
      "train loss:0.8728432186041637\n",
      "train loss:0.950929357782842\n",
      "train loss:0.9104880939990864\n",
      "train loss:1.0048108980494777\n",
      "train loss:0.9483974366363377\n",
      "train loss:0.7311207993224506\n",
      "train loss:0.964533924243987\n",
      "train loss:0.8835283602896509\n",
      "train loss:0.962928215120156\n",
      "train loss:0.9496405330612623\n",
      "train loss:0.9268987102594503\n",
      "train loss:1.0135939365581395\n",
      "train loss:0.9619302020674145\n",
      "train loss:0.953638507411691\n",
      "train loss:0.8515311823196201\n",
      "train loss:0.8751899594782848\n",
      "train loss:0.9300228822161383\n",
      "train loss:0.8849073077787958\n",
      "train loss:0.9775611361844194\n",
      "train loss:0.8699084621610084\n",
      "train loss:0.9770325435830669\n",
      "train loss:0.8361690814585532\n",
      "train loss:0.8170320219771635\n",
      "train loss:0.8895120188154216\n",
      "train loss:0.8280167180222046\n",
      "train loss:0.9555070259246145\n",
      "train loss:0.8268914005316259\n",
      "train loss:0.8922246547462253\n",
      "train loss:0.8248294710832085\n",
      "train loss:0.8776169788458804\n",
      "train loss:0.7594832025032019\n",
      "train loss:0.8450909575157033\n",
      "train loss:0.8174097213370801\n",
      "train loss:0.9068959848532735\n",
      "train loss:0.8892998502444547\n",
      "train loss:0.9196866486277504\n",
      "train loss:0.8630406450250679\n",
      "train loss:0.8636076720749013\n",
      "train loss:0.9123539512413456\n",
      "train loss:0.7805712778814459\n",
      "train loss:0.8891099557629025\n",
      "train loss:0.6408726398879687\n",
      "train loss:0.7939363831290186\n",
      "train loss:0.8106866000994551\n",
      "train loss:1.0610360037485864\n",
      "train loss:0.9756933219340315\n",
      "train loss:0.7497279572388265\n",
      "train loss:0.8247486822347824\n",
      "train loss:0.9260704829217066\n",
      "train loss:0.7345890625546594\n",
      "train loss:0.8543548251130205\n",
      "train loss:0.9987908333705099\n",
      "train loss:0.9362277069588276\n",
      "train loss:0.6707822115778848\n",
      "train loss:0.8530558198398525\n",
      "train loss:0.8114723953115686\n",
      "train loss:1.1188089657489946\n",
      "train loss:0.9518355535507673\n",
      "train loss:0.9584943306804596\n",
      "train loss:0.8769342609845583\n",
      "train loss:0.8085443428452771\n",
      "train loss:0.8699738876776419\n",
      "train loss:0.9029058015896021\n",
      "train loss:0.9081490554156642\n",
      "train loss:0.8736327683334104\n",
      "train loss:0.9648460286072985\n",
      "train loss:0.8109834587876965\n",
      "train loss:0.8921072741133333\n",
      "train loss:0.8765975111734349\n",
      "train loss:0.8808169145339843\n",
      "train loss:0.7873540465100066\n",
      "train loss:0.9159754575761393\n",
      "train loss:1.0143482621378865\n",
      "train loss:0.9239272356862507\n",
      "train loss:0.896628558249562\n",
      "train loss:0.8418529337744523\n",
      "train loss:0.828712423006382\n",
      "train loss:0.8839833061333994\n",
      "train loss:0.8462758801854053\n",
      "train loss:0.9899785391303344\n",
      "train loss:0.8722637311513128\n",
      "train loss:0.9073225224976718\n",
      "train loss:0.8289202839087164\n",
      "train loss:0.9986574981631051\n",
      "train loss:0.8913005296650839\n",
      "train loss:0.8776461367036573\n",
      "train loss:0.8806878313187562\n",
      "train loss:0.9801052749506901\n",
      "train loss:1.0390791264433223\n",
      "train loss:0.7491861993329529\n",
      "train loss:0.8195126039542925\n",
      "train loss:0.9001995144102769\n",
      "train loss:0.9544330759896051\n",
      "train loss:0.9817564511258122\n",
      "train loss:0.8060958606929524\n",
      "train loss:0.9275320026104752\n",
      "train loss:0.9583094422483924\n",
      "train loss:0.856003440033061\n",
      "train loss:0.9932881009938432\n",
      "train loss:0.9446875715988164\n",
      "train loss:1.0790555454052422\n",
      "train loss:0.9229642908886351\n",
      "train loss:0.9025550024185831\n",
      "train loss:0.919320879407712\n",
      "train loss:0.9463877331973332\n",
      "train loss:1.07820325097045\n",
      "train loss:0.7407302134702414\n",
      "train loss:0.8873601772596991\n",
      "train loss:0.9208473555325485\n",
      "train loss:0.8571556241199321\n",
      "train loss:0.8647993044434742\n",
      "train loss:0.8735586328298082\n",
      "train loss:0.9056534599037614\n",
      "train loss:1.0971689347363511\n",
      "train loss:0.8829011806325516\n",
      "train loss:0.9405965040568739\n",
      "train loss:0.8984104871992507\n",
      "train loss:0.8570838852416983\n",
      "train loss:0.9522763858714752\n",
      "train loss:0.8943246321822176\n",
      "train loss:0.8891901129074777\n",
      "train loss:0.9118315842055807\n",
      "train loss:0.8720074696153283\n",
      "train loss:0.8958845541871959\n",
      "train loss:1.068701429849003\n",
      "train loss:1.0254356941955962\n",
      "train loss:0.7911324290657857\n",
      "train loss:0.8796179071304077\n",
      "train loss:0.8633621593262458\n",
      "train loss:0.8589813917382965\n",
      "train loss:0.8367389980541919\n",
      "train loss:0.8636071026004708\n",
      "train loss:1.1200699294053067\n",
      "train loss:0.8077516189959606\n",
      "train loss:0.9651728931984479\n",
      "train loss:0.9342756113000398\n",
      "train loss:0.8758421413445019\n",
      "train loss:0.9228603109572018\n",
      "train loss:0.9819758111445707\n",
      "train loss:0.9041109392056245\n",
      "train loss:0.7508243944440721\n",
      "train loss:0.822627002626283\n",
      "train loss:0.9265567210847426\n",
      "train loss:0.8116873764615237\n",
      "train loss:0.8992142237421037\n",
      "train loss:0.8492104439977886\n",
      "train loss:0.9707413341057779\n",
      "train loss:0.9764581161562599\n",
      "train loss:0.7642262715213235\n",
      "train loss:1.1061528412312598\n",
      "=== epoch:8, train acc:0.995, test acc:0.99 ===\n",
      "train loss:0.889404957756654\n",
      "train loss:0.9413141939352309\n",
      "train loss:0.9503680246525383\n",
      "train loss:0.9259538505067497\n",
      "train loss:0.8966276618773376\n",
      "train loss:0.8745535198561847\n",
      "train loss:1.0014689029792985\n",
      "train loss:0.7393761893034945\n",
      "train loss:0.857934388401176\n",
      "train loss:0.913067888955695\n",
      "train loss:0.8407429278756436\n",
      "train loss:0.796528610000996\n",
      "train loss:0.870178210680072\n",
      "train loss:0.933456940718115\n",
      "train loss:0.9053228285432443\n",
      "train loss:0.81830297497856\n",
      "train loss:0.8714524519125714\n",
      "train loss:1.1107587131093732\n",
      "train loss:0.8816446600717963\n",
      "train loss:0.8385971595165642\n",
      "train loss:1.0342817078415403\n",
      "train loss:0.769410621448081\n",
      "train loss:1.0961753781072399\n",
      "train loss:0.9249562113176667\n",
      "train loss:0.9035103204930179\n",
      "train loss:0.7777777212911201\n",
      "train loss:0.8302485856809332\n",
      "train loss:1.0055318635668424\n",
      "train loss:0.9365389887842981\n",
      "train loss:0.8099064508216358\n",
      "train loss:0.8599399330959185\n",
      "train loss:0.9882447075167875\n",
      "train loss:0.9475724488913477\n",
      "train loss:0.9419683263896286\n",
      "train loss:0.8309348336920526\n",
      "train loss:0.8509477393870103\n",
      "train loss:0.9574161485980688\n",
      "train loss:0.9154444080138595\n",
      "train loss:0.8325664316558214\n",
      "train loss:0.8237700041699211\n",
      "train loss:0.789758440958298\n",
      "train loss:0.9796220130909761\n",
      "train loss:0.8043185444276914\n",
      "train loss:0.8626524289368388\n",
      "train loss:0.9486002272612659\n",
      "train loss:0.835688249687251\n",
      "train loss:1.0050947786984514\n",
      "train loss:0.8564823247441392\n",
      "train loss:0.9037795950106792\n",
      "train loss:0.8708986148669794\n",
      "train loss:1.0599825250640267\n",
      "train loss:1.027799711342172\n",
      "train loss:0.9166009245084382\n",
      "train loss:0.9429454322975203\n",
      "train loss:0.8614358289557933\n",
      "train loss:1.0983650081212768\n",
      "train loss:0.9173955570824489\n",
      "train loss:0.8285251854460793\n",
      "train loss:0.9074083255117895\n",
      "train loss:0.8613464681479145\n",
      "train loss:0.8847716351622457\n",
      "train loss:0.9377130417305473\n",
      "train loss:1.0011082661461461\n",
      "train loss:0.8618995155394314\n",
      "train loss:1.0259287062102473\n",
      "train loss:0.8048919386628568\n",
      "train loss:0.9987416416064789\n",
      "train loss:1.0864094259338346\n",
      "train loss:0.912868804111125\n",
      "train loss:0.7527540416833383\n",
      "train loss:0.9354972600875668\n",
      "train loss:0.8781300091056324\n",
      "train loss:0.7246081240384421\n",
      "train loss:0.9063655160711868\n",
      "train loss:0.8109945363060003\n",
      "train loss:0.9630876265597569\n",
      "train loss:0.9146239198252553\n",
      "train loss:0.7032347268412172\n",
      "train loss:0.8502884655231092\n",
      "train loss:1.0336815822402645\n",
      "train loss:0.9362770378171815\n",
      "train loss:0.8276435508786705\n",
      "train loss:0.8381010912477099\n",
      "train loss:0.7152085592757707\n",
      "train loss:0.8799208558882262\n",
      "train loss:0.9533653549517334\n",
      "train loss:0.7863075809651537\n",
      "train loss:0.9609859093671628\n",
      "train loss:0.8132469651723746\n",
      "train loss:0.9129924657416345\n",
      "train loss:0.8449749161121558\n",
      "train loss:1.2332206936317758\n",
      "train loss:1.0203500475303233\n",
      "train loss:0.7751937406454664\n",
      "train loss:1.0380937008134048\n",
      "train loss:1.0167730556804189\n",
      "train loss:0.9825811578406431\n",
      "train loss:0.8246673467717703\n",
      "train loss:0.8546345554356393\n",
      "train loss:0.8080472137592458\n",
      "train loss:0.9319754622284888\n",
      "train loss:0.8709886508623272\n",
      "train loss:0.9266586185904808\n",
      "train loss:0.9324384062619505\n",
      "train loss:1.004023397524577\n",
      "train loss:0.9597502241654997\n",
      "train loss:0.871613675194463\n",
      "train loss:0.795600355445198\n",
      "train loss:0.7399165098645634\n",
      "train loss:0.9474330504361503\n",
      "train loss:1.0152483467750575\n",
      "train loss:0.9013749189434631\n",
      "train loss:0.9142365268697695\n",
      "train loss:0.9323005211790187\n",
      "train loss:0.9757835463526806\n",
      "train loss:0.9074250287458999\n",
      "train loss:0.9649157553150662\n",
      "train loss:0.8751887028918852\n",
      "train loss:0.7387272676341675\n",
      "train loss:1.0707278027808649\n",
      "train loss:0.8529068707090637\n",
      "train loss:0.952861318933365\n",
      "train loss:0.9177003749163893\n",
      "train loss:0.9212034152103957\n",
      "train loss:0.8195824568529143\n",
      "train loss:1.0173176535736845\n",
      "train loss:0.847923242669529\n",
      "train loss:0.8610997465105302\n",
      "train loss:0.8543925403380099\n",
      "train loss:0.7300139281745315\n",
      "train loss:0.8926340500524621\n",
      "train loss:0.8224096793541692\n",
      "train loss:0.7123832050341017\n",
      "train loss:0.8987957600063388\n",
      "train loss:0.9751494677887702\n",
      "train loss:0.6859325018820083\n",
      "train loss:0.8468913117689408\n",
      "train loss:0.8796899629815583\n",
      "train loss:0.8690298578972617\n",
      "train loss:0.9001550030271568\n",
      "train loss:0.8346746355782902\n",
      "train loss:0.8830047950821707\n",
      "train loss:0.7866567696922862\n",
      "train loss:0.867395022221847\n",
      "train loss:0.7346296212199078\n",
      "train loss:1.0370850570448633\n",
      "train loss:0.7661990474616445\n",
      "train loss:0.9250407187025124\n",
      "train loss:0.822465204709694\n",
      "train loss:0.8461119476823885\n",
      "train loss:0.9007822821684615\n",
      "train loss:0.8908334753268258\n",
      "train loss:0.9285402286562291\n",
      "train loss:0.9333618166745029\n",
      "train loss:0.8518584971139284\n",
      "train loss:0.9110679859203904\n",
      "train loss:0.7140015743795839\n",
      "train loss:0.9087191377419098\n",
      "train loss:0.7920919930998787\n",
      "train loss:0.8728451514523248\n",
      "train loss:0.889381171637539\n",
      "train loss:0.9534566386938232\n",
      "train loss:0.8707065685656917\n",
      "train loss:0.8916686163284541\n",
      "train loss:1.0401868492857762\n",
      "train loss:1.0133820207422921\n",
      "train loss:0.913252205888153\n",
      "train loss:1.1011111950056103\n",
      "train loss:0.8765544216646033\n",
      "train loss:0.8362459757279785\n",
      "train loss:0.8898383912835132\n",
      "train loss:0.9517489831197087\n",
      "train loss:0.9206320503507832\n",
      "train loss:0.9973739284611659\n",
      "train loss:1.0069844356835083\n",
      "train loss:0.7771736764050433\n",
      "train loss:0.7744095527556659\n",
      "train loss:1.038080005556465\n",
      "train loss:0.8209107268315939\n",
      "train loss:1.0432322169809682\n",
      "train loss:0.8313957392926394\n",
      "train loss:0.7322373802844133\n",
      "train loss:0.9328511182547462\n",
      "train loss:1.0039873446785343\n",
      "train loss:0.678082146117941\n",
      "train loss:0.9772851462349815\n",
      "train loss:0.9087254630091433\n",
      "train loss:1.0360119918287405\n",
      "train loss:0.8814169616061949\n",
      "train loss:0.8646360967830976\n",
      "train loss:0.8889413399191879\n",
      "train loss:0.7584363774751608\n",
      "train loss:0.7868873866414248\n",
      "train loss:0.9383987174245718\n",
      "train loss:0.9744049230037234\n",
      "train loss:0.7346914281115585\n",
      "train loss:0.8774783591758706\n",
      "train loss:0.8185139857020797\n",
      "train loss:0.8714744624066875\n",
      "train loss:1.090602948912487\n",
      "train loss:0.92859824642104\n",
      "train loss:0.9125950078846266\n",
      "train loss:1.0377164190811643\n",
      "train loss:0.820935481256921\n",
      "train loss:0.9430761296272548\n",
      "train loss:0.7932275627076304\n",
      "train loss:0.7959228009010988\n",
      "train loss:0.8388702790310648\n",
      "train loss:1.0039547603167913\n",
      "train loss:0.9655024327351959\n",
      "train loss:0.9316553838052676\n",
      "train loss:0.8401258753967974\n",
      "train loss:0.937853526156104\n",
      "train loss:1.013153664280947\n",
      "train loss:0.8352693851546014\n",
      "train loss:0.8208464995409421\n",
      "train loss:0.8397842038397214\n",
      "train loss:0.9561210981378465\n",
      "train loss:1.0381663500788565\n",
      "train loss:0.9344859307773319\n",
      "train loss:0.9055532559164132\n",
      "train loss:0.8521453933862445\n",
      "train loss:1.012192568349478\n",
      "train loss:0.860972435359907\n",
      "train loss:0.9980505184929943\n",
      "train loss:0.8784917145438598\n",
      "train loss:0.9184166159790977\n",
      "train loss:0.8848545812781681\n",
      "train loss:0.9525053995848455\n",
      "train loss:0.9236862760328914\n",
      "train loss:0.8966411734680024\n",
      "train loss:0.7641882139948989\n",
      "train loss:0.7635652774717147\n",
      "train loss:0.8872187240728374\n",
      "train loss:0.8499009645660408\n",
      "train loss:0.8006217097973274\n",
      "train loss:1.0264009452474736\n",
      "train loss:0.7120325608725377\n",
      "train loss:0.9639893899430864\n",
      "train loss:0.9333259784161945\n",
      "train loss:0.8044852087416572\n",
      "train loss:0.8887831196898717\n",
      "train loss:0.8117446535002116\n",
      "train loss:0.8142235419509786\n",
      "train loss:0.8528830304394676\n",
      "train loss:0.8559445413459836\n",
      "train loss:0.9183423504965088\n",
      "train loss:0.8722319173550613\n",
      "train loss:0.8099747299803707\n",
      "train loss:0.9593195553139918\n",
      "train loss:0.963968861842451\n",
      "train loss:0.8387204704304223\n",
      "train loss:1.0084632675423797\n",
      "train loss:0.8779786236148429\n",
      "train loss:0.9007565205222114\n",
      "train loss:0.9370789559497189\n",
      "train loss:0.9986756284369698\n",
      "train loss:1.0633555202916642\n",
      "train loss:0.8954816476077628\n",
      "train loss:0.9877507676528041\n",
      "train loss:1.003891834920315\n",
      "train loss:0.850178442383045\n",
      "train loss:0.8665010125408447\n",
      "train loss:0.8210058337873939\n",
      "train loss:0.9868699828495738\n",
      "train loss:0.9371813595407356\n",
      "train loss:0.9263601676940618\n",
      "train loss:1.0235575722170744\n",
      "train loss:0.7649643222918932\n",
      "train loss:0.9271044441985058\n",
      "train loss:0.9228686297086419\n",
      "train loss:0.9127040369143341\n",
      "train loss:0.7380536039453566\n",
      "train loss:0.8769538820057333\n",
      "train loss:0.8153364793220859\n",
      "train loss:0.9843704298674639\n",
      "train loss:0.9491973355215247\n",
      "train loss:1.0006117834363317\n",
      "train loss:0.8877847235439726\n",
      "train loss:1.0446447258094516\n",
      "train loss:1.0187277515059427\n",
      "train loss:0.8761895767720402\n",
      "train loss:0.9025559493513202\n",
      "train loss:0.8245593502109021\n",
      "train loss:0.8744655226511847\n",
      "train loss:0.8968206497817461\n",
      "train loss:0.8121540946664995\n",
      "train loss:0.8884722848591163\n",
      "train loss:0.7951287144488043\n",
      "train loss:0.8001899917318522\n",
      "train loss:1.0527792205464226\n",
      "train loss:0.9683516116561414\n",
      "train loss:0.9544307012477159\n",
      "train loss:0.98089138454605\n",
      "train loss:0.8039306217523751\n",
      "train loss:0.8682785542396724\n",
      "train loss:0.9529723116578098\n",
      "train loss:0.9581552856423824\n",
      "train loss:0.9507052513453783\n",
      "train loss:0.8227670285912465\n",
      "train loss:0.8543291488392099\n",
      "train loss:1.0212576391765382\n",
      "train loss:0.7883208484187751\n",
      "train loss:1.044574069923254\n",
      "train loss:0.9256132628726078\n",
      "train loss:0.9293335135896466\n",
      "train loss:0.9323328474979262\n",
      "train loss:0.8302504469377007\n",
      "train loss:0.8126938633439684\n",
      "train loss:0.9905684894721594\n",
      "train loss:0.830954372542983\n",
      "train loss:0.803647562927774\n",
      "train loss:0.8774663279885371\n",
      "train loss:0.9788890454398147\n",
      "train loss:0.7277211503750141\n",
      "train loss:0.8172098632537977\n",
      "train loss:0.955050002664521\n",
      "train loss:0.9858373338434518\n",
      "train loss:0.7939639461137264\n",
      "train loss:0.8056885985366136\n",
      "train loss:0.8111865455378994\n",
      "train loss:0.7784367041987466\n",
      "train loss:1.004901913718374\n",
      "train loss:0.8513068782545045\n",
      "train loss:1.0605963323417322\n",
      "train loss:0.8430490459997376\n",
      "train loss:0.7971434422533082\n",
      "train loss:0.8878021582573267\n",
      "train loss:0.8723254830153226\n",
      "train loss:0.9638340487849684\n",
      "train loss:0.9314072985645298\n",
      "train loss:0.8787760811164782\n",
      "train loss:0.8146338404540594\n",
      "train loss:1.0045389452106472\n",
      "train loss:0.8707436568088975\n",
      "train loss:0.7730560381755904\n",
      "train loss:0.7259881826815863\n",
      "train loss:0.8486525117555144\n",
      "train loss:0.8117909599052469\n",
      "train loss:0.8895292285615117\n",
      "train loss:0.8754790591308098\n",
      "train loss:0.9336892672424024\n",
      "train loss:0.9133727330876809\n",
      "train loss:1.0047109576093032\n",
      "train loss:0.84271815832826\n",
      "train loss:0.8493880672820583\n",
      "train loss:0.835783504186019\n",
      "train loss:1.011925875825907\n",
      "train loss:1.0046050863433702\n",
      "train loss:0.9219021830987348\n",
      "train loss:0.7975958525012211\n",
      "train loss:0.8972053841243386\n",
      "train loss:0.8760315027487556\n",
      "train loss:0.872800881297993\n",
      "train loss:0.8222984933839962\n",
      "train loss:0.7542746501427448\n",
      "train loss:0.7992048329821769\n",
      "train loss:0.8636181364461298\n",
      "train loss:0.8739078079347559\n",
      "train loss:1.0212644961907607\n",
      "train loss:0.9445887327248925\n",
      "train loss:0.9944483512673488\n",
      "train loss:0.9472457572462584\n",
      "train loss:0.8439208158106899\n",
      "train loss:0.8171474025684458\n",
      "train loss:0.8624272418326241\n",
      "train loss:0.7571736906606347\n",
      "train loss:0.8243620666659781\n",
      "train loss:0.73822130154784\n",
      "train loss:0.8761364125243639\n",
      "train loss:0.9250669375465732\n",
      "train loss:0.8438422241120912\n",
      "train loss:0.9288669977269834\n",
      "train loss:0.9156836017263211\n",
      "train loss:0.8090042910842925\n",
      "train loss:0.8874492947684824\n",
      "train loss:0.8813820472733777\n",
      "train loss:0.7315698650169974\n",
      "train loss:0.855191005034178\n",
      "train loss:0.9359031648977034\n",
      "train loss:0.9916032032003069\n",
      "train loss:0.87561033352196\n",
      "train loss:0.6621113702715736\n",
      "train loss:0.8866987996527215\n",
      "train loss:0.9739240651195558\n",
      "train loss:0.8822002291665744\n",
      "train loss:0.9723819150323167\n",
      "train loss:0.823172468180967\n",
      "train loss:0.9831980538717727\n",
      "train loss:0.9772720370568375\n",
      "train loss:0.9328078206208456\n",
      "train loss:0.862867120734813\n",
      "train loss:0.985202860246761\n",
      "train loss:0.6891403727838272\n",
      "train loss:0.8671602988871375\n",
      "train loss:0.924825556409405\n",
      "train loss:0.7372672040558598\n",
      "train loss:1.0896642101429461\n",
      "train loss:0.9826681259082964\n",
      "train loss:0.8850729642641313\n",
      "train loss:0.899293691996836\n",
      "train loss:0.8187726171163111\n",
      "train loss:0.9794350340483255\n",
      "train loss:1.0857102768274443\n",
      "train loss:0.7968992397444322\n",
      "train loss:0.9050070710326158\n",
      "train loss:0.8449585713533639\n",
      "train loss:0.9082366311078199\n",
      "train loss:0.8948126658769777\n",
      "train loss:0.8718644837063486\n",
      "train loss:0.8719547705545098\n",
      "train loss:0.8491932766055487\n",
      "train loss:0.8440912603732488\n",
      "train loss:0.6563284893950623\n",
      "train loss:1.0248091579334992\n",
      "train loss:0.8930790282430272\n",
      "train loss:0.8765773732062442\n",
      "train loss:0.8633498863824945\n",
      "train loss:0.8725527999726946\n",
      "train loss:0.9152546973290271\n",
      "train loss:0.8435247008006747\n",
      "train loss:0.8870614104116568\n",
      "train loss:1.0004685897859664\n",
      "train loss:0.7703128629826328\n",
      "train loss:1.124269232435986\n",
      "train loss:0.8854518964410968\n",
      "train loss:0.8520284149638487\n",
      "train loss:0.9619460682841617\n",
      "train loss:0.8551989238403104\n",
      "train loss:0.9535897410466626\n",
      "train loss:0.7219956672287787\n",
      "train loss:0.873918449927158\n",
      "train loss:0.9157003036915353\n",
      "train loss:1.020417126876754\n",
      "train loss:0.9828565044991886\n",
      "train loss:0.8383950630065264\n",
      "train loss:0.7665384103790492\n",
      "train loss:0.8229420774530033\n",
      "train loss:1.1088499875348417\n",
      "train loss:0.8357471737864449\n",
      "train loss:0.9070755402957522\n",
      "train loss:0.90240037900331\n",
      "train loss:0.8770148657484131\n",
      "train loss:0.9373552738321886\n",
      "train loss:0.8874645893924424\n",
      "train loss:0.9588232574078536\n",
      "train loss:0.9256366237509616\n",
      "train loss:0.795139602364478\n",
      "train loss:0.9148888202789527\n",
      "train loss:0.9406889253658203\n",
      "train loss:0.9506860010489917\n",
      "train loss:1.057784466692041\n",
      "train loss:0.9079166550134872\n",
      "train loss:0.8560782778624372\n",
      "train loss:0.8170527843094785\n",
      "train loss:0.93015823196178\n",
      "train loss:0.9507917547162715\n",
      "train loss:0.766553475283883\n",
      "train loss:0.8534228930977386\n",
      "train loss:0.8168497730549059\n",
      "train loss:0.9291161438236278\n",
      "train loss:0.795204525017951\n",
      "train loss:0.8865050575093862\n",
      "train loss:0.9681200557222343\n",
      "train loss:0.9694095816241469\n",
      "train loss:0.7967821553163646\n",
      "train loss:0.970205115690218\n",
      "train loss:0.9053567073518402\n",
      "train loss:0.9392652376362801\n",
      "train loss:0.8743964976866075\n",
      "train loss:0.6796270318812212\n",
      "train loss:0.8765339700125699\n",
      "train loss:0.9926946724686533\n",
      "train loss:0.8125214840215194\n",
      "train loss:0.8056068654118193\n",
      "train loss:0.8645990990009688\n",
      "train loss:0.8591505290236315\n",
      "train loss:0.9029164643577322\n",
      "train loss:1.0247156627559615\n",
      "train loss:0.8344917643488519\n",
      "train loss:0.9544552648163939\n",
      "train loss:0.8164548890450749\n",
      "train loss:0.9159465336448769\n",
      "train loss:0.983509384114935\n",
      "train loss:0.8578804559591817\n",
      "train loss:0.8529159747593041\n",
      "train loss:0.6926688232686992\n",
      "train loss:0.829689072760607\n",
      "train loss:0.8314406110215112\n",
      "train loss:0.9690683876811289\n",
      "train loss:0.8679579972545575\n",
      "train loss:0.937955712495174\n",
      "train loss:0.9694943975715661\n",
      "train loss:0.8254519041134136\n",
      "train loss:0.8484588356707171\n",
      "train loss:1.1087508589483885\n",
      "train loss:1.0251411569439184\n",
      "train loss:0.7890254732579731\n",
      "train loss:0.7642941029795645\n",
      "train loss:0.7751232434527622\n",
      "train loss:1.0159248604534488\n",
      "train loss:0.9377761360163223\n",
      "train loss:0.8774975784031659\n",
      "train loss:0.8709243681952433\n",
      "train loss:0.8707502602474483\n",
      "train loss:0.7941897743590977\n",
      "train loss:0.9068706189999515\n",
      "train loss:0.97995390925477\n",
      "train loss:1.0164198637929764\n",
      "train loss:0.7974769446591696\n",
      "train loss:0.8987761395579339\n",
      "train loss:0.796416967672316\n",
      "train loss:1.013996084137686\n",
      "train loss:0.884506849816216\n",
      "train loss:1.0191840411601154\n",
      "train loss:0.8567415830690757\n",
      "train loss:0.8643239120841193\n",
      "train loss:0.9344472507529903\n",
      "train loss:0.8865556775475946\n",
      "train loss:0.786068769751339\n",
      "train loss:0.9682975948208763\n",
      "train loss:0.9367825290704581\n",
      "train loss:0.8443389123726996\n",
      "train loss:0.9083376080694004\n",
      "train loss:0.9756208678541806\n",
      "train loss:0.8143219873964452\n",
      "train loss:1.097389615425146\n",
      "train loss:0.9916482116870963\n",
      "train loss:0.8998932382084495\n",
      "train loss:0.9725384046804332\n",
      "train loss:0.8972425122287737\n",
      "train loss:0.985390129437739\n",
      "train loss:0.9284498833447627\n",
      "train loss:0.9269364148402904\n",
      "train loss:0.9194649582565141\n",
      "train loss:1.056079713696472\n",
      "train loss:0.9698433031181538\n",
      "train loss:0.7781413981605977\n",
      "train loss:1.0108050533673703\n",
      "train loss:0.9891000325560274\n",
      "train loss:0.82755904808513\n",
      "train loss:0.8808681068530925\n",
      "train loss:0.9991155561075977\n",
      "train loss:0.9153153573067253\n",
      "train loss:0.8923860323757152\n",
      "train loss:0.7628418091398008\n",
      "train loss:0.8992589762783344\n",
      "train loss:0.9105409116341557\n",
      "train loss:0.8519493033944403\n",
      "train loss:1.0847067541160782\n",
      "train loss:0.8356041251900714\n",
      "train loss:0.8849034841526568\n",
      "train loss:0.834015730860004\n",
      "train loss:0.8731106758220257\n",
      "train loss:0.9131699180359568\n",
      "train loss:0.688182378893207\n",
      "train loss:0.8869687640695103\n",
      "train loss:0.8979246581334048\n",
      "train loss:0.7052340244950279\n",
      "train loss:0.8023839701525175\n",
      "train loss:0.9720584457245601\n",
      "train loss:0.7339314394053494\n",
      "train loss:0.9801863283173171\n",
      "train loss:1.1282779318339566\n",
      "train loss:0.8580521302046368\n",
      "train loss:0.83270012757268\n",
      "train loss:0.5931542151501038\n",
      "train loss:0.921336345022284\n",
      "train loss:1.078185662654041\n",
      "train loss:0.8503834055839132\n",
      "train loss:0.878661360046538\n",
      "train loss:0.9408416480265005\n",
      "train loss:0.8101079160755108\n",
      "train loss:0.8895557462790619\n",
      "train loss:0.8792734967892906\n",
      "train loss:0.8540929934652419\n",
      "train loss:0.8250187259308575\n",
      "train loss:0.8489151238980088\n",
      "train loss:0.9334324002835886\n",
      "train loss:0.8763693195166582\n",
      "train loss:1.0252598517670686\n",
      "train loss:0.8306613724956107\n",
      "train loss:0.9733328127827328\n",
      "train loss:0.8992535387558179\n",
      "train loss:0.9967687831346449\n",
      "train loss:0.894892040465198\n",
      "train loss:0.9338392194070763\n",
      "train loss:0.9824514518601017\n",
      "train loss:0.8912662931581147\n",
      "train loss:1.0033455724355895\n",
      "train loss:0.7305337134619514\n",
      "train loss:0.8909551652719204\n",
      "train loss:0.8176089068749396\n",
      "train loss:0.9811785789625287\n",
      "train loss:0.8018535105062113\n",
      "train loss:0.8014309855685816\n",
      "train loss:0.8125712018160078\n",
      "train loss:0.9846915848350035\n",
      "train loss:0.8725289568081971\n",
      "train loss:1.0072871427690535\n",
      "=== epoch:9, train acc:0.993, test acc:0.994 ===\n",
      "train loss:0.918807831270184\n",
      "train loss:0.9151593996287639\n",
      "train loss:0.8511192237917514\n",
      "train loss:0.9369447095375185\n",
      "train loss:0.9078679169638879\n",
      "train loss:0.8761970148252151\n",
      "train loss:0.9951221922168014\n",
      "train loss:0.9662823686006784\n",
      "train loss:0.8866866606156572\n",
      "train loss:1.125286360420128\n",
      "train loss:0.6921019518290318\n",
      "train loss:0.7699094228754703\n",
      "train loss:0.755653442948976\n",
      "train loss:0.7831572161391682\n",
      "train loss:0.8120300473187485\n",
      "train loss:0.863370599912137\n",
      "train loss:0.9897687226067212\n",
      "train loss:1.0256565855048776\n",
      "train loss:0.7532126306682017\n",
      "train loss:0.9879195340821433\n",
      "train loss:0.7628315030006318\n",
      "train loss:0.8113921056071878\n",
      "train loss:1.0061910314467033\n",
      "train loss:1.0172450767347647\n",
      "train loss:0.8040205393423377\n",
      "train loss:0.9251587640298475\n",
      "train loss:1.225861579515098\n",
      "train loss:0.7852128901323276\n",
      "train loss:0.7628151289071825\n",
      "train loss:0.8147574661075505\n",
      "train loss:0.8107266453987674\n",
      "train loss:0.9841300753021106\n",
      "train loss:0.7954788399813038\n",
      "train loss:0.9399610095565365\n",
      "train loss:0.8784257966536978\n",
      "train loss:0.8953678878220264\n",
      "train loss:0.8397220398349907\n",
      "train loss:0.833856702676866\n",
      "train loss:0.8405590891207525\n",
      "train loss:0.8023607935589361\n",
      "train loss:0.8833257638745855\n",
      "train loss:0.7791784391836362\n",
      "train loss:0.8711714189963677\n",
      "train loss:0.8203802157485302\n",
      "train loss:0.7981972079298609\n",
      "train loss:1.1437754420053492\n",
      "train loss:0.9857003249093854\n",
      "train loss:0.8669075487032514\n",
      "train loss:0.8101113792558696\n",
      "train loss:0.7593696456203939\n",
      "train loss:0.7900136257414482\n",
      "train loss:0.9939600926188368\n",
      "train loss:1.026240276735138\n",
      "train loss:0.8607586000115494\n",
      "train loss:0.983470398295971\n",
      "train loss:0.9974778463253766\n",
      "train loss:1.086151340816137\n",
      "train loss:0.9125860629269482\n",
      "train loss:0.7152830050741066\n",
      "train loss:0.7569206765135997\n",
      "train loss:0.9716642081393978\n",
      "train loss:0.8228995936661352\n",
      "train loss:0.8296351471925465\n",
      "train loss:0.9359830653294839\n",
      "train loss:0.8977808494558421\n",
      "train loss:0.9004943058295836\n",
      "train loss:1.0264102025646\n",
      "train loss:0.94538070857662\n",
      "train loss:0.9554989768775835\n",
      "train loss:0.8444870757290294\n",
      "train loss:0.7894189731246294\n",
      "train loss:1.031730345953604\n",
      "train loss:0.8018127174445653\n",
      "train loss:0.9777985803945967\n",
      "train loss:0.951805779630476\n",
      "train loss:0.8158109355974585\n",
      "train loss:0.9339607553515787\n",
      "train loss:0.7770085148157374\n",
      "train loss:0.8735022444693561\n",
      "train loss:0.7698967255631028\n",
      "train loss:0.8083819698190234\n",
      "train loss:0.9819426578407612\n",
      "train loss:0.8530212374840945\n",
      "train loss:0.8231412290660757\n",
      "train loss:0.8297362341929595\n",
      "train loss:0.8656080592622432\n",
      "train loss:0.8532215287295495\n",
      "train loss:0.7549013238447125\n",
      "train loss:0.8061394860365039\n",
      "train loss:1.1093051337305113\n",
      "train loss:0.8659515935804016\n",
      "train loss:0.8097633927573239\n",
      "train loss:0.8679998444997956\n",
      "train loss:0.736413185676507\n",
      "train loss:0.8963195962110936\n",
      "train loss:0.8433666534107301\n",
      "train loss:0.8700900054928673\n",
      "train loss:0.8685050559386172\n",
      "train loss:0.9240839709931775\n",
      "train loss:1.0344523454246295\n",
      "train loss:0.9470473144359823\n",
      "train loss:0.9382831905947726\n",
      "train loss:0.8757013221999217\n",
      "train loss:0.8190325767271999\n",
      "train loss:0.9044846966966362\n",
      "train loss:0.8191931124199912\n",
      "train loss:1.0414975000642688\n",
      "train loss:0.9265192723478504\n",
      "train loss:0.6235829267475538\n",
      "train loss:0.9191017100052002\n",
      "train loss:0.9886286147792542\n",
      "train loss:0.9109795924852169\n",
      "train loss:0.7830522250139772\n",
      "train loss:0.8283423817924315\n",
      "train loss:0.6915912759295497\n",
      "train loss:0.9912296661838765\n",
      "train loss:1.001207843228598\n",
      "train loss:0.725642054342285\n",
      "train loss:1.023395268861169\n",
      "train loss:0.7493744659704911\n",
      "train loss:0.8497754662256238\n",
      "train loss:0.8100853572852452\n",
      "train loss:1.1422268370001956\n",
      "train loss:0.9848953294018677\n",
      "train loss:0.9755824441645692\n",
      "train loss:0.9136478254198452\n",
      "train loss:0.9723310776472752\n",
      "train loss:0.8418013229508119\n",
      "train loss:0.9303683827514584\n",
      "train loss:0.8387879547769803\n",
      "train loss:0.8207156819329297\n",
      "train loss:1.0101337542328577\n",
      "train loss:0.8684793539389923\n",
      "train loss:0.8825312356915193\n",
      "train loss:0.8867656915416409\n",
      "train loss:0.8049106370826167\n",
      "train loss:0.9935010890523869\n",
      "train loss:0.8546550435966195\n",
      "train loss:0.8249737380234372\n",
      "train loss:0.8529958138082203\n",
      "train loss:0.717125855539339\n",
      "train loss:0.9743035790118414\n",
      "train loss:0.8812395111296698\n",
      "train loss:0.9402038290098426\n",
      "train loss:0.8308062845720525\n",
      "train loss:0.7840913936965406\n",
      "train loss:0.7787707706480315\n",
      "train loss:0.8253164496087253\n",
      "train loss:0.8013250888143042\n",
      "train loss:0.8350114926625812\n",
      "train loss:1.0372177022644835\n",
      "train loss:0.8447138259080929\n",
      "train loss:0.8122042615991915\n",
      "train loss:0.9102616358981279\n",
      "train loss:0.8941971427586621\n",
      "train loss:1.0054663583336578\n",
      "train loss:0.7914744932591408\n",
      "train loss:1.0008807594090678\n",
      "train loss:0.8777877772368033\n",
      "train loss:0.9594266970715963\n",
      "train loss:1.0169581764422446\n",
      "train loss:0.9150144904702634\n",
      "train loss:0.891160322593019\n",
      "train loss:0.80981077590196\n",
      "train loss:0.7118645626570377\n",
      "train loss:0.8796882137337702\n",
      "train loss:0.8100157742314744\n",
      "train loss:0.786041497468483\n",
      "train loss:0.6205561088091808\n",
      "train loss:0.7321713377233074\n",
      "train loss:0.8471487599074584\n",
      "train loss:1.0137336996704533\n",
      "train loss:0.9593803015483645\n",
      "train loss:0.8379962766348806\n",
      "train loss:0.9520482319347177\n",
      "train loss:0.8427962022596438\n",
      "train loss:0.8124831957939969\n",
      "train loss:0.862924382309897\n",
      "train loss:0.8554032106753214\n",
      "train loss:1.0374971239138164\n",
      "train loss:1.0184845562891625\n",
      "train loss:0.865536450769578\n",
      "train loss:0.9301373333907125\n",
      "train loss:0.7545288142218441\n",
      "train loss:0.8438749272695801\n",
      "train loss:0.9204655234280503\n",
      "train loss:0.8902193420527064\n",
      "train loss:1.0356760378218945\n",
      "train loss:0.8219237788017106\n",
      "train loss:0.6495650530544139\n",
      "train loss:0.8063140151200683\n",
      "train loss:0.8510640581536328\n",
      "train loss:0.8045173557010544\n",
      "train loss:0.9225943294566851\n",
      "train loss:0.7226680178563304\n",
      "train loss:0.8894265146686021\n",
      "train loss:1.0512578589319759\n",
      "train loss:0.7204374680334547\n",
      "train loss:0.9793544817771419\n",
      "train loss:0.977103795040925\n",
      "train loss:0.9354068192186601\n",
      "train loss:0.8000701881473476\n",
      "train loss:0.9143055233347144\n",
      "train loss:0.9647693148267308\n",
      "train loss:0.9926387606316218\n",
      "train loss:0.9155660489513511\n",
      "train loss:0.9789589711334052\n",
      "train loss:0.8461927545933875\n",
      "train loss:0.835633824327669\n",
      "train loss:0.9290654718787663\n",
      "train loss:0.8583035135592293\n",
      "train loss:0.8188901879025617\n",
      "train loss:0.7110409182072799\n",
      "train loss:0.8154125217806887\n",
      "train loss:0.7271390541839164\n",
      "train loss:0.7765094680391216\n",
      "train loss:0.6552950942418424\n",
      "train loss:0.8472137245590545\n",
      "train loss:0.8572643198359315\n",
      "train loss:0.9873883526836273\n",
      "train loss:0.62651388861078\n",
      "train loss:0.8545897197278564\n",
      "train loss:0.8387359286836645\n",
      "train loss:0.9389316882528995\n",
      "train loss:0.826917019112504\n",
      "train loss:0.7548970463138435\n",
      "train loss:1.1397735964066777\n",
      "train loss:0.8272792425645459\n",
      "train loss:0.8511208430115123\n",
      "train loss:0.6108516964551581\n",
      "train loss:1.024196075812811\n",
      "train loss:0.9176746399264476\n",
      "train loss:1.0415016066209237\n",
      "train loss:0.7879577784799142\n",
      "train loss:0.9741029748434687\n",
      "train loss:0.7975249919595379\n",
      "train loss:0.826976638072328\n",
      "train loss:0.839295624684367\n",
      "train loss:0.7316701695652454\n",
      "train loss:0.9451316774033942\n",
      "train loss:0.7884777691763841\n",
      "train loss:1.0176237948491993\n",
      "train loss:1.0396563073999017\n",
      "train loss:0.788039039107052\n",
      "train loss:0.9168114380904464\n",
      "train loss:0.8136027108687823\n",
      "train loss:0.9819208095046309\n",
      "train loss:0.7476599425398001\n",
      "train loss:0.9677906090541897\n",
      "train loss:0.9476162524994229\n",
      "train loss:1.0210560550949657\n",
      "train loss:1.0674107762987515\n",
      "train loss:0.7402160972436238\n",
      "train loss:0.7617283678052945\n",
      "train loss:0.8315536959540099\n",
      "train loss:0.8207737478128544\n",
      "train loss:0.9284777340069901\n",
      "train loss:0.8621824583630959\n",
      "train loss:1.0047950667446992\n",
      "train loss:0.8280367879451732\n",
      "train loss:0.8173255401802204\n",
      "train loss:0.7400258943853579\n",
      "train loss:0.8528471231547562\n",
      "train loss:0.9360526629996165\n",
      "train loss:0.8767876078061684\n",
      "train loss:0.8528797714620988\n",
      "train loss:0.7631679404454303\n",
      "train loss:0.8223742107371909\n",
      "train loss:0.7573102346110804\n",
      "train loss:0.8597607637548533\n",
      "train loss:0.9464365757361916\n",
      "train loss:1.015229669008873\n",
      "train loss:0.9492736405634856\n",
      "train loss:0.873388211409728\n",
      "train loss:0.8856257015105445\n",
      "train loss:0.7063305775925558\n",
      "train loss:0.8665325731729361\n",
      "train loss:0.8171998433716268\n",
      "train loss:0.9210265661093093\n",
      "train loss:1.0582227690121915\n",
      "train loss:0.8905971390706425\n",
      "train loss:0.9602267861973295\n",
      "train loss:0.7922946652992883\n",
      "train loss:0.9082176889490742\n",
      "train loss:0.8126534728220629\n",
      "train loss:0.8545557084213299\n",
      "train loss:0.8857096755323518\n",
      "train loss:0.8841716281726378\n",
      "train loss:0.918400665464625\n",
      "train loss:0.8743003144782908\n",
      "train loss:0.8226677331997447\n",
      "train loss:0.8201355727429319\n",
      "train loss:0.9080972922538814\n",
      "train loss:0.8078761865608963\n",
      "train loss:0.8260480919254468\n",
      "train loss:0.7868072995880548\n",
      "train loss:0.9094521521557989\n",
      "train loss:0.8338186789189699\n",
      "train loss:0.9496862920980198\n",
      "train loss:0.9232726613062067\n",
      "train loss:0.8461464612904959\n",
      "train loss:0.8880820337672849\n",
      "train loss:0.8578263018079808\n",
      "train loss:1.0330214549809216\n",
      "train loss:0.9517868316391741\n",
      "train loss:1.0853011102276249\n",
      "train loss:0.9818716841151333\n",
      "train loss:0.88265993740676\n",
      "train loss:0.8637024617239879\n",
      "train loss:0.747945329761901\n",
      "train loss:0.8012407308058686\n",
      "train loss:0.8845624508558586\n",
      "train loss:0.906775768132031\n",
      "train loss:0.7956946172386538\n",
      "train loss:0.8454067271721205\n",
      "train loss:0.9996810637350362\n",
      "train loss:0.8865656026177378\n",
      "train loss:0.8733470854890945\n",
      "train loss:1.059328717027983\n",
      "train loss:1.0499177363394012\n",
      "train loss:0.823040231592908\n",
      "train loss:0.9041059081470632\n",
      "train loss:0.9746882949103761\n",
      "train loss:0.99236325410411\n",
      "train loss:0.8848627710603142\n",
      "train loss:0.92048945019834\n",
      "train loss:0.8619526027941816\n",
      "train loss:0.8739596656565755\n",
      "train loss:0.8871195319273862\n",
      "train loss:0.8595231755618196\n",
      "train loss:0.9795730859637743\n",
      "train loss:1.0271616846702243\n",
      "train loss:0.9056532781662461\n",
      "train loss:0.8179756876266864\n",
      "train loss:0.9301373801902053\n",
      "train loss:0.962313518108808\n",
      "train loss:0.7969942568863676\n",
      "train loss:0.8720473758129291\n",
      "train loss:0.8620761888766383\n",
      "train loss:0.895454197128723\n",
      "train loss:0.9047503840081893\n",
      "train loss:0.9508772577230377\n",
      "train loss:0.9678183510574254\n",
      "train loss:0.7751655455368917\n",
      "train loss:0.9864600164189821\n",
      "train loss:0.7965398710268949\n",
      "train loss:1.0124997737146504\n",
      "train loss:1.0542344941612765\n",
      "train loss:0.888525498467055\n",
      "train loss:0.7849262918921913\n",
      "train loss:0.9351958067653898\n",
      "train loss:0.9838184710410066\n",
      "train loss:0.9819290288915994\n",
      "train loss:0.887393709502097\n",
      "train loss:0.8600466218504808\n",
      "train loss:0.9256309516138493\n",
      "train loss:1.001026548145983\n",
      "train loss:0.8116281747985314\n",
      "train loss:0.854307009179159\n",
      "train loss:0.8169866824998853\n",
      "train loss:0.8849812391652884\n",
      "train loss:0.961458806357604\n",
      "train loss:0.913955843029184\n",
      "train loss:0.8768188953584066\n",
      "train loss:0.9101800918996328\n",
      "train loss:0.8320667135754948\n",
      "train loss:0.9916843989115433\n",
      "train loss:0.8048726972321227\n",
      "train loss:0.9899515990847586\n",
      "train loss:0.8864530686866751\n",
      "train loss:0.8867291889470372\n",
      "train loss:0.9065768425196669\n",
      "train loss:1.062148068359747\n",
      "train loss:0.8623053367484758\n",
      "train loss:0.9447318431711993\n",
      "train loss:0.8570124053157037\n",
      "train loss:0.768699545220302\n",
      "train loss:0.8893971970787624\n",
      "train loss:0.8133133832294415\n",
      "train loss:1.001689476691185\n",
      "train loss:0.9172351613991842\n",
      "train loss:0.888465395614718\n",
      "train loss:0.833288868176766\n",
      "train loss:0.8400482050469386\n",
      "train loss:1.0415552458687207\n",
      "train loss:1.0427878518728315\n",
      "train loss:0.9874884938649015\n",
      "train loss:0.9383532172401035\n",
      "train loss:0.9167652860715542\n",
      "train loss:0.9750256149416187\n",
      "train loss:1.0012178432515555\n",
      "train loss:0.7873077766991652\n",
      "train loss:0.7464943459350638\n",
      "train loss:0.7682164332204495\n",
      "train loss:0.8788227010864724\n",
      "train loss:1.195482069339792\n",
      "train loss:0.9692269765519468\n",
      "train loss:1.0495967962422288\n",
      "train loss:1.212329089417126\n",
      "train loss:0.9468199210975651\n",
      "train loss:0.8141186844282141\n",
      "train loss:0.9979514917781903\n",
      "train loss:0.9540743579363002\n",
      "train loss:0.9740708623933397\n",
      "train loss:0.7546972657121799\n",
      "train loss:0.7337592292533663\n",
      "train loss:0.8282363008777728\n",
      "train loss:0.7691934328140603\n",
      "train loss:0.9269663817601642\n",
      "train loss:0.8270285273406554\n",
      "train loss:0.9580881763188098\n",
      "train loss:0.7877365801852991\n",
      "train loss:0.8998540969845459\n",
      "train loss:0.8697952111342755\n",
      "train loss:0.8643711225735465\n",
      "train loss:0.8099867904073501\n",
      "train loss:0.9204640061805165\n",
      "train loss:1.062609880430864\n",
      "train loss:0.9789028654290283\n",
      "train loss:1.0246617432270824\n",
      "train loss:0.9646185257773461\n",
      "train loss:0.8577602186310647\n",
      "train loss:0.8483369911733756\n",
      "train loss:0.8105525506809912\n",
      "train loss:0.779791303311062\n",
      "train loss:0.9992291010751319\n",
      "train loss:0.899102077711936\n",
      "train loss:0.8465439805841907\n",
      "train loss:0.8105256788095835\n",
      "train loss:0.9691156829704791\n",
      "train loss:0.7581260276430432\n",
      "train loss:0.9064275857162071\n",
      "train loss:0.7660912957643663\n",
      "train loss:0.9487392647058748\n",
      "train loss:0.8880719814904565\n",
      "train loss:0.8936797087811719\n",
      "train loss:1.0223294297516938\n",
      "train loss:0.9944465304000069\n",
      "train loss:0.8428045521739737\n",
      "train loss:0.9308360703711941\n",
      "train loss:0.7997592181678086\n",
      "train loss:1.01538164849181\n",
      "train loss:1.0653382955229085\n",
      "train loss:0.9497822200789026\n",
      "train loss:0.9805684580591422\n",
      "train loss:0.8657728327396831\n",
      "train loss:0.821156424134191\n",
      "train loss:0.8471775283093822\n",
      "train loss:1.0468099149188062\n",
      "train loss:0.706283052083067\n",
      "train loss:0.9135900514516545\n",
      "train loss:0.695149298960907\n",
      "train loss:0.8771640717577545\n",
      "train loss:1.0623174480015036\n",
      "train loss:0.919126618338713\n",
      "train loss:0.9048085800805556\n",
      "train loss:0.8749427941275595\n",
      "train loss:0.8351937850363464\n",
      "train loss:0.9038029514629017\n",
      "train loss:0.9243726154851553\n",
      "train loss:0.951367726440469\n",
      "train loss:0.7457142936651251\n",
      "train loss:0.9277484143692031\n",
      "train loss:1.0465822721321738\n",
      "train loss:0.8059595478829295\n",
      "train loss:0.9275428722250326\n",
      "train loss:0.8573259933354488\n",
      "train loss:0.8978760682071651\n",
      "train loss:0.8848335242879102\n",
      "train loss:0.851913432589065\n",
      "train loss:1.043384837136464\n",
      "train loss:0.9016337338767804\n",
      "train loss:0.8997182189598939\n",
      "train loss:0.916104281529418\n",
      "train loss:0.8075239576763644\n",
      "train loss:0.8584790386709849\n",
      "train loss:0.9888598240516602\n",
      "train loss:0.6807450653851995\n",
      "train loss:0.7628819826068193\n",
      "train loss:0.8463275136017063\n",
      "train loss:0.8774756938342573\n",
      "train loss:0.7531524671141896\n",
      "train loss:0.8748297814976067\n",
      "train loss:0.8728195100881715\n",
      "train loss:0.9411898582672608\n",
      "train loss:0.717359110547382\n",
      "train loss:0.7606792954324494\n",
      "train loss:0.9571919767603524\n",
      "train loss:0.9138565833960495\n",
      "train loss:0.9665358476966149\n",
      "train loss:0.9943823995216973\n",
      "train loss:0.8656713537312453\n",
      "train loss:0.7873081281881151\n",
      "train loss:0.8444250712933433\n",
      "train loss:0.8687960427738224\n",
      "train loss:1.0066332267146518\n",
      "train loss:1.0109684987800238\n",
      "train loss:0.8427805171880334\n",
      "train loss:0.9114892549886581\n",
      "train loss:0.7860912247675337\n",
      "train loss:1.001053640684325\n",
      "train loss:0.9468880009527151\n",
      "train loss:0.9654623129900003\n",
      "train loss:0.8825919224416893\n",
      "train loss:0.8767187827978296\n",
      "train loss:0.780803044408758\n",
      "train loss:0.8590913425123713\n",
      "train loss:0.9280105741871038\n",
      "train loss:0.9987044260432735\n",
      "train loss:1.022574354502955\n",
      "train loss:1.0138076967929068\n",
      "train loss:0.8617450399320685\n",
      "train loss:0.9400244897380605\n",
      "train loss:0.8316057989724455\n",
      "train loss:0.8529596518675818\n",
      "train loss:0.8776976872017552\n",
      "train loss:0.9477922471223853\n",
      "train loss:0.8900798759941345\n",
      "train loss:0.7445275759777591\n",
      "train loss:0.8625935174524464\n",
      "train loss:0.7809805629929184\n",
      "train loss:0.9829396102113581\n",
      "train loss:0.8703468285836575\n",
      "train loss:0.8403361904990305\n",
      "train loss:0.8942342721700087\n",
      "train loss:0.8562224044804119\n",
      "train loss:0.8896727018267582\n",
      "train loss:0.8542171041347938\n",
      "train loss:0.9279445898526815\n",
      "train loss:0.9674717768130736\n",
      "train loss:0.9650961864556404\n",
      "train loss:0.6453512576163293\n",
      "train loss:0.9091321088666452\n",
      "train loss:0.649404027903213\n",
      "train loss:0.814996497500741\n",
      "train loss:0.6759202678220972\n",
      "train loss:0.739193524206001\n",
      "train loss:0.9476916439304375\n",
      "train loss:0.908826984008655\n",
      "train loss:0.7239451393077349\n",
      "train loss:0.6811281652354526\n",
      "train loss:0.8490177763036566\n",
      "train loss:0.9674774797434295\n",
      "train loss:0.8290246903515616\n",
      "train loss:0.9202633390851294\n",
      "train loss:1.021934370392315\n",
      "train loss:0.936059581582887\n",
      "train loss:0.8937247780546298\n",
      "train loss:0.7806552543752407\n",
      "train loss:1.0168729000228358\n",
      "train loss:0.911630030380612\n",
      "train loss:0.797504362863517\n",
      "train loss:0.9117131394208144\n",
      "train loss:0.8196889858164166\n",
      "train loss:0.9698715100068\n",
      "train loss:0.965238288640052\n",
      "train loss:0.8746101247691862\n",
      "train loss:0.8677869797326764\n",
      "train loss:0.8020017521638367\n",
      "train loss:0.7146307751680948\n",
      "train loss:0.9175288065644497\n",
      "train loss:0.8258428985935822\n",
      "train loss:0.8738463877218922\n",
      "train loss:0.9035218671051579\n",
      "train loss:1.0153542246843559\n",
      "train loss:0.8874280260879243\n",
      "train loss:0.9646932447973302\n",
      "train loss:0.8076032101756516\n",
      "train loss:0.9243920663926573\n",
      "train loss:0.9999890687764496\n",
      "train loss:1.0387091203947054\n",
      "train loss:0.8625844698953296\n",
      "train loss:0.9369018585217802\n",
      "train loss:0.8470547232631739\n",
      "train loss:0.8478904681612857\n",
      "train loss:1.0577600601237747\n",
      "train loss:0.7313129741906818\n",
      "train loss:0.8691728899204608\n",
      "train loss:0.9096024930464803\n",
      "train loss:0.9191292917551089\n",
      "train loss:1.1807068900635003\n",
      "train loss:1.190001844987966\n",
      "train loss:0.8911095304517876\n",
      "train loss:0.849145332780894\n",
      "train loss:0.8609894936281048\n",
      "train loss:0.8257922551610025\n",
      "train loss:0.9414526177997208\n",
      "train loss:0.8220270248500268\n",
      "train loss:0.8990341367217651\n",
      "train loss:0.9510433740129747\n",
      "train loss:0.9810312791097622\n",
      "train loss:0.8633792315779293\n",
      "train loss:0.7743628071009533\n",
      "train loss:0.8512001525819508\n",
      "train loss:0.7574989899134137\n",
      "train loss:0.9244463929717326\n",
      "train loss:0.9458059997838336\n",
      "train loss:0.9201467833610402\n",
      "train loss:0.8619987354853065\n",
      "train loss:0.8258228306582628\n",
      "=== epoch:10, train acc:0.994, test acc:0.992 ===\n",
      "train loss:0.9575822493756451\n",
      "train loss:0.8233375529333418\n",
      "train loss:0.8324128273391215\n",
      "train loss:1.0272445137372508\n",
      "train loss:0.8986378662885747\n",
      "train loss:0.9162188497455724\n",
      "train loss:0.8469195177631725\n",
      "train loss:0.7207172623817422\n",
      "train loss:0.7613389228162079\n",
      "train loss:0.9269110620728945\n",
      "train loss:0.8849835044938527\n",
      "train loss:0.996553451618902\n",
      "train loss:0.8939293636826398\n",
      "train loss:0.684225225782382\n",
      "train loss:1.046349792055687\n",
      "train loss:0.9374779215541486\n",
      "train loss:1.0357299444378127\n",
      "train loss:0.8617834031315045\n",
      "train loss:0.8692031920177287\n",
      "train loss:0.8715535633113598\n",
      "train loss:0.7723695817730245\n",
      "train loss:1.037532779912471\n",
      "train loss:0.9028027399145349\n",
      "train loss:0.8653856628517222\n",
      "train loss:0.8828878879300024\n",
      "train loss:0.8329280655049128\n",
      "train loss:0.9075529435145604\n",
      "train loss:0.9452206822979392\n",
      "train loss:0.9436838009160219\n",
      "train loss:0.896963789492562\n",
      "train loss:0.9376052742702895\n",
      "train loss:0.7550415801106802\n",
      "train loss:0.8935064532928549\n",
      "train loss:1.064578111063054\n",
      "train loss:0.9335196135529219\n",
      "train loss:0.7733763716077847\n",
      "train loss:0.804998593364531\n",
      "train loss:0.9508402324446839\n",
      "train loss:1.1228512351376247\n",
      "train loss:0.854684972009741\n",
      "train loss:0.8539095205854048\n",
      "train loss:1.0109185362903943\n",
      "train loss:0.9846976466703009\n",
      "train loss:0.8160905815783052\n",
      "train loss:0.7655164807274971\n",
      "train loss:0.7281880831155728\n",
      "train loss:0.9671336795936831\n",
      "train loss:0.504789123211784\n",
      "train loss:0.9610741114978555\n",
      "train loss:0.8800721271549535\n",
      "train loss:1.0718567616256072\n",
      "train loss:0.8073892723004839\n",
      "train loss:0.9756887662754046\n",
      "train loss:0.905478809024048\n",
      "train loss:0.916464474829019\n",
      "train loss:0.9016984735208944\n",
      "train loss:0.9022216926617967\n",
      "train loss:0.9014223863805566\n",
      "train loss:0.833175179548632\n",
      "train loss:0.8872439590041149\n",
      "train loss:0.9357371976463785\n",
      "train loss:0.8312333287791508\n",
      "train loss:0.92808423726287\n",
      "train loss:0.9709622361902369\n",
      "train loss:0.9289423331164477\n",
      "train loss:0.874100082590198\n",
      "train loss:0.8783655176639368\n",
      "train loss:0.7313853293884022\n",
      "train loss:0.7704618569982766\n",
      "train loss:0.8009832360426359\n",
      "train loss:1.086757939149188\n",
      "train loss:0.8350789358031931\n",
      "train loss:0.8982652936161787\n",
      "train loss:0.9267076358269367\n",
      "train loss:0.8643186953060292\n",
      "train loss:0.9235952675339427\n",
      "train loss:0.9805529721719217\n",
      "train loss:0.9283648938070062\n",
      "train loss:0.8761532572378877\n",
      "train loss:0.8456729684348159\n",
      "train loss:0.8663769705145452\n",
      "train loss:0.9801866679826281\n",
      "train loss:1.0131392432099837\n",
      "train loss:0.8985840604627552\n",
      "train loss:0.8338945806795403\n",
      "train loss:0.9239067895753235\n",
      "train loss:0.8664702699861221\n",
      "train loss:0.8433950218890318\n",
      "train loss:0.7510655230644033\n",
      "train loss:0.8967649384724078\n",
      "train loss:0.9129270563260868\n",
      "train loss:0.8939953204286958\n",
      "train loss:1.0503184523226539\n",
      "train loss:0.8152593225027647\n",
      "train loss:0.7547578619516849\n",
      "train loss:1.0033567949703148\n",
      "train loss:0.8174250198565194\n",
      "train loss:0.9511885411660782\n",
      "train loss:1.0359393056070003\n",
      "train loss:0.8630967855252524\n",
      "train loss:0.8992632441333707\n",
      "train loss:0.9217536510605469\n",
      "train loss:0.8448644391870194\n",
      "train loss:0.7959479366296633\n",
      "train loss:0.8065426210217411\n",
      "train loss:0.7035903741154746\n",
      "train loss:1.0173478324751635\n",
      "train loss:0.8831186831188321\n",
      "train loss:0.6678043093573567\n",
      "train loss:0.7712789179796542\n",
      "train loss:0.9329202915292101\n",
      "train loss:0.7821993134668423\n",
      "train loss:0.8030086628799524\n",
      "train loss:0.8350376292336652\n",
      "train loss:0.8761451158437846\n",
      "train loss:0.904261622492039\n",
      "train loss:0.749869098059508\n",
      "train loss:0.8314596078289007\n",
      "train loss:0.9211274012024532\n",
      "train loss:0.9734858011885781\n",
      "train loss:0.8044193480841014\n",
      "train loss:0.7947968254733186\n",
      "train loss:0.7900826057511989\n",
      "train loss:0.8786864818442452\n",
      "train loss:0.9134052816787346\n",
      "train loss:0.9355829583548761\n",
      "train loss:1.0538233979125953\n",
      "train loss:0.8633407828698432\n",
      "train loss:0.8773555067518908\n",
      "train loss:1.176782847469201\n",
      "train loss:0.9036417846248794\n",
      "train loss:0.9488169148460591\n",
      "train loss:1.0035484950870746\n",
      "train loss:0.7701748578910447\n",
      "train loss:0.7087788676246326\n",
      "train loss:0.7960509927710975\n",
      "train loss:0.952202533070106\n",
      "train loss:0.815435649472437\n",
      "train loss:0.8991000614435039\n",
      "train loss:0.9865096929989015\n",
      "train loss:0.9958695989612926\n",
      "train loss:0.8505581919510411\n",
      "train loss:1.0268943730390057\n",
      "train loss:0.8525635537882856\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-e37eb3f8da08>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m                   \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer_param\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'lr'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m                   evaluate_sample_num_per_epoch=1000)\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m# 保存参数\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\datasets\\Deep learning from scratch\\common\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[0mtest_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\datasets\\Deep learning from scratch\\common\\trainer.py\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_loss_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"train loss:\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\datasets\\Deep learning from scratch\\deep_convnet.py\u001b[0m in \u001b[0;36mloss\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_flg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlast_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\datasets\\Deep learning from scratch\\deep_convnet.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, train_flg)\u001b[0m\n\u001b[0;32m     77\u001b[0m                 \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_flg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m                 \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\datasets\\Deep learning from scratch\\common\\layers.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    218\u001b[0m         \u001b[0mout_w\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpad\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mFW\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 220\u001b[1;33m         \u001b[0mcol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mim2col\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    221\u001b[0m         \u001b[0mcol_W\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\datasets\\Deep learning from scratch\\common\\util.py\u001b[0m in \u001b[0;36mim2col\u001b[1;34m(input_data, filter_h, filter_w, stride, pad)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilter_w\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m             \u001b[0mx_max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstride\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mout_w\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[0mcol\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0my_max\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mx_max\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[0mcol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mout_h\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mout_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# %load train_deepnet.py\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 为了导入父目录而进行的设定\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mnist import load_mnist\n",
    "from deep_convnet import DeepConvNet\n",
    "from common.trainer import Trainer\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)    #x_train长度为60000，x_test长度为10000\n",
    "\n",
    "network = DeepConvNet()  \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=20, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr':0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "# 保存参数\n",
    "network.save_params(\"deep_convnet_params.pkl\")\n",
    "print(\"Saved Network Parameters!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strategic-sixth",
   "metadata": {},
   "source": [
    "运行25min 1 epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hidden-arrest",
   "metadata": {},
   "source": [
    "对于MNIST数据集，层不用特别深就获得了最高的识别精度，对于手写数字识别这样比较简单的任务，没有必要将网络的表现力提高到那么高的程度。而对于大规模的一般物体识别的情况，因为问题复杂，所以加深层对提高识别精度大有裨益。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aging-prevention",
   "metadata": {},
   "source": [
    "进一步提高识别精度的方法有集成学习、学习率衰减，数据扩充（data augmentation）等"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vital-upset",
   "metadata": {},
   "source": [
    "- 加深层的动机"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "settled-allowance",
   "metadata": {},
   "source": [
    "加深神经网络的好处是减少神经网络的参数，扩大感受野。\n",
    "\n",
    "并且通过叠加层，将ReLU等激活函数夹在卷积层中间，进一步提高了网络的表现力。\n",
    "\n",
    "加深层的另一个好处是使学习更加高效。通过加深网络，可以分层次地分解需要学习的问题。因此，各层需要学习的问题就变成了更简单的问题。这样一来，只需用较少的学习数据就可以高效进行学习。\n",
    "\n",
    "\n",
    "感受野（Receptive Field）的定义是卷积神经网络每一层输出的特征图（feature map）上的像素点在输入图片上映射的区域大小。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revolutionary-wealth",
   "metadata": {},
   "source": [
    "## 8.2 深度学习的小历史 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secure-cleveland",
   "metadata": {},
   "source": [
    "2012年举办的大规模图像识别大赛ILSVRC(IamgeNet Large Scale Visual Recognition Challenge)中，基于深度学习的方法（AlexNet）以压倒性的优势胜出，彻底颠覆了以往的图像识别方法。\n",
    "\n",
    "2015年的ResNet将错误识别率降低到了3.5%，这个结果甚至超过了普通人的识别能力。\n",
    "\n",
    "这些年深度学习取得了不斐的成绩，其中，VGG、GoogleNet、ResNet已广为人知。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turned-renaissance",
   "metadata": {},
   "source": [
    "- VGG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behavioral-wrist",
   "metadata": {},
   "source": [
    "VGG是由卷积层和池化层构成的基础的CNN，它的特点在于将有权重的层叠加至16层（或19层），具备了深度，根据层的不同，优势也称为“VGG16”或“VGG19”。\n",
    "\n",
    "VGG中基于3*3小型滤波器的卷积层的计算是连续进行的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optical-audience",
   "metadata": {},
   "source": [
    "- GoogLeNet\n",
    "\n",
    "GoogLeNet的特征是不仅在纵向上有深度，在横向上也有深度（广度）。在横向上有宽度，这称为“Inception结构”，Inception结构使用了多个大小不同的滤波器，最后再合并他们的结果。\n",
    "\n",
    "在GoogLeNet中，使用了多个大小为$1*1$的滤波器的卷积层，由于减少参数和实现高速化处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hydraulic-enclosure",
   "metadata": {},
   "source": [
    "- ResNet\n",
    "\n",
    "在深度学习中，过度加深层的话，很多情况下学习不能顺利进行，导致最终性能不佳。在ResNet中，导入了“快捷结构”，导入这个快捷结构后，就可以随着层的加深而不断提高性能了。\n",
    "\n",
    "快捷结构横跨了输入数据的卷积层，将输入合计到输出，通过快捷结构，之前因加深层而导致的梯度消失问题有望得到缓解。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turned-georgia",
   "metadata": {},
   "source": [
    "## 8.3 深度学习的高速化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "general-antique",
   "metadata": {},
   "source": [
    "在AlexNet中，大多数时间都被耗费在卷积层上，卷积层的处理时间加起来占GPU整体的95%,占CPU整体的89%，如何高速、高效地进行卷积层的运算是深度学习的一大课题。卷积层中进行的运算可以追溯至乘积累加运算。\n",
    "\n",
    "因此，深度学习的主要课题就变成了如何高速、高效地进行乘积累加运算。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strong-hundred",
   "metadata": {},
   "source": [
    "- 基于GPU的高速化\n",
    "\n",
    "深度学习中需要进行大量的乘积累加运算（或者大型矩阵的乘积运算），这种大量的并行运算正是GPU所擅长的，反过来，CPU比较擅长连续的、复杂的计算。\n",
    "\n",
    "通过im2col可以将卷积层进行的运算转换为大型矩阵的乘积，这个im2col的方式的实现对GPU来说是非常方便的实现，GPU更擅长计算大规模的汇总好的数据，通过基于im2col以大型矩阵的乘积的方式汇总计算，更容易发挥出GPU的能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flush-poison",
   "metadata": {},
   "source": [
    "- 分布式学习\n",
    "\n",
    "为了进一步提高深度学习所需的计算的速度，可以考虑在多个GPU或者多台机器上进行分布式计算。\n",
    "\n",
    "“如何进行分布式计算”是一个非常难的课题。它包含了机器之间的通信、数据的同步等多个无法轻易解决的问题，可以将这些难题都交给tensorflow这个框架。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adolescent-assignment",
   "metadata": {},
   "source": [
    "- 位数缩减\n",
    "\n",
    "计算机中表示小数时，有32位单精度浮点数和64位双警服浮点数等格式。在深度学习中，即使是16位的半精度浮点数，也可以顺利进行学习。\n",
    "\n",
    "Numpy中提供了16位的半精度浮点数，只是16位类型的存储，运算本身不用16位进行，即使用Numpy的半精度浮点数，识别精度也不会下降。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "appreciated-jenny",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caluculate accuracy (float64) ... \n",
      "0.9935\n",
      "caluculate accuracy (float16) ... \n",
      "0.9935\n"
     ]
    }
   ],
   "source": [
    "# %load half_float_network.py\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 为了导入父目录而进行的设定\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from deep_convnet import DeepConvNet\n",
    "from mnist import load_mnist\n",
    "\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "network = DeepConvNet()\n",
    "network.load_params(\"deep_convnet_params.pkl\")\n",
    "\n",
    "sampled = 10000 # 为了实现高速化\n",
    "x_test = x_test[:sampled]\n",
    "t_test = t_test[:sampled]\n",
    "\n",
    "print(\"caluculate accuracy (float64) ... \")\n",
    "print(network.accuracy(x_test, t_test))\n",
    "\n",
    "# 转换为float16型\n",
    "x_test = x_test.astype(np.float16)\n",
    "for param in network.params.values():\n",
    "    param[...] = param.astype(np.float16)\n",
    "\n",
    "print(\"caluculate accuracy (float16) ... \")\n",
    "print(network.accuracy(x_test, t_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "three-tongue",
   "metadata": {},
   "source": [
    "## 8.4 深度学习的应用案例 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norwegian-delhi",
   "metadata": {},
   "source": [
    "深度学习不局限于物体识别，在图像、语音、自然语言等各个不同的领域，深度学习都展现了优异的性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ancient-fault",
   "metadata": {},
   "source": [
    "- 物体检测\n",
    "\n",
    "物体检测是从图像上确定物体的位置，并进行分类的问题。\n",
    "对于物体检测问题，有多个基于CNN的方法，其中有一个R-CNN的方法。\n",
    "\n",
    "R-CNN的处理流：1.Inuput image 2.Extract region proposals \n",
    "3. Compute CNN festures 4. Classify regions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "homeless-hardware",
   "metadata": {},
   "source": [
    "- 图像分割\n",
    "\n",
    "图像分割是指在像素水平上对图像进行分类。\n",
    "\n",
    "有人提出一个FCN的方法（Fully Convolutional Network）,该方法通过一次forward处理，对所有像素进行分类，FCN在最后导入了扩大空间大小的处理，可将变小的中间数据扩大到和输入图像一样的大小。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporate-allergy",
   "metadata": {},
   "source": [
    "- 图像标题的生成\n",
    "\n",
    "一个基于深度学习生成图像标题的代表性方法是被称为NIC（Neural Image Caption）的模型，NIC由深层的CNN和处理自然语言的RNN构成，RNN具有循环连接的网络，经常被用于自然语言、时间序列数据等连续性的数据上。\n",
    "\n",
    "基于NIC,可以生成惊人的高精度的图像标题，我们将组合图像和自然语言等多种信息进行的处理称为多模态处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capital-arizona",
   "metadata": {},
   "source": [
    "## 8.5 深度学习的未来 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driving-objective",
   "metadata": {},
   "source": [
    "- 图像风格变换"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convertible-reflection",
   "metadata": {},
   "source": [
    "输入两个图像后，会生成一个新的图像，两个输入图像中，一个称为“内容图像”，一个称为“风格图像”。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constitutional-newsletter",
   "metadata": {},
   "source": [
    "- 图像的生成\n",
    "\n",
    "基于DBSCAN可以生成以假乱真的图像，使用大量图像训练这个模型，学习结束后，使用这个模型就可以生成新的图像。\n",
    "\n",
    "DBSCAN中使用了深度学习，其技术要点是使用了Generator(生成者)和Discriminator(识别者)者两个神经网络。前者生成近似真品的图像，后者判断它是不是真的图像。像这样，通过让两者以竞争的方式去学习，Generator会学习到更精妙的图像作假技术，Discrinator会成为为能以更高精度辨别真假的鉴定师。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dependent-jewel",
   "metadata": {},
   "source": [
    "- 自动驾驶\n",
    "\n",
    "自动驾驶需要结合多种技术力量来实现，比如路线计划技术、照相机或激光传感技术等，在这些技术中，正确识别时刻变化的环境是非常困难的。\n",
    "\n",
    "基于CNN的神经网络SegNet，可以高精度地识别行驶环境。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grateful-outline",
   "metadata": {},
   "source": [
    "- 强化学习\n",
    "\n",
    "让计算机在摸索实践中自主学习，这称为强化学习。\n",
    "\n",
    "强化学习的基本框架是，代理根据环境选择行动，然后通过这个行动改变环境，根据环境的变化，代理获得某种报酬。强化学习的目的是，决定代理的行动方针，以获得更好地报酬。\n",
    "\n",
    "在深度学习中，有一个叫做Deep Q Network(DQN)的方法，这个方法基于被称为Q学习的强化学习算法，为了确定最合适的行动，需要确定一个被称为最优行动价值函数的函数。\n",
    "\n",
    "在DQN的研究中，已有让电子游戏自动学习，实现超过人类水平操作的例子。\n",
    "\n",
    "人工智能AlphaGo击败围棋冠军的新闻受到了广泛关注，这个AlphaGo技术的内部也使用了深度学习和强化学习。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
